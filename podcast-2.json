{"text": " Papers read on AI, with Rob, keeping you up to date with the latest research. This reading is brought to you by Mars Race, Stake a Claim on the Red Planet, available on Android and iOS. Convolutions Die Hard \u2013 Open vocabulary segmentation with single frozen convolutional clip. Authored 2023 by Qihang Yu, Zhu He, Xueqing Deng, Shao Weisheng, Liang Che Chen. Abstract \u2013 Open vocabulary segmentation is a challenging task requiring segmenting and recognizing objects from an open set of categories in diverse environments. One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide image and text features in a shared embedding space, which effectively bridges the gap between closed vocabulary and open vocabulary recognition. Hence, existing methods often adopt a two-stage framework to tackle the problem, where the inputs first go through a mask generator and then through the clip model along with the predicted masks. This process involves extracting features from raw images multiple times, which can be ineffective and inefficient. By contrast, we propose to build everything into a single stage framework using a shared frozen convolutional clip backbone, which not only significantly simplifies the current two-stage pipeline, but also remarkably yields a better accuracy cost tradeoff. The resulting single stage system, called FC-CLIP, benefits from the following observations \u2013 the frozen clip backbone maintains the ability of open vocabulary classification and can also serve as a strong mask generator, and the convolutional clip generalizes well to a larger input resolution than the one used during contrastive image-text pre-training. Surprisingly, FC-CLIP advances state-of-the-art results on various benchmarks while running practically fast. Specifically, when training on Cocoa panoptic data only and testing in a zero-shot manner, FC-CLIP achieved 26.8pq, 16.8ap, and 34.1miou on ADE20K, 18.2pq, 27.9miou on Mapillary Vista's, and 34.0pq, 26.8ap, 56.2miou on Cityscapes, outperforming the prior art under the same setting by plus 4.2pq, plus 2.4ap, plus 4.2miou on ADE20K, plus 4.0pq on Mapillary Vista's and plus 20.1pq on Cityscapes, respectively. Additionally, the training and testing time of FC-CLIP is 7.5 times and 6.6 times significantly faster than the same prior art, while using 5.9 times fewer total model parameters. Meanwhile, FC-CLIP also sets a new state-of-the-art performance across various open vocabulary semantic segmentation datasets. Code will be available at https://github.com/.bytedance/.fc-clip. 1. Introduction. Panoptic Segmentation, reference 42, is a complex computer vision task that aims to predict a set of non-overlapping masks, each with its corresponding class label. It combines the tasks of semantic segmentation, reference 35, and instance segmentation, reference 32, making it a challenging problem to solve. Many methods, reference 41, 82, 17, 78, 49, 88, 19, 89, 51, have been proposed to tackle this problem, and a significant progress has been made in terms of panoptic quality, pq. However, due to the high cost of annotating such a fine-grained dataset, reference 52, 21, the number of semantic classes is typically limited to a few dozens or hundreds. This restriction hinders the further application of existing approaches to real-world settings, where the number of possible semantic classes is unlimited. To overcome the limitations of closed vocabulary segmentation, open vocabulary segmentation, reference 46, 85, 28, 24, has been proposed. These approaches uses text embeddings of category names, reference 92, represented in natural language, as label embeddings, instead of learning them from the training dataset. By doing so, models can classify objects from a wider vocabulary, which improves their ability to handle a broader range of categories. To ensure that meaningful embeddings are provided, a pre-trained text encoder, reference 22, 67, 55, 66, is typically used. This encoder can effectively capture the semantic meaning of words and phrases, which is critical for open vocabulary segmentation. Multi-modal models, such as CLIP, reference 66, and ALIGN, reference 38, have shown promise for open vocabulary segmentation due to their ability to learn aligned image text feature representations from large-scale internet data, reference 70. SIMBASELINE, reference 85, and OVSEG, reference 50, are two recent methods that use a two-stage framework to adapt CLIP for open vocabulary segmentation. In these methods, images are first processed by a heavy mask generator, reference 34, 19, to obtain mask proposals, and then each masked image crop is generated and fed into a frozen CLIP model for classification. MASKCLIP, reference 24, extends this approach to open vocabulary panoptic segmentation, but additionally leverages mask proposals as attention masks in the CLIP backbone to multiple forwarding processes for the masked crops. More recently, OTISE, reference 84, employs a stable diffusion UNET, reference 69, 68, as a frozen backbone for mask generator, which significantly boosts the state-of-the-art performance. However, despite these advances, they still rely on a two-stage framework, where the mask generator and CLIP classifier extract features from raw images separately, resulting in inefficiency and ineffectiveness. A natural question thus arises as to whether it is possible to unify the mask generator and CLIP classifier into a single-stage framework for open vocabulary segmentation. Sharing the feature extractor between them is a straightforward solution, but it poses two challenges. First, fine-tuning CLIP backbone can disrupt the alignment between image and text features, resulting in a much worse performance on out-of-vocabulary categories. Same methods, reference 85, 50, 24, 84, rely on another separate backbone for mask generator, increasing model size and computational costs. Second, CLIP models are typically pre-trained on relatively lower resolution inputs, while dense prediction tasks require a much higher resolution for optimal performance. This makes it difficult to directly apply CLIP pre-trained backbones to downstream dense prediction tasks, particularly VIT-based CLIP models, reference 25, where careful treatments are required, e.g., side adapter, reference 16, 86, are cost aggregation, reference 96, 20. Consequently, existing methods, reference 24, 84, perform mask segmentation and CLIP classification at different input scales, leading to sub-optimal performance. To alleviate the two challenges, we propose to build both mask generator and CLIP classifier on top of a shared frozen convolutional CLIP backbone, resulting in a single-stage framework FC CLIP. Since design is based on the following observations, the frozen CLIP backbone ensures that the pre-trained image-text feature alignment is intact, allowing out of vocabulary classification. It can also serve as a strong mask generator by appending a lightweight pixel decoder and mask decoder, reference 19, 89. The convolutional CLIP, based on a convolutional neural network, CNN, reference 45, empirically shows a better generalization ability compared to VIT-based CLIP, reference 25, when the input size scales up. This echoes the success of fully convolutional networks, reference 58, in dense prediction tasks. Both observations are critical for developing a single-stage framework, but they have been overlooked and undiscovered by existing two-stage pipelines, reference 24, 84. In Fig. 1, we visualize the learned visual representation of VIT-based and CNN-based CLIP via K-means clustering, reference 57. As shown in the figure, the features learned by CNN-based CLIP are more robust across different input sizes. Surprisingly, the adoption of a single frozen convolutional CLIP as the shared feature extractor results in an extremely simple yet effective design. Specifically, the single-stage FC CLIP consists of three modules built upon a shared frozen convolutional CLIP backbone, a class-agnostic mask generator, an in-vocabulary classifier, see Fig. 2 for comparison between pipelines. The proposed method not only enjoys a simple design, but also comes with a very low cost for both training and testing. As a comparison, our model has only 238M frozen parameters and 21M trainable parameters, against the state-of-the-art work ODISE, reference 84, that has 1494M frozen and 28M trainable parameters. Furthermore, our model training only takes 25.6V 100 GPU days, which is 7.5 times faster compared to ODIISE's 192V 100 GPU days. During inference, our model also runs 6.6 times faster. Although FC CLIP enjoys a simple design, it still outperforms previous methods across multiple datasets. Trained on Cocoa Panoptic dataset only, FC CLIP surpasses prior state-of-the-art ODISE, significantly in a zero-shot manner. Specifically, FC CLIP achieves 26.8PQ, plus 3.4, 18.2PQ, plus 4.0, and 44.0PQ, plus 2.0.1, on ADE20K, Mapillary Vistas, and Cityscapes, respectively. As Panoptic segmentation unifies semantic and instance segmentation, FC CLIP naturally extends to open-vocabulary semantic and instance segmentation. With the same model trained on Cocoa Panoptic data only, i.e., no task-specific fine-tuning, FC CLIP achieves state-of-the-art performance on open-vocabulary instance and semantic segmentation. Specifically, FC CLIP achieves 16.8AP on ADE20K, surpassing the state-of-art ODIISE, reference 84, by plus 2.4. FC CLIP also outperforms the state-of-art specialized open-vocabulary semantic segmentation model SAN, reference 86, by plus 1.1 and plus 1.1 MIOU on the challenging ADE20K847, A847, and Pascal Context 459, PC 459, benchmarks, respectively. In summary, through the lens of a careful redesign of existing two-stage open-vocabulary segmentation models, we establish a simple, strong, and fast baseline for the community. The proposed FC CLIP adopts a single-stage framework by exploiting a shared frozen convolutional CLIP, which not only advances the state-of-the-art performances on multiple benchmarks, but also enjoys a practically fast training and inference speed. We hope our study will inspire future research on efficient single-stage open-vocabulary segmentation models. 2. RELATED WORK. Vision language models target at encoding vision and language jointly in a fusion model. 73, 15, 93. Extract visual representations by pre-trained object detectors and fine-tune on downstream tasks with language supervision. Recently, with the breakthrough of large language models, reference 22, 3, rapid progress has been made in this field. CLIP, reference 66, and ALIGN, reference 38, demonstrate that pre-training dual encoder models with contrastive objectives on large-scale noisy image textpairs can learn representation with cross-modal alignment ability and show strong performance in zero-shot downstream tasks. The following works, reference 90, 1, 87, further confirm these points and achieve impressive results in zero-shot transfer learning such as open-vocabulary image recognition. Closed-vocabulary segmentation can be divided into three types according to the semantics of the grouping pixels, i.e. semantic, instance and panoptic segmentation. Semantic segmentation interprets high-level category semantic concepts. Prior works, 9, 69, 10-12, 27, 91, 81, 94, 29, mainly treat this task as a per-pixel classification problem and build their models on top of the idea of FCN, reference 58. Instance segmentation groups foreground pixels into different object instances, starting from mask rCNN, reference 34, prior works, reference 40, 54, 6, 2, 8, 75, 79, 64, mainly address this task with mask classification, where a set of bounding boxes and binary masks are predicted. Panoptic segmentation seeks for holistic scene understanding including both stuff and things. The pioneering work, reference 42, and prevalent ones, reference 53, 41, 82, 17, 48, 77, 13, decompose the problem into various proxy tasks and merge the results in the end. Recently, following Dieter, reference 7, most works, reference 78, 72, 18, 19, 49, 88, 89, 37, 47, present end-to-end solutions based on the idea of mask classification. Standing on their shoulders, our proposed method builds on top of the pixel decoder and mask decoder of mask 2 former, reference 19, by additionally exploiting the open vocabulary recognition ability from CLIP, reference 66. Open vocabulary segmentation aims at segmenting arbitrary classes including those that can not be accessed during the training procedure. Pryors works, reference 46, 28, 85, 50, 23, 83, 96, 86, 99, 60, 97, perform open vocabulary semantic segmentation through leveraging large pre-trained vision language models, reference 66, 38, 68. Recently, mask CLIP, reference 24, presents a two-stage pipeline, which consists of a class-agnostic mask generator and a frozen CLIP, reference 66, encoder for cross-modal alignment and thus expands the scope of the CLIP models into open vocabulary panoptic segmentation. ODISE, reference 84, digs out the innate potential of pre-trained text-image diffusion models, reference 68, in terms of the ability to present open concepts in the representation space for performing strong open vocabulary panoptic segmentation. Free Seg, reference 65, encodes multigranularity concepts into a compact textural abstraction, enabling generalizability to arbitrary text description. Unlike those methods, we propose a single-stage framework by exploiting a single frozen convolutional CLIP backbone, resulting in a simpler, faster, and stronger model than existing works. 3. Method In our first-stage segmentation, we first define the problem of open vocabulary segmentation. We then introduce the existing two-stage pipeline, followed by our proposed single-stage framework FCCLIP. Problem definition Open vocabulary segmentation aims to segment the image I element of R-H \u00d7 W \u00d7 3 into a set of masks with associated semantic labels, the K-Ground Truth masks M-I element of 0, 1. H \u00d7 W contain the corresponding Ground Truth class label C-I. During training, a fixed set of class labels C-TRAIN is used, while during inference, another set of categories C-TEST is used. In the open vocabulary setting, C-TEST may contain novel categories unseen during training, i.e., C-TRAIN equals C-TEST. We follow previous works, reference 24, 84, and assume the availability of the category names of C-TEST, represented in natural language, during testing. Although this framework has achieved impressive open vocabulary segmentation performance, it has two limitations. First, the image features are extracted twice, once for mask generation and the other for mask classification. The double-feature extractions incur heavy computation, making it costly to scale up backbone parameters. Second, the mask generator often requires high-resolution inputs, e.g., 1024 \u00d7 1024, whereas the CLIP model is usually pre-trained with lower-resolution images, e.g., 224 \u00d7 224. The two-stage pipeline thus needs to feed high-resolution images into the mask generator and low-resolution images into the CLIP classifier, making the model inefficient. Naive single-stage open vocabulary segmentation to avoid increasing the model size and computational cost of duplicate feature extractions, one may naively formulate everything together into a single-stage framework F, where both mask generator and mask classifier share the same CLIP pre-trained backbone CLIP, not frozen, for extracting features from an input image I. However, we empirically discover that fine-tuning this naive single-stage framework causes a misalignment between image and text features in the pre-trained CLIP model, leading to sub-optimal performance, especially for novel unseen classes. It also increases the training costs by 2.1 times to 52.8 GPU days. Interestingly, our experiments also show that a frozen CLIP backbone can provide sufficient features for mask generation, while preserving the image-text-aligned representation. Nevertheless, we still face another challenge, where CLIP models are usually pre-trained on low-resolution images, e.g., 224x224, whereas segmentation models prefer higher-resolution inputs, e.g., 800x1333 for Cocoa or 1024x for Cityscapes. This discrepancy results in the significant performance degradation when applying a frozen CLIP on large input images. Digging into the details, we found that it is related to the popular VIT reference 25 backbone used in CLIP that does not transfer well to different input sizes, which could be alleviated by extra careful designs, e.g., side adapter reference 16, 86, or cost aggregation reference 96, 20. On the other hand, CNN-based CLIP models, such as ResNet reference 33 and ConvNext reference 56, exhibit better generalization ability to different input sizes, due to their fully convolutional nature, reference 58. Additionally, the CNN-based CLIP backbone, extracting multi-scale feature maps, can be used as a simple plug-in module into modern closed vocabulary segmentation models, reference 19, 89. Motivated by the observations, we thus propose FC-CLIP, a simple yet effective single-stage open vocabulary segmentation framework built entirely on a single frozen convolutional CLIP backbone CLIP asterisk operator CNN, FC-CLIP The proposed FC-CLIP leverages the semantic features of a frozen CNN-based CLIP backbone for both mask generation and CLIP classification. Unlike previous works, reference 85, 50, 24, 84, which often train a separate mask generator and ignore the potential reuse of CLIP's semantic features, we incorporate the CNN-based CLIP backbone into the state-of-the-art segmentation method MASK2 former, reference 19. We note that FC-CLIP is a general meta-architecture that can build on top of several modern segmentation methods, reference 19, 89. Our approach offers several advantages. By freezing and sharing the backbone features, our model is significantly more efficient during both training and testing, i.e., avoiding feature duplication. The CNN-based CLIP backbone not only transfers well to different input resolutions, from its pre-trained image size, but also generates multi-scale feature maps, seamlessly compatible with modern segmentation methods, reference 19, 89. At a high level, FC-CLIP consists of three components, class-agnostic mask generator, in vocabulary classifier, and out-of-vocabulary classifier. We detail each component below, class-agnostic mask generator following MASK2 former, reference 19. We use a pixel decoder enhanced with multi-scale deformable attention, reference 98, to improve the features extracted from the frozen CNN-based CLIP backbone. The enhanced pixel features, together with a set of object queries, reference 7, 78, are then passed through a series of mask decoders, where each consists of masked cross-attention, reference 19, self-attention, reference 76, and a feed-forward network. The resulting segmentation logits are obtained by performing a matrix multiplication between the object query and pixel features. The predicted masks are matched with ground-truth masks in a one-to-one manner through Hungarian matching, reference 43, and are supervised accordingly. Moreover, as the number of object queries is often greater than the number of labeled masks, only a subset of predicted masks are optimized through this matching process. We apply no penalty to the remaining unmatched proposals, which ensures that more mask proposals are obtained. In vocabulary classifier once the mask proposals are predicted, they are classified with category text embedding in a contrastive manner, where the class embeddings for each mask and category are projected into a common embedding space. That is, the predicted class probability by in vocabulary classifier is defined as follows. For all i equals 1, n where T is a learnable temperature parameter with initialization of 0.07 to control the sharpness of the distribution, cos is cosine distance measurement, vi is the class embeddings for i-th predicted mask, which is obtained by mask pooling over the final pixel features from pixel decoder, similar to, reference 28. Tj is the category name's text embeddings of class j, which is obtained by feeding the category name to a clip pre-trained text encoder. Note that these category text embeddings only need to be generated once. They are then kept in memory to serve as text classifiers, and thus it incurs negligible additional cost during training. This forms our in vocabulary classifier. Out of vocabulary classifier during inference, however, we notice that using the in vocabulary classifier alone fails to generalize to completely novel unseen classes, as the model is only trained on a finite set of categories and thus could not recognize diverse novel concepts. To address this issue, we introduce an out of vocabulary classifier, which applies mask pooling to the frozen clip backbone features, aiming to borrow the pre-trained, intact, open vocabulary recognition ability from clip. Unlike the other two stage methods, reference 85, 50, 24, 84, where one or multiple forward processes of clip are needed, the adopted out of vocabulary classifier introduces marginal additional costs, since the backbone features are already extracted, and only lightweight mask pooling is performed. The predicted class probability by out of vocabulary classifier chi, out is then obtained in a manner similar to eq. 6. By replacing vi with the mask pooled features over frozen clip backbone features. This classifier strictly maintains the original clipped feature distribution, allowing us brand new categories. Note that the out of vocabulary classifier is only performed during testing. Combining in and out of vocabulary classifiers following prior works, reference 30, 28, 44, 84, we employ geometric ensemble to fuse the classification scores between in vocabulary and out of vocabulary classifiers. That is, for all j equals 1, c, waric i, j denotes the jth element ofci, and the underscripts in and out referred to in vocabulary and out of vocabulary classifier, respectively. Alpha, beta element of, reference 0, 1, balance the predictions between in and out of vocabulary classifiers for seen and novel unseen categories. 4. Experimental results. Herein, we provide implementation details of FC clip in sec. 4.1. After setting the stage, we introduce our main results, compared with state-of-the-art methods and ablation studies in sec. 4.2. 4.1 Implementation details architecture We use Convnext Large Clip, reference 56, 66, Backbones from Open Clip, reference 36, one pre-trained on LAION2B, reference 70, dataset. On top of the clip backbone, we build the mask generator, following mask 2 former, reference 19. Nine mask decoders are employed to generate the class agnostic masks by taking as inputs the enhanced pixel features in a set of object queries. For in vocabulary classification, following, reference 28, the class embeddings are obtained by mask pooling the pixel features from the pixel decoder's final output. Afterwards, the classification logits, before softmax, is obtained by matrix multiplication between the predicted class embeddings and categories text embeddings. Training strategy We follow, reference 19, and adopt the same training recipe and losses without any special design. The training is optimized with Atom W, reference 39, 59, Optimizer and Weight Decay 0.05. We use a crop size of 1024\u00d71024. We employ the learning rate 1\u00d710-4 and a multi-step decay schedule. The training batch size is 16, and the model is trained for 50 epochs on Cocoa Panoptic Training Set, reference 52. Inference strategy During inference, the shorted side of input images will be resized to 800 while ensuring longer side not exceeds 1333. For cityscapes and mappillary vistas, we increase the shorter side size to 1024. We adopt mask-wise merging scheme, reference 19, for the mask predictions. The out-of-vocabulary classifier is only performed during inference by mask pooling over the frozen-clip backbone features. The final classification results are then obtained by geometric ensembling in-and-out-of-vocabulary classifiers, reference 30, 28, 44, 84, as in EQ. 7, where we default \u03b1 equals 0.4 and \u03b2 equals 0.8. Following prior arts, we also adopt prompt engineering from, reference 28, 84, and prompt templates from, reference 30, 50. If not specified, FC-CLIP is only trained on Cocoa Panoptic Dataset, reference 52. Following prior works, reference 28, 84, we zero-shot evaluate the model on ADE20K, reference 95, cityscapes, reference 21, and mappillary vistas, reference 62, for open-vocabulary panoptic segmentation. We also report open-vocabulary semantic segmentation results on those datasets along with Pascal datasets, reference 26, 61. The panoptic segmentation results are evaluated with the panoptic quality, PQ, reference 42, average precision, AP, and mean intersection over union, MIOU, and semantic segmentation is evaluated with MIOU, reference 26. Note that all results are obtained with the same single checkpoint trained on Cocoa Panoptic Data only. 4.2. Results. We summarize the main results for open-vocabulary panoptic segmentation and semantic segmentation in tab. 1, tab. 2 in tab. 3, where we train FC clip on Cocoa trains set with panoptic annotation and evaluated on various datasets in a zero-shot manner. Open-vocabulary panoptic segmentation evaluation on ADE20K in tab. 1, we compare our FC clip with other state-of-the-art methods on ADE20K, reference 95, the main testbed of zero-shot open-vocabulary panoptic segmentation. As shown in the table, our method achieves significantly better performance compared to mask clip, reference 24, with plus 11.7pq, plus 10.8 AP and plus 10.4 MIOU, even though we use fewer frozen, minus 66m, and trainable, minus 42m, parameters. When compared to the concurrent methods free seg, reference 65, and ODISE, reference 84, the advantage of FC clip persists. FC clip is plus 10.5pq, plus 10.3 AP, and plus 9.5 MIOU better than free seg without using Cocoa stuff annotations, reference 5, which contains more semantic classes than Cocoa panoptic. Our pq, AP, MIOU score are also plus 4.2, plus 2.4, plus 4.2 higher than ODISE under the same training settings. Compared to ODISE with caption, reference 14, for supervision, our model still outperforms it by plus 3.4pq, setting a new state of the art record. Meanwhile, it is noticeable that our model has 6.3 times, 5.9 times, significantly fewer frozen, total, parameters compared to ODISE, which utilizes a strong large backbone from stable diffusion, reference 68, for feature extraction. Open vocabulary panoptic segmentation evaluation on street view datasets in tab 2, we evaluate on cityscapes and mappaleri vistas, which focus on street driving scenes. Compared to state of the art method ODISE, FC clip achieves better performances on both datasets. Specifically, it outperforms ODISE by plus 4.0pq and plus 20.1pq on mappaleri vistas and cityscapes, respectively. Notably, FC clip has a slightly lower SQ, which indicates our mask generator is actually weaker than the one in ODISE, which utilizes a much larger backbone. Open vocabulary semantics segmentation evaluation Although our model was trained on cocoa panoptic data only, it also performs well on open vocabulary semantic segmentation. In tab 3, we report our model's performance on various benchmarks against other open vocabulary segmentation models, where FC clip shows an overall superior performance. Specifically, with the same training annotations used, FC clip outperforms mask clip by plus 6.6, plus 8.2, plus 10.4, plus 12.5 miou across A847, PC459, A150, and PC59, respectively. Compared to methods with caption annotations, FC clip persists its advantages, where it outperforms ODISE, caption, by plus 3.8, plus 4.4, plus 5.4, plus 3.1 miou across datasets A847, PC459, A150, PC59, respectively. Against other open vocabulary semantic segmentation methods, our model maintains its advantages across different datasets, despite being trained solely with panoptic annotations. Furthermore, it demonstrates comparable performance to state-of-the-art open vocabulary semantic segmentation methods, which utilize the Cocoa Stuff dataset as their training set. The Cocoa Stuff dataset comprises 171 classes, 38 more classes than Cocoa Panoptic, and offers highly desirable annotations for semantic segmentation tasks. It is worth mentioning that these methods build their approach on top of VIT-L, with extra designs, reference 86, resulting in a significantly larger model size compared to our deployed Convinext-L, 304M vs. 198M. Despite the disparity in model size, FC clip remains competitive in terms of performance. Specifically, FC clip outperforms state-of-the-art open vocabulary semantic segmentation methods SAN, reference 86, by 1.1 and 1.1 miou on the challenging A847 and PC459 datasets. Inference speed we provide a comparison of FPS, frames per second, in tab.4. The proposed FC clip not only demonstrates superior performances, but also enjoys a significant fast inference time. FC clip runs 6.61 times and 7.08 times faster than ODISE evaluated on ADE 20K and Cocoa datasets, respectively. Training on ADE 20K and evaluating on Cocoa we further validate the effectiveness of FC clip by using a different training dataset. Specifically, we follow reference 65, 84, to train our model on ADE 20K dataset with panoptic annotation, and evaluate it on Cocoa panoptic dataset. As shown in tab.5, FC clip outperforms FreeSeg, reference 65, by plus 10.5pq, and ODISE, reference 84, by plus 2.0pq on Cocoa dataset. Notably, our model actually has a lower SQ, minus 1.4, compared to ODISE, which utilizes a much larger backbone and thus has a stronger mask generator. Nevertheless, FC clip still outperforms ODISE significantly with a simple yet effective design. Fine tuning clip backbone harms performance on novel vocabularies we validate the necessity of freezing clip backbone to ensure a better generalization to novel vocabularies. We compare the performance of trainable clip variant and frozen clip variant in Fig.4, where we use the same mask proposals to ensure a fair comparison. Specifically, we compare the performance on 10 scene classes, which are shared by both Cocoa and ADE20K, e.g., Person, Sky, and 10 unseen classes, which are only included in ADE20K dataset, e.g., Arcade Machine, Dishwasher. As shown in the figure, tuning clip backbone leads to a worse performance on unseen concepts, which breaks the clip feature alignment and thus loses its recognition ability on a much wider vocabulary. 5. Conclusion. In this work, we have presented FC clip, a simple yet effective single stage framework for open vocabulary segmentation. FC clip shows great potential by building everything on top of a shared frozen convolutional clip backbone, which not only significantly reduces training and testing costs, but also establishes a strong baseline on multiple benchmarks. Our study demonstrates how to better adapt a pre-trained clip model for downstream dense prediction tasks, which we hope will shed the light on unleashing CLIP's potential for other various downstream tasks. The Clip's FC clip presents a simple single stage open vocabulary segmentation framework with state-of-the-art performance. We note that there exists some interesting research topics to be explored in the near future, such as better unleashing CLIP's potential in both mask segmentation and classification, how to deal with conflict or overlapping vocabularies, e.g., cat vs. cathead, etc. Broader Impact FC clip shows great potential for segmenting and naming every object in the scene, which could facilitate many applications including intelligent home assistance, robots, self-driving, etc. Yet it relies on CLIP model pre-trained on the internet data that may be biased, which calls for future research for calibration to avoid misuse. Appendix in the following supplementary materials, we present additional experimental results pertaining to the design of FC clip. Our supplementary analysis also includes comparisons against other methods that specifically address vocabulary semantic segmentation, ensemble methods, and hyperparameter tuning. Furthermore, we provide a quantitative comparison between VIT-based CLIP and CNN-based CLIP across varying input sizes, along with additional visualizations and comprehensive dataset details. Six additional experimental results fine tuning or freezing CLIP backbone in FC clip in this study, we provide a comprehensive analysis of the impact of fine tuning or freezing the CLIP backbone in our framework. We specifically focus on the PQ scene and PQ unseen metrics, which evaluate the performance for classes that overlap and do not overlap between the training and testing datasets, respectively. To determine whether a class is seen or unseen, we adopt the prompt engineering technique described in reference 28, which provides synonyms or subcategories of classes. Specifically, if any category name in test dataset overlaps with a category name in training dataset, we consider it as a seen class, otherwise unseen. As discussed in the main paper, the proposed FC CLIP contains three components, a class agnostic mask generator, an invocabulary classifier, and an out-of-vocabulary classifier. We thus explore using frozen or trainable CLIP for each component, and summarize the results in tab. 6. To ensure a fair comparison, all, trainable, modules utilize the same weights, resulting in identical mask proposals and invocabulary classification results. Our findings reveal that an invocabulary classifier built upon a trainable CLIP backbone achieves a higher PQ scene score, 37.9 compared to 32.4, but experiences a decrease in PQ unseen, 2.6 compared to 12.6, compared to a frozen out-of-vocabulary classifier. Consequently, a model that incorporates a trainable CLIP backbone for all components yields a PQ of 24.1, which is 2.7 lower than our final model, last row, that relies on a single frozen CLIP backbone. Using a trainable mask generator and invocabulary classifier, along with a frozen out-of-vocabulary classifier boosts the performance but requires maintaining one trainable and one frozen CLIP weights, resulting in two times more backbone parameters. In summary, our observations demonstrate that building the entire framework upon a frozen CLIP backbone is not only effective but also efficient, providing a better balance between PQ scene and PQ unseen metrics. In combination with grounding PQ and grounding MIOU it is worth emphasizing that despite the absence of grounding loss, reference 31, 92, 28, 84, during training, our model exhibits exceptional grounding segmentation capabilities. Tab 7 presents the grounding PQ and grounding MIOU scores of FCCLIP, following the evaluation methodology outlined in, reference 28. In this evaluation, we exclusively employ ground truth classes as text query inputs to assess the effectiveness of concept grounding. Compared to OpenSEG, reference 28, FCCLIP achieves a substantial performance improvement, with notable enhancements of plus 11.6, plus 9.1, plus 13.1, and plus 17.7 on A847, PC459, A150, and PC59, respectively. Even when compared to OpenSEG trained with a localized narrative dataset, reference 63, which enables training on a significantly larger vocabulary, FCCLIP still surpasses it with improvements of plus 8.0, plus 2.2, plus 8.6, and plus 13.4 on A847, PC459, A150, and PC59, respectively, underscoring the grounding proficiency of FCCLIP. Ensemble in vocabulary and out of vocabulary classifiers in Tab. 8. We present experiments conducted to evaluate the impact of ensemble methods and ensemble parameters on the performance of the in vocabulary and out of vocabulary classifiers. Specifically, we examine two ensemble methods, arithmetic and geometric. The arithmetic method involves a linear combination of the in vocabulary classifier and the out of vocabulary classifier, while the geometric method is defined as shown in equation 7 of main paper. It is worth noting that FCCLIP exhibits robustness to different ensemble methods, with both methods displaying a consistent trend within the explored hyperparameter ranges. However, the geometric ensemble consistently outperforms the arithmetic ensemble by a slight margin. Additionally, we observe that preference is given to values of \u03b1 is less than or equal to 0.5 and \u03b2 is greater than or equal to 0.5, which biases the model towards using the in vocabulary classifier for seen classes and the out of vocabulary classifier for unseen classes. We also explore extreme cases, including \u03b1 equals 0.0 and \u03b2 equals 0.0, i.e., exclusively utilizing the in vocabulary classifier for every class, \u03b1 equals 1.0 and \u03b2 equals 1.0, i.e., exclusively utilizing the out of vocabulary classifier for every class, \u03b1 equals 0.0 and \u03b2 equals 1.0, i.e., using the in vocabulary classifier for seen classes and the out of vocabulary classifier for unseen classes, and \u03b1 equals 1.0 and \u03b2 equals 0.0, i.e., using the out of vocabulary classifier for seen classes and the in vocabulary classifier for unseen classes. The results align with our observations that it is preferable to bias towards the in vocabulary classifier for seen classes and the out of vocabulary classifier for unseen classes. Quantitative VIT-based clip versus CNN-based clip when input size scales training our model solely with VIT-based clip, without any additional modifications, reference 96, 16, 86, 20, is infeasible. Furthermore, applying VIT to large input sizes is computationally expensive. Therefore, to evaluate the effects of using VTOR CNN-based clip in our framework, we incorporate them into our out of vocabulary classifier, which is performed only during inference. To ensure a fair comparison, we use the same MASC proposals and disable the geometric ensemble scheme. In tab 9, we conduct an ablation study to analyze the impact of different input resolutions for clip models. We consider both VIT-based, VIT-L, 14, and CNN-based, Convinext-L, clip models. By employing them as zero-shot MASC classifiers and varying the input resolutions, we observe that CNN-based clip demonstrates superior generalization ability as the input size scales up. Specifically, we observe that the VIT-L, 14 clip has a higher PQ at a lower resolution, i.e., input size 224, but suffers from a higher resolution, which leads existing two-stage methods, reference 85, 50, 24, 86, 84, to adopt different input resolutions for MASC generator and classifier branches. On the contrary, FC-clip provides a simple solution by adopting a CNN-based clip that generalizes well to different input sizes. Visualization We provide visualization on ADE20K val set in Fig. 5. 7 datasets information and licenses The datasets we used for training and or testing FC-clip are described as follows. Cocoa We train FC-clip on cocoa data with panoptic annotation, reference 52. We follow the 2017 splits which include 118K images for train split and 5K images for val split. If not specified, we train our model on the cocoa train split and report results on val set of various datasets. License Creative Commons Attribution 4.0 License URL https://cocodataset.org/.home-ade20k ADE20K, reference 95, covers a wide range of indoor and outdoor scenes, with 2K val images. We evaluate FC-clip on both the version with 847 classes, A847, and the more widely used version with 150 frequent categories, A150. License Creative Commons BSD3 License URL https://groups.csale.mit.edu/.vision/.dataset/.ade20k/.citiescapes.citiescapes.reference21.focuses on semantic understanding of urban street scenes. We use the fine data includes 500 images for validation set. License This dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. URL https://www.citiescapes-dataset.com/.mapillaryvistas.mapillaryvistas.reference62 is a large-scale traffic-related dataset, including 2K images for validation purposes. License Creative Commons Attribution Non-Commercial Share-Alike. We evaluate FC-clip on both its full version, PC459, with 459 classes and the more common version, PC59, with 59 classes. URL https://www.cs.stanford.edu/.ruseba/.pascal-context/.pascalvoc.pascalvoc.reference26.contains1.5Kval images with 20 foreground classes and 1 background class. Due to the ambiguity in definition of background, we assign the background class to the pixels predicted as PC59 categories that are not in Pascal VOC following, Reference 28, which leads to PAS21. We also evaluate the model with background class excluded, which leads to PAS20. URL https://host.robots.ox.ac.uk/.pascal/.voc/. Thanks for listening to this reading. For the entire paper, and more, check out our homepage, papersread.ai.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 18.06, "text": " Papers read on AI, with Rob, keeping you up to date with the latest research.", "tokens": [50364, 430, 14441, 1401, 322, 7318, 11, 365, 5424, 11, 5145, 291, 493, 281, 4002, 365, 264, 6792, 2132, 13, 51267], "temperature": 0.0, "avg_logprob": -0.22645836956096146, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.1516686975955963}, {"id": 1, "seek": 0, "start": 18.06, "end": 23.16, "text": " This reading is brought to you by Mars Race, Stake a Claim on the Red Planet, available", "tokens": [51267, 639, 3760, 307, 3038, 281, 291, 538, 9692, 25908, 11, 745, 619, 257, 383, 10970, 322, 264, 4477, 22146, 11, 2435, 51522], "temperature": 0.0, "avg_logprob": -0.22645836956096146, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.1516686975955963}, {"id": 2, "seek": 0, "start": 23.16, "end": 27.36, "text": " on Android and iOS.", "tokens": [51522, 322, 8853, 293, 17430, 13, 51732], "temperature": 0.0, "avg_logprob": -0.22645836956096146, "compression_ratio": 1.3028169014084507, "no_speech_prob": 0.1516686975955963}, {"id": 3, "seek": 2736, "start": 27.36, "end": 32.32, "text": " Convolutions Die Hard \u2013 Open vocabulary segmentation with single frozen convolutional", "tokens": [50364, 2656, 85, 15892, 3229, 11817, 1662, 7238, 19864, 9469, 399, 365, 2167, 12496, 45216, 304, 50612], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 4, "seek": 2736, "start": 32.32, "end": 33.32, "text": " clip.", "tokens": [50612, 7353, 13, 50662], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 5, "seek": 2736, "start": 33.32, "end": 41.2, "text": " Authored 2023 by Qihang Yu, Zhu He, Xueqing Deng, Shao Weisheng, Liang Che Chen.", "tokens": [50662, 20216, 292, 44377, 538, 21430, 23850, 10767, 11, 31680, 634, 11, 43999, 40055, 413, 1501, 11, 14944, 78, 492, 742, 1501, 11, 35842, 3351, 13682, 13, 51056], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 6, "seek": 2736, "start": 41.2, "end": 45.8, "text": " Abstract \u2013 Open vocabulary segmentation is a challenging task requiring segmenting and", "tokens": [51056, 46853, 1897, 1662, 7238, 19864, 9469, 399, 307, 257, 7595, 5633, 24165, 9469, 278, 293, 51286], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 7, "seek": 2736, "start": 45.8, "end": 50.32, "text": " recognizing objects from an open set of categories in diverse environments.", "tokens": [51286, 18538, 6565, 490, 364, 1269, 992, 295, 10479, 294, 9521, 12388, 13, 51512], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 8, "seek": 2736, "start": 50.32, "end": 54.92, "text": " One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide", "tokens": [51512, 1485, 636, 281, 2985, 341, 3430, 307, 281, 13982, 4825, 12, 8014, 304, 5245, 11, 1270, 382, 12855, 9139, 11, 281, 2893, 51742], "temperature": 0.0, "avg_logprob": -0.24364944673934072, "compression_ratio": 1.5409252669039146, "no_speech_prob": 0.6781833171844482}, {"id": 9, "seek": 5492, "start": 54.92, "end": 59.480000000000004, "text": " image and text features in a shared embedding space, which effectively bridges the gap between", "tokens": [50364, 3256, 293, 2487, 4122, 294, 257, 5507, 12240, 3584, 1901, 11, 597, 8659, 21114, 264, 7417, 1296, 50592], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 10, "seek": 5492, "start": 59.480000000000004, "end": 62.72, "text": " closed vocabulary and open vocabulary recognition.", "tokens": [50592, 5395, 19864, 293, 1269, 19864, 11150, 13, 50754], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 11, "seek": 5492, "start": 62.72, "end": 67.56, "text": " Hence, existing methods often adopt a two-stage framework to tackle the problem, where the", "tokens": [50754, 22229, 11, 6741, 7150, 2049, 6878, 257, 732, 12, 17882, 8388, 281, 14896, 264, 1154, 11, 689, 264, 50996], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 12, "seek": 5492, "start": 67.56, "end": 71.08, "text": " inputs first go through a mask generator and then through the clip model along with the", "tokens": [50996, 15743, 700, 352, 807, 257, 6094, 19265, 293, 550, 807, 264, 7353, 2316, 2051, 365, 264, 51172], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 13, "seek": 5492, "start": 71.08, "end": 72.88, "text": " predicted masks.", "tokens": [51172, 19147, 11830, 13, 51262], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 14, "seek": 5492, "start": 72.88, "end": 76.92, "text": " This process involves extracting features from raw images multiple times, which can", "tokens": [51262, 639, 1399, 11626, 49844, 4122, 490, 8936, 5267, 3866, 1413, 11, 597, 393, 51464], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 15, "seek": 5492, "start": 76.92, "end": 79.04, "text": " be ineffective and inefficient.", "tokens": [51464, 312, 48836, 293, 43495, 13, 51570], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 16, "seek": 5492, "start": 79.04, "end": 83.36, "text": " By contrast, we propose to build everything into a single stage framework using a shared", "tokens": [51570, 3146, 8712, 11, 321, 17421, 281, 1322, 1203, 666, 257, 2167, 3233, 8388, 1228, 257, 5507, 51786], "temperature": 0.0, "avg_logprob": -0.09030291464476459, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.7114337682723999}, {"id": 17, "seek": 8336, "start": 83.4, "end": 87.72, "text": " frozen convolutional clip backbone, which not only significantly simplifies the current", "tokens": [50366, 12496, 45216, 304, 7353, 34889, 11, 597, 406, 787, 10591, 6883, 11221, 264, 2190, 50582], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 18, "seek": 8336, "start": 87.72, "end": 92.72, "text": " two-stage pipeline, but also remarkably yields a better accuracy cost tradeoff.", "tokens": [50582, 732, 12, 17882, 15517, 11, 457, 611, 37381, 32168, 257, 1101, 14170, 2063, 4923, 4506, 13, 50832], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 19, "seek": 8336, "start": 92.72, "end": 97.32, "text": " The resulting single stage system, called FC-CLIP, benefits from the following observations", "tokens": [50832, 440, 16505, 2167, 3233, 1185, 11, 1219, 27168, 12, 22458, 9139, 11, 5311, 490, 264, 3480, 18163, 51062], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 20, "seek": 8336, "start": 97.32, "end": 101.88, "text": " \u2013 the frozen clip backbone maintains the ability of open vocabulary classification", "tokens": [51062, 1662, 264, 12496, 7353, 34889, 33385, 264, 3485, 295, 1269, 19864, 21538, 51290], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 21, "seek": 8336, "start": 101.88, "end": 106.32, "text": " and can also serve as a strong mask generator, and the convolutional clip generalizes well", "tokens": [51290, 293, 393, 611, 4596, 382, 257, 2068, 6094, 19265, 11, 293, 264, 45216, 304, 7353, 2674, 5660, 731, 51512], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 22, "seek": 8336, "start": 106.32, "end": 110.76, "text": " to a larger input resolution than the one used during contrastive image-text pre-training.", "tokens": [51512, 281, 257, 4833, 4846, 8669, 813, 264, 472, 1143, 1830, 8712, 488, 3256, 12, 25111, 659, 12, 17227, 1760, 13, 51734], "temperature": 0.0, "avg_logprob": -0.1371599919087178, "compression_ratio": 1.6284829721362228, "no_speech_prob": 0.816771924495697}, {"id": 23, "seek": 11076, "start": 111.44000000000001, "end": 116.08000000000001, "text": " Surprisingly, FC-CLIP advances state-of-the-art results on various benchmarks while running", "tokens": [50398, 49908, 11, 27168, 12, 22458, 9139, 25297, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 3683, 43751, 1339, 2614, 50630], "temperature": 0.0, "avg_logprob": -0.1998040771484375, "compression_ratio": 1.339366515837104, "no_speech_prob": 0.10963057726621628}, {"id": 24, "seek": 11076, "start": 116.08000000000001, "end": 117.80000000000001, "text": " practically fast.", "tokens": [50630, 15667, 2370, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1998040771484375, "compression_ratio": 1.339366515837104, "no_speech_prob": 0.10963057726621628}, {"id": 25, "seek": 11076, "start": 117.80000000000001, "end": 122.92, "text": " Specifically, when training on Cocoa panoptic data only and testing in a zero-shot manner,", "tokens": [50716, 26058, 11, 562, 3097, 322, 29787, 64, 2462, 5747, 299, 1412, 787, 293, 4997, 294, 257, 4018, 12, 18402, 9060, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1998040771484375, "compression_ratio": 1.339366515837104, "no_speech_prob": 0.10963057726621628}, {"id": 26, "seek": 11076, "start": 122.92, "end": 135.92000000000002, "text": " FC-CLIP achieved 26.8pq, 16.8ap, and 34.1miou on ADE20K, 18.2pq, 27.9miou on Mapillary Vista's,", "tokens": [50972, 27168, 12, 22458, 9139, 11042, 7551, 13, 23, 79, 80, 11, 3165, 13, 23, 569, 11, 293, 12790, 13, 16, 3057, 263, 322, 9135, 36, 2009, 42, 11, 2443, 13, 17, 79, 80, 11, 7634, 13, 24, 3057, 263, 322, 22053, 46367, 691, 5236, 311, 11, 51622], "temperature": 0.0, "avg_logprob": -0.1998040771484375, "compression_ratio": 1.339366515837104, "no_speech_prob": 0.10963057726621628}, {"id": 27, "seek": 13592, "start": 135.92, "end": 143.92, "text": " and 34.0pq, 26.8ap, 56.2miou on Cityscapes, outperforming the prior art under the same", "tokens": [50364, 293, 12790, 13, 15, 79, 80, 11, 7551, 13, 23, 569, 11, 19687, 13, 17, 3057, 263, 322, 4392, 44239, 5190, 11, 484, 26765, 278, 264, 4059, 1523, 833, 264, 912, 50764], "temperature": 0.0, "avg_logprob": -0.0851627904125768, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.20676158368587494}, {"id": 28, "seek": 13592, "start": 143.92, "end": 154.16, "text": " setting by plus 4.2pq, plus 2.4ap, plus 4.2miou on ADE20K, plus 4.0pq on Mapillary Vista's", "tokens": [50764, 3287, 538, 1804, 1017, 13, 17, 79, 80, 11, 1804, 568, 13, 19, 569, 11, 1804, 1017, 13, 17, 3057, 263, 322, 9135, 36, 2009, 42, 11, 1804, 1017, 13, 15, 79, 80, 322, 22053, 46367, 691, 5236, 311, 51276], "temperature": 0.0, "avg_logprob": -0.0851627904125768, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.20676158368587494}, {"id": 29, "seek": 13592, "start": 154.16, "end": 157.83999999999997, "text": " and plus 20.1pq on Cityscapes, respectively.", "tokens": [51276, 293, 1804, 945, 13, 16, 79, 80, 322, 4392, 44239, 5190, 11, 25009, 13, 51460], "temperature": 0.0, "avg_logprob": -0.0851627904125768, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.20676158368587494}, {"id": 30, "seek": 13592, "start": 157.83999999999997, "end": 163.79999999999998, "text": " Additionally, the training and testing time of FC-CLIP is 7.5 times and 6.6 times significantly", "tokens": [51460, 19927, 11, 264, 3097, 293, 4997, 565, 295, 27168, 12, 22458, 9139, 307, 1614, 13, 20, 1413, 293, 1386, 13, 21, 1413, 10591, 51758], "temperature": 0.0, "avg_logprob": -0.0851627904125768, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.20676158368587494}, {"id": 31, "seek": 16380, "start": 163.8, "end": 169.20000000000002, "text": " faster than the same prior art, while using 5.9 times fewer total model parameters.", "tokens": [50364, 4663, 813, 264, 912, 4059, 1523, 11, 1339, 1228, 1025, 13, 24, 1413, 13366, 3217, 2316, 9834, 13, 50634], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 32, "seek": 16380, "start": 169.20000000000002, "end": 174.56, "text": " Meanwhile, FC-CLIP also sets a new state-of-the-art performance across various open vocabulary", "tokens": [50634, 13879, 11, 27168, 12, 22458, 9139, 611, 6352, 257, 777, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 2108, 3683, 1269, 19864, 50902], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 33, "seek": 16380, "start": 174.56, "end": 177.24, "text": " semantic segmentation datasets.", "tokens": [50902, 47982, 9469, 399, 42856, 13, 51036], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 34, "seek": 16380, "start": 177.24, "end": 181.12, "text": " Code will be available at https://github.com/.bytedance/.fc-clip.", "tokens": [51036, 15549, 486, 312, 2435, 412, 34426, 21492, 70, 355, 836, 13, 1112, 48550, 2322, 14727, 719, 48550, 69, 66, 12, 21614, 13, 51230], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 35, "seek": 16380, "start": 181.12, "end": 182.76000000000002, "text": " 1.", "tokens": [51230, 502, 13, 51312], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 36, "seek": 16380, "start": 182.76000000000002, "end": 186.56, "text": " Introduction.", "tokens": [51312, 27193, 882, 13, 51502], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 37, "seek": 16380, "start": 186.56, "end": 191.04000000000002, "text": " Panoptic Segmentation, reference 42, is a complex computer vision task that aims to", "tokens": [51502, 7557, 5747, 299, 1100, 10433, 399, 11, 6408, 14034, 11, 307, 257, 3997, 3820, 5201, 5633, 300, 24683, 281, 51726], "temperature": 0.0, "avg_logprob": -0.21788417067483207, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.42964306473731995}, {"id": 38, "seek": 19104, "start": 191.04, "end": 195.67999999999998, "text": " predict a set of non-overlapping masks, each with its corresponding class label.", "tokens": [50364, 6069, 257, 992, 295, 2107, 12, 3570, 15639, 11830, 11, 1184, 365, 1080, 11760, 1508, 7645, 13, 50596], "temperature": 0.0, "avg_logprob": -0.11353661673409599, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.7179903984069824}, {"id": 39, "seek": 19104, "start": 195.67999999999998, "end": 201.23999999999998, "text": " It combines the tasks of semantic segmentation, reference 35, and instance segmentation, reference", "tokens": [50596, 467, 29520, 264, 9608, 295, 47982, 9469, 399, 11, 6408, 6976, 11, 293, 5197, 9469, 399, 11, 6408, 50874], "temperature": 0.0, "avg_logprob": -0.11353661673409599, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.7179903984069824}, {"id": 40, "seek": 19104, "start": 201.23999999999998, "end": 204.92, "text": " 32, making it a challenging problem to solve.", "tokens": [50874, 8858, 11, 1455, 309, 257, 7595, 1154, 281, 5039, 13, 51058], "temperature": 0.0, "avg_logprob": -0.11353661673409599, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.7179903984069824}, {"id": 41, "seek": 19104, "start": 204.92, "end": 214.84, "text": " Many methods, reference 41, 82, 17, 78, 49, 88, 19, 89, 51, have been proposed to tackle", "tokens": [51058, 5126, 7150, 11, 6408, 18173, 11, 29097, 11, 3282, 11, 26369, 11, 16513, 11, 24587, 11, 1294, 11, 31877, 11, 18485, 11, 362, 668, 10348, 281, 14896, 51554], "temperature": 0.0, "avg_logprob": -0.11353661673409599, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.7179903984069824}, {"id": 42, "seek": 19104, "start": 214.84, "end": 220.04, "text": " this problem, and a significant progress has been made in terms of panoptic quality, pq.", "tokens": [51554, 341, 1154, 11, 293, 257, 4776, 4205, 575, 668, 1027, 294, 2115, 295, 2462, 5747, 299, 3125, 11, 280, 80, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11353661673409599, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.7179903984069824}, {"id": 43, "seek": 22004, "start": 220.04, "end": 225.35999999999999, "text": " However, due to the high cost of annotating such a fine-grained dataset, reference 52,", "tokens": [50364, 2908, 11, 3462, 281, 264, 1090, 2063, 295, 25339, 990, 1270, 257, 2489, 12, 20735, 2001, 28872, 11, 6408, 18079, 11, 50630], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 44, "seek": 22004, "start": 225.35999999999999, "end": 230.72, "text": " 21, the number of semantic classes is typically limited to a few dozens or hundreds.", "tokens": [50630, 5080, 11, 264, 1230, 295, 47982, 5359, 307, 5850, 5567, 281, 257, 1326, 18431, 420, 6779, 13, 50898], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 45, "seek": 22004, "start": 230.72, "end": 235.28, "text": " This restriction hinders the further application of existing approaches to real-world settings,", "tokens": [50898, 639, 29529, 20138, 433, 264, 3052, 3861, 295, 6741, 11587, 281, 957, 12, 13217, 6257, 11, 51126], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 46, "seek": 22004, "start": 235.28, "end": 238.44, "text": " where the number of possible semantic classes is unlimited.", "tokens": [51126, 689, 264, 1230, 295, 1944, 47982, 5359, 307, 21950, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 47, "seek": 22004, "start": 238.44, "end": 244.14, "text": " To overcome the limitations of closed vocabulary segmentation, open vocabulary segmentation,", "tokens": [51284, 1407, 10473, 264, 15705, 295, 5395, 19864, 9469, 399, 11, 1269, 19864, 9469, 399, 11, 51569], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 48, "seek": 22004, "start": 244.14, "end": 249.01999999999998, "text": " reference 46, 85, 28, 24, has been proposed.", "tokens": [51569, 6408, 17835, 11, 14695, 11, 7562, 11, 4022, 11, 575, 668, 10348, 13, 51813], "temperature": 0.0, "avg_logprob": -0.09165816037160046, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.09264596551656723}, {"id": 49, "seek": 24902, "start": 249.02, "end": 254.3, "text": " These approaches uses text embeddings of category names, reference 92, represented in natural", "tokens": [50364, 1981, 11587, 4960, 2487, 12240, 29432, 295, 7719, 5288, 11, 6408, 28225, 11, 10379, 294, 3303, 50628], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 50, "seek": 24902, "start": 254.3, "end": 258.8, "text": " language, as label embeddings, instead of learning them from the training dataset.", "tokens": [50628, 2856, 11, 382, 7645, 12240, 29432, 11, 2602, 295, 2539, 552, 490, 264, 3097, 28872, 13, 50853], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 51, "seek": 24902, "start": 258.8, "end": 263.82, "text": " By doing so, models can classify objects from a wider vocabulary, which improves their ability", "tokens": [50853, 3146, 884, 370, 11, 5245, 393, 33872, 6565, 490, 257, 11842, 19864, 11, 597, 24771, 641, 3485, 51104], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 52, "seek": 24902, "start": 263.82, "end": 266.5, "text": " to handle a broader range of categories.", "tokens": [51104, 281, 4813, 257, 13227, 3613, 295, 10479, 13, 51238], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 53, "seek": 24902, "start": 266.5, "end": 272.90000000000003, "text": " To ensure that meaningful embeddings are provided, a pre-trained text encoder, reference 22, 67,", "tokens": [51238, 1407, 5586, 300, 10995, 12240, 29432, 366, 5649, 11, 257, 659, 12, 17227, 2001, 2487, 2058, 19866, 11, 6408, 5853, 11, 23879, 11, 51558], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 54, "seek": 24902, "start": 272.90000000000003, "end": 275.94, "text": " 55, 66, is typically used.", "tokens": [51558, 12330, 11, 21126, 11, 307, 5850, 1143, 13, 51710], "temperature": 0.0, "avg_logprob": -0.09693197175568226, "compression_ratio": 1.620817843866171, "no_speech_prob": 0.18702922761440277}, {"id": 55, "seek": 27594, "start": 275.94, "end": 280.3, "text": " This encoder can effectively capture the semantic meaning of words and phrases, which is critical", "tokens": [50364, 639, 2058, 19866, 393, 8659, 7983, 264, 47982, 3620, 295, 2283, 293, 20312, 11, 597, 307, 4924, 50582], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 56, "seek": 27594, "start": 280.3, "end": 283.04, "text": " for open vocabulary segmentation.", "tokens": [50582, 337, 1269, 19864, 9469, 399, 13, 50719], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 57, "seek": 27594, "start": 283.04, "end": 288.86, "text": " Multi-modal models, such as CLIP, reference 66, and ALIGN, reference 38, have shown promise", "tokens": [50719, 29238, 12, 8014, 304, 5245, 11, 1270, 382, 12855, 9139, 11, 6408, 21126, 11, 293, 7056, 38455, 11, 6408, 12843, 11, 362, 4898, 6228, 51010], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 58, "seek": 27594, "start": 288.86, "end": 293.15999999999997, "text": " for open vocabulary segmentation due to their ability to learn aligned image text feature", "tokens": [51010, 337, 1269, 19864, 9469, 399, 3462, 281, 641, 3485, 281, 1466, 17962, 3256, 2487, 4111, 51225], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 59, "seek": 27594, "start": 293.15999999999997, "end": 297.62, "text": " representations from large-scale internet data, reference 70.", "tokens": [51225, 33358, 490, 2416, 12, 20033, 4705, 1412, 11, 6408, 5285, 13, 51448], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 60, "seek": 27594, "start": 297.62, "end": 303.58, "text": " SIMBASELINE, reference 85, and OVSEG, reference 50, are two recent methods that use a two-stage", "tokens": [51448, 24738, 33, 3160, 3158, 16258, 11, 6408, 14695, 11, 293, 422, 53, 5879, 38, 11, 6408, 2625, 11, 366, 732, 5162, 7150, 300, 764, 257, 732, 12, 17882, 51746], "temperature": 0.0, "avg_logprob": -0.11275458128555961, "compression_ratio": 1.6185567010309279, "no_speech_prob": 0.21194294095039368}, {"id": 61, "seek": 30358, "start": 303.58, "end": 307.4, "text": " framework to adapt CLIP for open vocabulary segmentation.", "tokens": [50364, 8388, 281, 6231, 12855, 9139, 337, 1269, 19864, 9469, 399, 13, 50555], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 62, "seek": 30358, "start": 307.4, "end": 313.21999999999997, "text": " In these methods, images are first processed by a heavy mask generator, reference 34, 19,", "tokens": [50555, 682, 613, 7150, 11, 5267, 366, 700, 18846, 538, 257, 4676, 6094, 19265, 11, 6408, 12790, 11, 1294, 11, 50846], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 63, "seek": 30358, "start": 313.21999999999997, "end": 317.74, "text": " to obtain mask proposals, and then each masked image crop is generated and fed into a frozen", "tokens": [50846, 281, 12701, 6094, 20198, 11, 293, 550, 1184, 45249, 3256, 9086, 307, 10833, 293, 4636, 666, 257, 12496, 51072], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 64, "seek": 30358, "start": 317.74, "end": 320.21999999999997, "text": " CLIP model for classification.", "tokens": [51072, 12855, 9139, 2316, 337, 21538, 13, 51196], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 65, "seek": 30358, "start": 320.21999999999997, "end": 325.9, "text": " MASKCLIP, reference 24, extends this approach to open vocabulary panoptic segmentation,", "tokens": [51196, 42129, 42, 22458, 9139, 11, 6408, 4022, 11, 26448, 341, 3109, 281, 1269, 19864, 2462, 5747, 299, 9469, 399, 11, 51480], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 66, "seek": 30358, "start": 325.9, "end": 329.76, "text": " but additionally leverages mask proposals as attention masks in the CLIP backbone to", "tokens": [51480, 457, 43181, 12451, 1660, 6094, 20198, 382, 3202, 11830, 294, 264, 12855, 9139, 34889, 281, 51673], "temperature": 0.0, "avg_logprob": -0.0929856624418092, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.621886134147644}, {"id": 67, "seek": 32976, "start": 330.76, "end": 333.84, "text": " multiple forwarding processes for the masked crops.", "tokens": [50414, 3866, 2128, 278, 7555, 337, 264, 45249, 16829, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 68, "seek": 32976, "start": 333.84, "end": 341.09999999999997, "text": " More recently, OTISE, reference 84, employs a stable diffusion UNET, reference 69, 68,", "tokens": [50568, 5048, 3938, 11, 38617, 19413, 11, 6408, 29018, 11, 846, 49522, 257, 8351, 25242, 8229, 4850, 11, 6408, 28267, 11, 23317, 11, 50931], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 69, "seek": 32976, "start": 341.09999999999997, "end": 346.15999999999997, "text": " as a frozen backbone for mask generator, which significantly boosts the state-of-the-art performance.", "tokens": [50931, 382, 257, 12496, 34889, 337, 6094, 19265, 11, 597, 10591, 9194, 82, 264, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 70, "seek": 32976, "start": 346.15999999999997, "end": 351.2, "text": " However, despite these advances, they still rely on a two-stage framework, where the mask", "tokens": [51184, 2908, 11, 7228, 613, 25297, 11, 436, 920, 10687, 322, 257, 732, 12, 17882, 8388, 11, 689, 264, 6094, 51436], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 71, "seek": 32976, "start": 351.2, "end": 356.18, "text": " generator and CLIP classifier extract features from raw images separately, resulting in inefficiency", "tokens": [51436, 19265, 293, 12855, 9139, 1508, 9902, 8947, 4122, 490, 8936, 5267, 14759, 11, 16505, 294, 7167, 49086, 51685], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 72, "seek": 32976, "start": 356.18, "end": 357.18, "text": " and ineffectiveness.", "tokens": [51685, 293, 7167, 11259, 8477, 13, 51735], "temperature": 0.0, "avg_logprob": -0.1569223580537019, "compression_ratio": 1.5218855218855218, "no_speech_prob": 0.3555682599544525}, {"id": 73, "seek": 35718, "start": 357.74, "end": 362.04, "text": " A natural question thus arises as to whether it is possible to unify the mask generator", "tokens": [50392, 316, 3303, 1168, 8807, 27388, 382, 281, 1968, 309, 307, 1944, 281, 517, 2505, 264, 6094, 19265, 50607], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 74, "seek": 35718, "start": 362.04, "end": 367.3, "text": " and CLIP classifier into a single-stage framework for open vocabulary segmentation.", "tokens": [50607, 293, 12855, 9139, 1508, 9902, 666, 257, 2167, 12, 17882, 8388, 337, 1269, 19864, 9469, 399, 13, 50870], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 75, "seek": 35718, "start": 367.3, "end": 370.86, "text": " Sharing the feature extractor between them is a straightforward solution, but it poses", "tokens": [50870, 49060, 264, 4111, 8947, 284, 1296, 552, 307, 257, 15325, 3827, 11, 457, 309, 26059, 51048], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 76, "seek": 35718, "start": 370.86, "end": 372.18, "text": " two challenges.", "tokens": [51048, 732, 4759, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 77, "seek": 35718, "start": 372.18, "end": 377.46000000000004, "text": " First, fine-tuning CLIP backbone can disrupt the alignment between image and text features,", "tokens": [51114, 2386, 11, 2489, 12, 83, 37726, 12855, 9139, 34889, 393, 14124, 264, 18515, 1296, 3256, 293, 2487, 4122, 11, 51378], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 78, "seek": 35718, "start": 377.46000000000004, "end": 381.5, "text": " resulting in a much worse performance on out-of-vocabulary categories.", "tokens": [51378, 16505, 294, 257, 709, 5324, 3389, 322, 484, 12, 2670, 12, 20836, 19189, 10479, 13, 51580], "temperature": 0.0, "avg_logprob": -0.13058161735534668, "compression_ratio": 1.5663082437275986, "no_speech_prob": 0.03307954594492912}, {"id": 79, "seek": 38150, "start": 381.5, "end": 388.94, "text": " Same methods, reference 85, 50, 24, 84, rely on another separate backbone for mask generator,", "tokens": [50364, 10635, 7150, 11, 6408, 14695, 11, 2625, 11, 4022, 11, 29018, 11, 10687, 322, 1071, 4994, 34889, 337, 6094, 19265, 11, 50736], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 80, "seek": 38150, "start": 388.94, "end": 391.66, "text": " increasing model size and computational costs.", "tokens": [50736, 5662, 2316, 2744, 293, 28270, 5497, 13, 50872], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 81, "seek": 38150, "start": 391.66, "end": 396.42, "text": " Second, CLIP models are typically pre-trained on relatively lower resolution inputs, while", "tokens": [50872, 5736, 11, 12855, 9139, 5245, 366, 5850, 659, 12, 17227, 2001, 322, 7226, 3126, 8669, 15743, 11, 1339, 51110], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 82, "seek": 38150, "start": 396.42, "end": 401.22, "text": " dense prediction tasks require a much higher resolution for optimal performance.", "tokens": [51110, 18011, 17630, 9608, 3651, 257, 709, 2946, 8669, 337, 16252, 3389, 13, 51350], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 83, "seek": 38150, "start": 401.22, "end": 405.16, "text": " This makes it difficult to directly apply CLIP pre-trained backbones to downstream dense", "tokens": [51350, 639, 1669, 309, 2252, 281, 3838, 3079, 12855, 9139, 659, 12, 17227, 2001, 646, 44954, 281, 30621, 18011, 51547], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 84, "seek": 38150, "start": 405.16, "end": 410.7, "text": " prediction tasks, particularly VIT-based CLIP models, reference 25, where careful treatments", "tokens": [51547, 17630, 9608, 11, 4098, 691, 3927, 12, 6032, 12855, 9139, 5245, 11, 6408, 3552, 11, 689, 5026, 15795, 51824], "temperature": 0.0, "avg_logprob": -0.13999749996044017, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.14025762677192688}, {"id": 85, "seek": 41070, "start": 410.7, "end": 418.02, "text": " are required, e.g., side adapter, reference 16, 86, are cost aggregation, reference 96,", "tokens": [50364, 366, 4739, 11, 308, 13, 70, 7933, 1252, 22860, 11, 6408, 3165, 11, 26687, 11, 366, 2063, 16743, 399, 11, 6408, 24124, 11, 50730], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 86, "seek": 41070, "start": 418.02, "end": 419.02, "text": " 20.", "tokens": [50730, 945, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 87, "seek": 41070, "start": 419.02, "end": 424.38, "text": " Consequently, existing methods, reference 24, 84, perform mask segmentation and CLIP", "tokens": [50780, 2656, 46027, 11, 6741, 7150, 11, 6408, 4022, 11, 29018, 11, 2042, 6094, 9469, 399, 293, 12855, 9139, 51048], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 88, "seek": 41070, "start": 424.38, "end": 428.96, "text": " classification at different input scales, leading to sub-optimal performance.", "tokens": [51048, 21538, 412, 819, 4846, 17408, 11, 5775, 281, 1422, 12, 5747, 10650, 3389, 13, 51277], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 89, "seek": 41070, "start": 428.96, "end": 433.5, "text": " To alleviate the two challenges, we propose to build both mask generator and CLIP classifier", "tokens": [51277, 1407, 42701, 264, 732, 4759, 11, 321, 17421, 281, 1322, 1293, 6094, 19265, 293, 12855, 9139, 1508, 9902, 51504], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 90, "seek": 41070, "start": 433.5, "end": 438.34, "text": " on top of a shared frozen convolutional CLIP backbone, resulting in a single-stage framework", "tokens": [51504, 322, 1192, 295, 257, 5507, 12496, 45216, 304, 12855, 9139, 34889, 11, 16505, 294, 257, 2167, 12, 17882, 8388, 51746], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 91, "seek": 41070, "start": 438.34, "end": 439.74, "text": " FC CLIP.", "tokens": [51746, 27168, 12855, 9139, 13, 51816], "temperature": 0.0, "avg_logprob": -0.12689408921358877, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.14411437511444092}, {"id": 92, "seek": 43974, "start": 439.78000000000003, "end": 444.18, "text": " Since design is based on the following observations, the frozen CLIP backbone ensures that the", "tokens": [50366, 4162, 1715, 307, 2361, 322, 264, 3480, 18163, 11, 264, 12496, 12855, 9139, 34889, 28111, 300, 264, 50586], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 93, "seek": 43974, "start": 444.18, "end": 449.74, "text": " pre-trained image-text feature alignment is intact, allowing out of vocabulary classification.", "tokens": [50586, 659, 12, 17227, 2001, 3256, 12, 25111, 4111, 18515, 307, 23493, 11, 8293, 484, 295, 19864, 21538, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 94, "seek": 43974, "start": 449.74, "end": 453.86, "text": " It can also serve as a strong mask generator by appending a lightweight pixel decoder and", "tokens": [50864, 467, 393, 611, 4596, 382, 257, 2068, 6094, 19265, 538, 724, 2029, 257, 22052, 19261, 979, 19866, 293, 51070], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 95, "seek": 43974, "start": 453.86, "end": 457.22, "text": " mask decoder, reference 19, 89.", "tokens": [51070, 6094, 979, 19866, 11, 6408, 1294, 11, 31877, 13, 51238], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 96, "seek": 43974, "start": 457.22, "end": 463.1, "text": " The convolutional CLIP, based on a convolutional neural network, CNN, reference 45, empirically", "tokens": [51238, 440, 45216, 304, 12855, 9139, 11, 2361, 322, 257, 45216, 304, 18161, 3209, 11, 24859, 11, 6408, 6905, 11, 25790, 984, 51532], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 97, "seek": 43974, "start": 463.1, "end": 468.14, "text": " shows a better generalization ability compared to VIT-based CLIP, reference 25, when the", "tokens": [51532, 3110, 257, 1101, 2674, 2144, 3485, 5347, 281, 691, 3927, 12, 6032, 12855, 9139, 11, 6408, 3552, 11, 562, 264, 51784], "temperature": 0.0, "avg_logprob": -0.0721380119649773, "compression_ratio": 1.636963696369637, "no_speech_prob": 0.24492396414279938}, {"id": 98, "seek": 46814, "start": 468.14, "end": 470.09999999999997, "text": " input size scales up.", "tokens": [50364, 4846, 2744, 17408, 493, 13, 50462], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 99, "seek": 46814, "start": 470.09999999999997, "end": 474.9, "text": " This echoes the success of fully convolutional networks, reference 58, in dense prediction", "tokens": [50462, 639, 47051, 264, 2245, 295, 4498, 45216, 304, 9590, 11, 6408, 21786, 11, 294, 18011, 17630, 50702], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 100, "seek": 46814, "start": 474.9, "end": 476.3, "text": " tasks.", "tokens": [50702, 9608, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 101, "seek": 46814, "start": 476.3, "end": 480.14, "text": " Both observations are critical for developing a single-stage framework, but they have been", "tokens": [50772, 6767, 18163, 366, 4924, 337, 6416, 257, 2167, 12, 17882, 8388, 11, 457, 436, 362, 668, 50964], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 102, "seek": 46814, "start": 480.14, "end": 485.94, "text": " overlooked and undiscovered by existing two-stage pipelines, reference 24, 84.", "tokens": [50964, 32269, 293, 674, 40080, 292, 538, 6741, 732, 12, 17882, 40168, 11, 6408, 4022, 11, 29018, 13, 51254], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 103, "seek": 46814, "start": 485.94, "end": 491.14, "text": " In Fig. 1, we visualize the learned visual representation of VIT-based and CNN-based", "tokens": [51254, 682, 22443, 13, 502, 11, 321, 23273, 264, 3264, 5056, 10290, 295, 691, 3927, 12, 6032, 293, 24859, 12, 6032, 51514], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 104, "seek": 46814, "start": 491.14, "end": 494.82, "text": " CLIP via K-means clustering, reference 57.", "tokens": [51514, 12855, 9139, 5766, 591, 12, 1398, 599, 596, 48673, 11, 6408, 21423, 13, 51698], "temperature": 0.0, "avg_logprob": -0.1090656008039202, "compression_ratio": 1.5163636363636364, "no_speech_prob": 0.39201095700263977}, {"id": 105, "seek": 49482, "start": 494.82, "end": 499.42, "text": " As shown in the figure, the features learned by CNN-based CLIP are more robust across different", "tokens": [50364, 1018, 4898, 294, 264, 2573, 11, 264, 4122, 3264, 538, 24859, 12, 6032, 12855, 9139, 366, 544, 13956, 2108, 819, 50594], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 106, "seek": 49482, "start": 499.42, "end": 500.82, "text": " input sizes.", "tokens": [50594, 4846, 11602, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 107, "seek": 49482, "start": 500.82, "end": 505.58, "text": " Surprisingly, the adoption of a single frozen convolutional CLIP as the shared feature extractor", "tokens": [50664, 49908, 11, 264, 19215, 295, 257, 2167, 12496, 45216, 304, 12855, 9139, 382, 264, 5507, 4111, 8947, 284, 50902], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 108, "seek": 49482, "start": 505.58, "end": 508.82, "text": " results in an extremely simple yet effective design.", "tokens": [50902, 3542, 294, 364, 4664, 2199, 1939, 4942, 1715, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 109, "seek": 49482, "start": 508.82, "end": 513.8199999999999, "text": " Specifically, the single-stage FC CLIP consists of three modules built upon a shared frozen", "tokens": [51064, 26058, 11, 264, 2167, 12, 17882, 27168, 12855, 9139, 14689, 295, 1045, 16679, 3094, 3564, 257, 5507, 12496, 51314], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 110, "seek": 49482, "start": 513.8199999999999, "end": 519.66, "text": " convolutional CLIP backbone, a class-agnostic mask generator, an in-vocabulary classifier,", "tokens": [51314, 45216, 304, 12855, 9139, 34889, 11, 257, 1508, 12, 4535, 19634, 6094, 19265, 11, 364, 294, 12, 20836, 19189, 1508, 9902, 11, 51606], "temperature": 0.0, "avg_logprob": -0.09326413961557242, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0259522944688797}, {"id": 111, "seek": 51966, "start": 520.66, "end": 525.1, "text": " see Fig. 2 for comparison between pipelines.", "tokens": [50414, 536, 22443, 13, 568, 337, 9660, 1296, 40168, 13, 50636], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 112, "seek": 51966, "start": 525.1, "end": 529.66, "text": " The proposed method not only enjoys a simple design, but also comes with a very low cost", "tokens": [50636, 440, 10348, 3170, 406, 787, 29750, 257, 2199, 1715, 11, 457, 611, 1487, 365, 257, 588, 2295, 2063, 50864], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 113, "seek": 51966, "start": 529.66, "end": 531.6999999999999, "text": " for both training and testing.", "tokens": [50864, 337, 1293, 3097, 293, 4997, 13, 50966], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 114, "seek": 51966, "start": 531.6999999999999, "end": 537.86, "text": " As a comparison, our model has only 238M frozen parameters and 21M trainable parameters, against", "tokens": [50966, 1018, 257, 9660, 11, 527, 2316, 575, 787, 6673, 23, 44, 12496, 9834, 293, 5080, 44, 3847, 712, 9834, 11, 1970, 51274], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 115, "seek": 51966, "start": 537.86, "end": 544.38, "text": " the state-of-the-art work ODISE, reference 84, that has 1494M frozen and 28M trainable", "tokens": [51274, 264, 1785, 12, 2670, 12, 3322, 12, 446, 589, 422, 35, 19413, 11, 6408, 29018, 11, 300, 575, 3499, 27032, 44, 12496, 293, 7562, 44, 3847, 712, 51600], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 116, "seek": 51966, "start": 544.38, "end": 545.38, "text": " parameters.", "tokens": [51600, 9834, 13, 51650], "temperature": 0.0, "avg_logprob": -0.13407612830093227, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.36264970898628235}, {"id": 117, "seek": 54538, "start": 545.74, "end": 552.5, "text": " Furthermore, our model training only takes 25.6V 100 GPU days, which is 7.5 times faster", "tokens": [50382, 23999, 11, 527, 2316, 3097, 787, 2516, 3552, 13, 21, 53, 2319, 18407, 1708, 11, 597, 307, 1614, 13, 20, 1413, 4663, 50720], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 118, "seek": 54538, "start": 552.5, "end": 556.98, "text": " compared to ODIISE's 192V 100 GPU days.", "tokens": [50720, 5347, 281, 422, 3085, 19413, 311, 1294, 17, 53, 2319, 18407, 1708, 13, 50944], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 119, "seek": 54538, "start": 556.98, "end": 561.1, "text": " During inference, our model also runs 6.6 times faster.", "tokens": [50944, 6842, 38253, 11, 527, 2316, 611, 6676, 1386, 13, 21, 1413, 4663, 13, 51150], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 120, "seek": 54538, "start": 561.1, "end": 565.54, "text": " Although FC CLIP enjoys a simple design, it still outperforms previous methods across", "tokens": [51150, 5780, 27168, 12855, 9139, 29750, 257, 2199, 1715, 11, 309, 920, 484, 26765, 82, 3894, 7150, 2108, 51372], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 121, "seek": 54538, "start": 565.54, "end": 567.54, "text": " multiple datasets.", "tokens": [51372, 3866, 42856, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 122, "seek": 54538, "start": 567.54, "end": 573.5, "text": " Trained on Cocoa Panoptic dataset only, FC CLIP surpasses prior state-of-the-art ODISE,", "tokens": [51472, 5403, 2001, 322, 29787, 64, 7557, 5747, 299, 28872, 787, 11, 27168, 12855, 9139, 27650, 279, 4059, 1785, 12, 2670, 12, 3322, 12, 446, 422, 35, 19413, 11, 51770], "temperature": 0.0, "avg_logprob": -0.15431052121249111, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.03307728096842766}, {"id": 123, "seek": 57350, "start": 574.46, "end": 576.78, "text": " significantly in a zero-shot manner.", "tokens": [50412, 10591, 294, 257, 4018, 12, 18402, 9060, 13, 50528], "temperature": 0.0, "avg_logprob": -0.14558142584723396, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.1402575522661209}, {"id": 124, "seek": 57350, "start": 576.78, "end": 588.06, "text": " Specifically, FC CLIP achieves 26.8PQ, plus 3.4, 18.2PQ, plus 4.0, and 44.0PQ, plus 2.0.1,", "tokens": [50528, 26058, 11, 27168, 12855, 9139, 3538, 977, 7551, 13, 23, 47, 48, 11, 1804, 805, 13, 19, 11, 2443, 13, 17, 47, 48, 11, 1804, 1017, 13, 15, 11, 293, 16408, 13, 15, 47, 48, 11, 1804, 568, 13, 15, 13, 16, 11, 51092], "temperature": 0.0, "avg_logprob": -0.14558142584723396, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.1402575522661209}, {"id": 125, "seek": 57350, "start": 588.06, "end": 593.16, "text": " on ADE20K, Mapillary Vistas, and Cityscapes, respectively.", "tokens": [51092, 322, 9135, 36, 2009, 42, 11, 22053, 46367, 691, 14858, 11, 293, 4392, 44239, 5190, 11, 25009, 13, 51347], "temperature": 0.0, "avg_logprob": -0.14558142584723396, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.1402575522661209}, {"id": 126, "seek": 57350, "start": 593.16, "end": 598.06, "text": " As Panoptic segmentation unifies semantic and instance segmentation, FC CLIP naturally", "tokens": [51347, 1018, 7557, 5747, 299, 9469, 399, 517, 11221, 47982, 293, 5197, 9469, 399, 11, 27168, 12855, 9139, 8195, 51592], "temperature": 0.0, "avg_logprob": -0.14558142584723396, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.1402575522661209}, {"id": 127, "seek": 57350, "start": 598.06, "end": 602.14, "text": " extends to open-vocabulary semantic and instance segmentation.", "tokens": [51592, 26448, 281, 1269, 12, 20836, 19189, 47982, 293, 5197, 9469, 399, 13, 51796], "temperature": 0.0, "avg_logprob": -0.14558142584723396, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.1402575522661209}, {"id": 128, "seek": 60214, "start": 602.14, "end": 607.58, "text": " With the same model trained on Cocoa Panoptic data only, i.e., no task-specific fine-tuning,", "tokens": [50364, 2022, 264, 912, 2316, 8895, 322, 29787, 64, 7557, 5747, 299, 1412, 787, 11, 741, 13, 68, 7933, 572, 5633, 12, 29258, 2489, 12, 83, 37726, 11, 50636], "temperature": 0.0, "avg_logprob": -0.1318461465053871, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.019716190174221992}, {"id": 129, "seek": 60214, "start": 607.58, "end": 613.1, "text": " FC CLIP achieves state-of-the-art performance on open-vocabulary instance and semantic segmentation.", "tokens": [50636, 27168, 12855, 9139, 3538, 977, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 322, 1269, 12, 20836, 19189, 5197, 293, 47982, 9469, 399, 13, 50912], "temperature": 0.0, "avg_logprob": -0.1318461465053871, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.019716190174221992}, {"id": 130, "seek": 60214, "start": 613.1, "end": 621.02, "text": " Specifically, FC CLIP achieves 16.8AP on ADE20K, surpassing the state-of-art ODIISE, reference", "tokens": [50912, 26058, 11, 27168, 12855, 9139, 3538, 977, 3165, 13, 23, 4715, 322, 9135, 36, 2009, 42, 11, 27650, 278, 264, 1785, 12, 2670, 12, 446, 422, 3085, 19413, 11, 6408, 51308], "temperature": 0.0, "avg_logprob": -0.1318461465053871, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.019716190174221992}, {"id": 131, "seek": 60214, "start": 621.02, "end": 623.5, "text": " 84, by plus 2.4.", "tokens": [51308, 29018, 11, 538, 1804, 568, 13, 19, 13, 51432], "temperature": 0.0, "avg_logprob": -0.1318461465053871, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.019716190174221992}, {"id": 132, "seek": 60214, "start": 623.5, "end": 628.58, "text": " FC CLIP also outperforms the state-of-art specialized open-vocabulary semantic segmentation", "tokens": [51432, 27168, 12855, 9139, 611, 484, 26765, 82, 264, 1785, 12, 2670, 12, 446, 19813, 1269, 12, 20836, 19189, 47982, 9469, 399, 51686], "temperature": 0.0, "avg_logprob": -0.1318461465053871, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.019716190174221992}, {"id": 133, "seek": 62858, "start": 628.58, "end": 636.74, "text": " model SAN, reference 86, by plus 1.1 and plus 1.1 MIOU on the challenging ADE20K847,", "tokens": [50364, 2316, 49557, 11, 6408, 26687, 11, 538, 1804, 502, 13, 16, 293, 1804, 502, 13, 16, 13696, 4807, 322, 264, 7595, 9135, 36, 2009, 42, 23, 14060, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1322555034718615, "compression_ratio": 1.3697478991596639, "no_speech_prob": 0.24493667483329773}, {"id": 134, "seek": 62858, "start": 636.74, "end": 644.1800000000001, "text": " A847, and Pascal Context 459, PC 459, benchmarks, respectively.", "tokens": [50772, 316, 23, 14060, 11, 293, 41723, 4839, 3828, 6905, 24, 11, 6465, 6905, 24, 11, 43751, 11, 25009, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1322555034718615, "compression_ratio": 1.3697478991596639, "no_speech_prob": 0.24493667483329773}, {"id": 135, "seek": 62858, "start": 644.1800000000001, "end": 648.74, "text": " In summary, through the lens of a careful redesign of existing two-stage open-vocabulary", "tokens": [51144, 682, 12691, 11, 807, 264, 6765, 295, 257, 5026, 39853, 295, 6741, 732, 12, 17882, 1269, 12, 20836, 19189, 51372], "temperature": 0.0, "avg_logprob": -0.1322555034718615, "compression_ratio": 1.3697478991596639, "no_speech_prob": 0.24493667483329773}, {"id": 136, "seek": 62858, "start": 648.74, "end": 654.1800000000001, "text": " segmentation models, we establish a simple, strong, and fast baseline for the community.", "tokens": [51372, 9469, 399, 5245, 11, 321, 8327, 257, 2199, 11, 2068, 11, 293, 2370, 20518, 337, 264, 1768, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1322555034718615, "compression_ratio": 1.3697478991596639, "no_speech_prob": 0.24493667483329773}, {"id": 137, "seek": 65418, "start": 654.18, "end": 659.3, "text": " The proposed FC CLIP adopts a single-stage framework by exploiting a shared frozen convolutional", "tokens": [50364, 440, 10348, 27168, 12855, 9139, 22486, 1373, 257, 2167, 12, 17882, 8388, 538, 12382, 1748, 257, 5507, 12496, 45216, 304, 50620], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 138, "seek": 65418, "start": 659.3, "end": 663.8, "text": " CLIP, which not only advances the state-of-the-art performances on multiple benchmarks, but also", "tokens": [50620, 12855, 9139, 11, 597, 406, 787, 25297, 264, 1785, 12, 2670, 12, 3322, 12, 446, 16087, 322, 3866, 43751, 11, 457, 611, 50845], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 139, "seek": 65418, "start": 663.8, "end": 667.06, "text": " enjoys a practically fast training and inference speed.", "tokens": [50845, 29750, 257, 15667, 2370, 3097, 293, 38253, 3073, 13, 51008], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 140, "seek": 65418, "start": 667.06, "end": 671.4599999999999, "text": " We hope our study will inspire future research on efficient single-stage open-vocabulary", "tokens": [51008, 492, 1454, 527, 2979, 486, 15638, 2027, 2132, 322, 7148, 2167, 12, 17882, 1269, 12, 20836, 19189, 51228], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 141, "seek": 65418, "start": 671.4599999999999, "end": 673.3, "text": " segmentation models.", "tokens": [51228, 9469, 399, 5245, 13, 51320], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 142, "seek": 65418, "start": 673.3, "end": 674.3, "text": " 2.", "tokens": [51320, 568, 13, 51370], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 143, "seek": 65418, "start": 674.3, "end": 675.7199999999999, "text": " RELATED WORK.", "tokens": [51370, 497, 3158, 2218, 4731, 30029, 42, 13, 51441], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 144, "seek": 65418, "start": 675.7199999999999, "end": 680.38, "text": " Vision language models target at encoding vision and language jointly in a fusion model.", "tokens": [51441, 25170, 2856, 5245, 3779, 412, 43430, 5201, 293, 2856, 46557, 294, 257, 23100, 2316, 13, 51674], "temperature": 0.0, "avg_logprob": -0.12377278846606873, "compression_ratio": 1.514657980456026, "no_speech_prob": 0.09941624850034714}, {"id": 145, "seek": 68038, "start": 681.38, "end": 684.3, "text": " 73, 15, 93.", "tokens": [50414, 28387, 11, 2119, 11, 28876, 13, 50560], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 146, "seek": 68038, "start": 684.3, "end": 688.48, "text": " Extract visual representations by pre-trained object detectors and fine-tune on downstream", "tokens": [50560, 9881, 1897, 5056, 33358, 538, 659, 12, 17227, 2001, 2657, 46866, 293, 2489, 12, 83, 2613, 322, 30621, 50769], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 147, "seek": 68038, "start": 688.48, "end": 690.58, "text": " tasks with language supervision.", "tokens": [50769, 9608, 365, 2856, 32675, 13, 50874], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 148, "seek": 68038, "start": 690.58, "end": 696.22, "text": " Recently, with the breakthrough of large language models, reference 22, 3, rapid progress has", "tokens": [50874, 20072, 11, 365, 264, 22397, 295, 2416, 2856, 5245, 11, 6408, 5853, 11, 805, 11, 7558, 4205, 575, 51156], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 149, "seek": 68038, "start": 696.22, "end": 697.74, "text": " been made in this field.", "tokens": [51156, 668, 1027, 294, 341, 2519, 13, 51232], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 150, "seek": 68038, "start": 697.74, "end": 703.42, "text": " CLIP, reference 66, and ALIGN, reference 38, demonstrate that pre-training dual encoder", "tokens": [51232, 12855, 9139, 11, 6408, 21126, 11, 293, 7056, 38455, 11, 6408, 12843, 11, 11698, 300, 659, 12, 17227, 1760, 11848, 2058, 19866, 51516], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 151, "seek": 68038, "start": 703.42, "end": 708.26, "text": " models with contrastive objectives on large-scale noisy image textpairs can learn representation", "tokens": [51516, 5245, 365, 8712, 488, 15961, 322, 2416, 12, 20033, 24518, 3256, 2487, 79, 4094, 393, 1466, 10290, 51758], "temperature": 0.0, "avg_logprob": -0.16652637057834202, "compression_ratio": 1.579136690647482, "no_speech_prob": 0.5152246356010437}, {"id": 152, "seek": 70826, "start": 708.26, "end": 712.54, "text": " with cross-modal alignment ability and show strong performance in zero-shot downstream", "tokens": [50364, 365, 3278, 12, 8014, 304, 18515, 3485, 293, 855, 2068, 3389, 294, 4018, 12, 18402, 30621, 50578], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 153, "seek": 70826, "start": 712.54, "end": 713.86, "text": " tasks.", "tokens": [50578, 9608, 13, 50644], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 154, "seek": 70826, "start": 713.86, "end": 719.18, "text": " The following works, reference 90, 1, 87, further confirm these points and achieve impressive", "tokens": [50644, 440, 3480, 1985, 11, 6408, 4289, 11, 502, 11, 27990, 11, 3052, 9064, 613, 2793, 293, 4584, 8992, 50910], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 155, "seek": 70826, "start": 719.18, "end": 724.38, "text": " results in zero-shot transfer learning such as open-vocabulary image recognition.", "tokens": [50910, 3542, 294, 4018, 12, 18402, 5003, 2539, 1270, 382, 1269, 12, 20836, 19189, 3256, 11150, 13, 51170], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 156, "seek": 70826, "start": 724.38, "end": 728.42, "text": " Closed-vocabulary segmentation can be divided into three types according to the semantics", "tokens": [51170, 2033, 1744, 12, 20836, 19189, 9469, 399, 393, 312, 6666, 666, 1045, 3467, 4650, 281, 264, 4361, 45298, 51372], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 157, "seek": 70826, "start": 728.42, "end": 733.54, "text": " of the grouping pixels, i.e. semantic, instance and panoptic segmentation.", "tokens": [51372, 295, 264, 40149, 18668, 11, 741, 13, 68, 13, 47982, 11, 5197, 293, 2462, 5747, 299, 9469, 399, 13, 51628], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 158, "seek": 70826, "start": 733.54, "end": 737.54, "text": " Semantic segmentation interprets high-level category semantic concepts.", "tokens": [51628, 14421, 7128, 9469, 399, 17489, 1373, 1090, 12, 12418, 7719, 47982, 10392, 13, 51828], "temperature": 0.0, "avg_logprob": -0.11595598317809024, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.21193408966064453}, {"id": 159, "seek": 73754, "start": 737.54, "end": 746.8199999999999, "text": " Prior works, 9, 69, 10-12, 27, 91, 81, 94, 29, mainly treat this task as a per-pixel", "tokens": [50364, 24032, 1985, 11, 1722, 11, 28267, 11, 1266, 12, 4762, 11, 7634, 11, 31064, 11, 30827, 11, 30849, 11, 9413, 11, 8704, 2387, 341, 5633, 382, 257, 680, 12, 79, 34599, 50828], "temperature": 0.0, "avg_logprob": -0.087265440561239, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.5075975656509399}, {"id": 160, "seek": 73754, "start": 746.8199999999999, "end": 752.66, "text": " classification problem and build their models on top of the idea of FCN, reference 58.", "tokens": [50828, 21538, 1154, 293, 1322, 641, 5245, 322, 1192, 295, 264, 1558, 295, 27168, 45, 11, 6408, 21786, 13, 51120], "temperature": 0.0, "avg_logprob": -0.087265440561239, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.5075975656509399}, {"id": 161, "seek": 73754, "start": 752.66, "end": 756.8199999999999, "text": " Instance segmentation groups foreground pixels into different object instances, starting", "tokens": [51120, 2730, 719, 9469, 399, 3935, 32058, 18668, 666, 819, 2657, 14519, 11, 2891, 51328], "temperature": 0.0, "avg_logprob": -0.087265440561239, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.5075975656509399}, {"id": 162, "seek": 73754, "start": 756.8199999999999, "end": 767.1999999999999, "text": " from mask rCNN, reference 34, prior works, reference 40, 54, 6, 2, 8, 75, 79, 64, mainly", "tokens": [51328, 490, 6094, 367, 34, 45, 45, 11, 6408, 12790, 11, 4059, 1985, 11, 6408, 3356, 11, 20793, 11, 1386, 11, 568, 11, 1649, 11, 9562, 11, 32803, 11, 12145, 11, 8704, 51847], "temperature": 0.0, "avg_logprob": -0.087265440561239, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.5075975656509399}, {"id": 163, "seek": 76720, "start": 767.5200000000001, "end": 771.88, "text": " address this task with mask classification, where a set of bounding boxes and binary masks", "tokens": [50380, 2985, 341, 5633, 365, 6094, 21538, 11, 689, 257, 992, 295, 5472, 278, 9002, 293, 17434, 11830, 50598], "temperature": 0.0, "avg_logprob": -0.11046641213553292, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.07157962024211884}, {"id": 164, "seek": 76720, "start": 771.88, "end": 773.6, "text": " are predicted.", "tokens": [50598, 366, 19147, 13, 50684], "temperature": 0.0, "avg_logprob": -0.11046641213553292, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.07157962024211884}, {"id": 165, "seek": 76720, "start": 773.6, "end": 778.48, "text": " Panoptic segmentation seeks for holistic scene understanding including both stuff and things.", "tokens": [50684, 7557, 5747, 299, 9469, 399, 28840, 337, 30334, 4145, 3701, 3009, 1293, 1507, 293, 721, 13, 50928], "temperature": 0.0, "avg_logprob": -0.11046641213553292, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.07157962024211884}, {"id": 166, "seek": 76720, "start": 778.48, "end": 788.72, "text": " The pioneering work, reference 42, and prevalent ones, reference 53, 41, 82, 17, 48, 77, 13,", "tokens": [50928, 440, 19761, 1794, 589, 11, 6408, 14034, 11, 293, 30652, 2306, 11, 6408, 21860, 11, 18173, 11, 29097, 11, 3282, 11, 11174, 11, 25546, 11, 3705, 11, 51440], "temperature": 0.0, "avg_logprob": -0.11046641213553292, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.07157962024211884}, {"id": 167, "seek": 76720, "start": 788.72, "end": 793.08, "text": " decompose the problem into various proxy tasks and merge the results in the end.", "tokens": [51440, 22867, 541, 264, 1154, 666, 3683, 29690, 9608, 293, 22183, 264, 3542, 294, 264, 917, 13, 51658], "temperature": 0.0, "avg_logprob": -0.11046641213553292, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.07157962024211884}, {"id": 168, "seek": 79308, "start": 793.1600000000001, "end": 803.0, "text": " Recently, following Dieter, reference 7, most works, reference 78, 72, 18, 19, 49, 88, 89,", "tokens": [50368, 20072, 11, 3480, 29606, 260, 11, 6408, 1614, 11, 881, 1985, 11, 6408, 26369, 11, 18731, 11, 2443, 11, 1294, 11, 16513, 11, 24587, 11, 31877, 11, 50860], "temperature": 0.0, "avg_logprob": -0.12798728194891237, "compression_ratio": 1.576, "no_speech_prob": 0.3555791676044464}, {"id": 169, "seek": 79308, "start": 803.0, "end": 809.48, "text": " 37, 47, present end-to-end solutions based on the idea of mask classification.", "tokens": [50860, 13435, 11, 16953, 11, 1974, 917, 12, 1353, 12, 521, 6547, 2361, 322, 264, 1558, 295, 6094, 21538, 13, 51184], "temperature": 0.0, "avg_logprob": -0.12798728194891237, "compression_ratio": 1.576, "no_speech_prob": 0.3555791676044464}, {"id": 170, "seek": 79308, "start": 809.48, "end": 813.0400000000001, "text": " Standing on their shoulders, our proposed method builds on top of the pixel decoder", "tokens": [51184, 33655, 322, 641, 10245, 11, 527, 10348, 3170, 15182, 322, 1192, 295, 264, 19261, 979, 19866, 51362], "temperature": 0.0, "avg_logprob": -0.12798728194891237, "compression_ratio": 1.576, "no_speech_prob": 0.3555791676044464}, {"id": 171, "seek": 79308, "start": 813.0400000000001, "end": 818.7800000000001, "text": " and mask decoder of mask 2 former, reference 19, by additionally exploiting the open vocabulary", "tokens": [51362, 293, 6094, 979, 19866, 295, 6094, 568, 5819, 11, 6408, 1294, 11, 538, 43181, 12382, 1748, 264, 1269, 19864, 51649], "temperature": 0.0, "avg_logprob": -0.12798728194891237, "compression_ratio": 1.576, "no_speech_prob": 0.3555791676044464}, {"id": 172, "seek": 79308, "start": 818.7800000000001, "end": 822.6, "text": " recognition ability from CLIP, reference 66.", "tokens": [51649, 11150, 3485, 490, 12855, 9139, 11, 6408, 21126, 13, 51840], "temperature": 0.0, "avg_logprob": -0.12798728194891237, "compression_ratio": 1.576, "no_speech_prob": 0.3555791676044464}, {"id": 173, "seek": 82260, "start": 822.6, "end": 826.84, "text": " Open vocabulary segmentation aims at segmenting arbitrary classes including those that can", "tokens": [50364, 7238, 19864, 9469, 399, 24683, 412, 9469, 278, 23211, 5359, 3009, 729, 300, 393, 50576], "temperature": 0.0, "avg_logprob": -0.13558363360027934, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.06007150933146477}, {"id": 174, "seek": 82260, "start": 826.84, "end": 829.6800000000001, "text": " not be accessed during the training procedure.", "tokens": [50576, 406, 312, 34211, 1830, 264, 3097, 10747, 13, 50718], "temperature": 0.0, "avg_logprob": -0.13558363360027934, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.06007150933146477}, {"id": 175, "seek": 82260, "start": 829.6800000000001, "end": 841.48, "text": " Pryors works, reference 46, 28, 85, 50, 23, 83, 96, 86, 99, 60, 97, perform open vocabulary", "tokens": [50718, 430, 627, 830, 1985, 11, 6408, 17835, 11, 7562, 11, 14695, 11, 2625, 11, 6673, 11, 30997, 11, 24124, 11, 26687, 11, 11803, 11, 4060, 11, 23399, 11, 2042, 1269, 19864, 51308], "temperature": 0.0, "avg_logprob": -0.13558363360027934, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.06007150933146477}, {"id": 176, "seek": 82260, "start": 841.48, "end": 846.12, "text": " semantic segmentation through leveraging large pre-trained vision language models, reference", "tokens": [51308, 47982, 9469, 399, 807, 32666, 2416, 659, 12, 17227, 2001, 5201, 2856, 5245, 11, 6408, 51540], "temperature": 0.0, "avg_logprob": -0.13558363360027934, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.06007150933146477}, {"id": 177, "seek": 82260, "start": 846.12, "end": 848.64, "text": " 66, 38, 68.", "tokens": [51540, 21126, 11, 12843, 11, 23317, 13, 51666], "temperature": 0.0, "avg_logprob": -0.13558363360027934, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.06007150933146477}, {"id": 178, "seek": 84864, "start": 848.68, "end": 853.92, "text": " Recently, mask CLIP, reference 24, presents a two-stage pipeline, which consists of a", "tokens": [50366, 20072, 11, 6094, 12855, 9139, 11, 6408, 4022, 11, 13533, 257, 732, 12, 17882, 15517, 11, 597, 14689, 295, 257, 50628], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 179, "seek": 84864, "start": 853.92, "end": 859.4399999999999, "text": " class-agnostic mask generator and a frozen CLIP, reference 66, encoder for cross-modal", "tokens": [50628, 1508, 12, 4535, 19634, 6094, 19265, 293, 257, 12496, 12855, 9139, 11, 6408, 21126, 11, 2058, 19866, 337, 3278, 12, 8014, 304, 50904], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 180, "seek": 84864, "start": 859.4399999999999, "end": 863.84, "text": " alignment and thus expands the scope of the CLIP models into open vocabulary panoptic", "tokens": [50904, 18515, 293, 8807, 33706, 264, 11923, 295, 264, 12855, 9139, 5245, 666, 1269, 19864, 2462, 5747, 299, 51124], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 181, "seek": 84864, "start": 863.84, "end": 864.84, "text": " segmentation.", "tokens": [51124, 9469, 399, 13, 51174], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 182, "seek": 84864, "start": 864.84, "end": 871.68, "text": " ODISE, reference 84, digs out the innate potential of pre-trained text-image diffusion models,", "tokens": [51174, 48447, 19413, 11, 6408, 29018, 11, 2528, 82, 484, 264, 41766, 3995, 295, 659, 12, 17227, 2001, 2487, 12, 26624, 25242, 5245, 11, 51516], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 183, "seek": 84864, "start": 871.68, "end": 876.2, "text": " reference 68, in terms of the ability to present open concepts in the representation space", "tokens": [51516, 6408, 23317, 11, 294, 2115, 295, 264, 3485, 281, 1974, 1269, 10392, 294, 264, 10290, 1901, 51742], "temperature": 0.0, "avg_logprob": -0.14653882565705673, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.02842855267226696}, {"id": 184, "seek": 87620, "start": 876.76, "end": 880.44, "text": " for performing strong open vocabulary panoptic segmentation.", "tokens": [50392, 337, 10205, 2068, 1269, 19864, 2462, 5747, 299, 9469, 399, 13, 50576], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 185, "seek": 87620, "start": 880.44, "end": 887.0400000000001, "text": " Free Seg, reference 65, encodes multigranularity concepts into a compact textural abstraction,", "tokens": [50576, 11551, 21595, 11, 6408, 11624, 11, 2058, 4789, 2120, 328, 4257, 1040, 507, 10392, 666, 257, 14679, 2487, 1807, 37765, 11, 50906], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 186, "seek": 87620, "start": 887.0400000000001, "end": 890.6, "text": " enabling generalizability to arbitrary text description.", "tokens": [50906, 23148, 2674, 590, 2310, 281, 23211, 2487, 3855, 13, 51084], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 187, "seek": 87620, "start": 890.6, "end": 895.5200000000001, "text": " Unlike those methods, we propose a single-stage framework by exploiting a single frozen convolutional", "tokens": [51084, 17657, 729, 7150, 11, 321, 17421, 257, 2167, 12, 17882, 8388, 538, 12382, 1748, 257, 2167, 12496, 45216, 304, 51330], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 188, "seek": 87620, "start": 895.5200000000001, "end": 901.08, "text": " CLIP backbone, resulting in a simpler, faster, and stronger model than existing works.", "tokens": [51330, 12855, 9139, 34889, 11, 16505, 294, 257, 18587, 11, 4663, 11, 293, 7249, 2316, 813, 6741, 1985, 13, 51608], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 189, "seek": 87620, "start": 901.08, "end": 902.2800000000001, "text": " 3.", "tokens": [51608, 805, 13, 51668], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 190, "seek": 87620, "start": 902.2800000000001, "end": 903.2800000000001, "text": " Method", "tokens": [51668, 25285, 51718], "temperature": 0.0, "avg_logprob": -0.19337561695846087, "compression_ratio": 1.5278810408921932, "no_speech_prob": 0.256707102060318}, {"id": 191, "seek": 90328, "start": 903.36, "end": 907.4, "text": " In our first-stage segmentation, we first define the problem of open vocabulary segmentation.", "tokens": [50368, 682, 527, 700, 12, 17882, 9469, 399, 11, 321, 700, 6964, 264, 1154, 295, 1269, 19864, 9469, 399, 13, 50570], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 192, "seek": 90328, "start": 907.4, "end": 912.52, "text": " We then introduce the existing two-stage pipeline, followed by our proposed single-stage framework", "tokens": [50570, 492, 550, 5366, 264, 6741, 732, 12, 17882, 15517, 11, 6263, 538, 527, 10348, 2167, 12, 17882, 8388, 50826], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 193, "seek": 90328, "start": 912.52, "end": 914.04, "text": " FCCLIP.", "tokens": [50826, 27168, 22458, 9139, 13, 50902], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 194, "seek": 90328, "start": 914.04, "end": 918.56, "text": " Problem definition Open vocabulary segmentation aims to segment the image I element of R-H", "tokens": [50902, 11676, 7123, 7238, 19864, 9469, 399, 24683, 281, 9469, 264, 3256, 286, 4478, 295, 497, 12, 39, 51128], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 195, "seek": 90328, "start": 918.56, "end": 924.3, "text": " \u00d7 W \u00d7 3 into a set of masks with associated semantic labels, the K-Ground Truth masks", "tokens": [51128, 690, 245, 343, 690, 245, 805, 666, 257, 992, 295, 11830, 365, 6615, 47982, 16949, 11, 264, 591, 12, 38, 5453, 20522, 11830, 51415], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 196, "seek": 90328, "start": 924.3, "end": 926.88, "text": " M-I element of 0, 1.", "tokens": [51415, 376, 12, 40, 4478, 295, 1958, 11, 502, 13, 51544], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 197, "seek": 90328, "start": 926.88, "end": 931.6, "text": " H \u00d7 W contain the corresponding Ground Truth class label C-I.", "tokens": [51544, 389, 690, 245, 343, 5304, 264, 11760, 28371, 20522, 1508, 7645, 383, 12, 40, 13, 51780], "temperature": 0.0, "avg_logprob": -0.28606660701026604, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.22804301977157593}, {"id": 198, "seek": 93160, "start": 931.9200000000001, "end": 936.24, "text": " During training, a fixed set of class labels C-TRAIN is used, while during inference, another", "tokens": [50380, 6842, 3097, 11, 257, 6806, 992, 295, 1508, 16949, 383, 12, 51, 3750, 1464, 307, 1143, 11, 1339, 1830, 38253, 11, 1071, 50596], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 199, "seek": 93160, "start": 936.24, "end": 938.84, "text": " set of categories C-TEST is used.", "tokens": [50596, 992, 295, 10479, 383, 12, 51, 14497, 307, 1143, 13, 50726], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 200, "seek": 93160, "start": 938.84, "end": 943.96, "text": " In the open vocabulary setting, C-TEST may contain novel categories unseen during training,", "tokens": [50726, 682, 264, 1269, 19864, 3287, 11, 383, 12, 51, 14497, 815, 5304, 7613, 10479, 40608, 1830, 3097, 11, 50982], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 201, "seek": 93160, "start": 943.96, "end": 946.76, "text": " i.e., C-TRAIN equals C-TEST.", "tokens": [50982, 741, 13, 68, 7933, 383, 12, 51, 3750, 1464, 6915, 383, 12, 51, 14497, 13, 51122], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 202, "seek": 93160, "start": 946.76, "end": 951.96, "text": " We follow previous works, reference 24, 84, and assume the availability of the category", "tokens": [51122, 492, 1524, 3894, 1985, 11, 6408, 4022, 11, 29018, 11, 293, 6552, 264, 17945, 295, 264, 7719, 51382], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 203, "seek": 93160, "start": 951.96, "end": 956.32, "text": " names of C-TEST, represented in natural language, during testing.", "tokens": [51382, 5288, 295, 383, 12, 51, 14497, 11, 10379, 294, 3303, 2856, 11, 1830, 4997, 13, 51600], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 204, "seek": 93160, "start": 956.32, "end": 960.5400000000001, "text": " Although this framework has achieved impressive open vocabulary segmentation performance,", "tokens": [51600, 5780, 341, 8388, 575, 11042, 8992, 1269, 19864, 9469, 399, 3389, 11, 51811], "temperature": 0.0, "avg_logprob": -0.10021835327148437, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.12248072028160095}, {"id": 205, "seek": 96054, "start": 960.54, "end": 962.02, "text": " it has two limitations.", "tokens": [50364, 309, 575, 732, 15705, 13, 50438], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 206, "seek": 96054, "start": 962.02, "end": 966.5799999999999, "text": " First, the image features are extracted twice, once for mask generation and the other for", "tokens": [50438, 2386, 11, 264, 3256, 4122, 366, 34086, 6091, 11, 1564, 337, 6094, 5125, 293, 264, 661, 337, 50666], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 207, "seek": 96054, "start": 966.5799999999999, "end": 968.4, "text": " mask classification.", "tokens": [50666, 6094, 21538, 13, 50757], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 208, "seek": 96054, "start": 968.4, "end": 972.7199999999999, "text": " The double-feature extractions incur heavy computation, making it costly to scale up", "tokens": [50757, 440, 3834, 12, 2106, 1503, 8947, 626, 35774, 4676, 24903, 11, 1455, 309, 28328, 281, 4373, 493, 50973], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 209, "seek": 96054, "start": 972.7199999999999, "end": 974.26, "text": " backbone parameters.", "tokens": [50973, 34889, 9834, 13, 51050], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 210, "seek": 96054, "start": 974.26, "end": 981.18, "text": " Second, the mask generator often requires high-resolution inputs, e.g., 1024 \u00d7 1024,", "tokens": [51050, 5736, 11, 264, 6094, 19265, 2049, 7029, 1090, 12, 495, 3386, 15743, 11, 308, 13, 70, 7933, 1266, 7911, 690, 245, 1266, 7911, 11, 51396], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 211, "seek": 96054, "start": 981.18, "end": 988.0999999999999, "text": " whereas the CLIP model is usually pre-trained with lower-resolution images, e.g., 224 \u00d7 224.", "tokens": [51396, 9735, 264, 12855, 9139, 2316, 307, 2673, 659, 12, 17227, 2001, 365, 3126, 12, 495, 3386, 5267, 11, 308, 13, 70, 7933, 5853, 19, 690, 245, 5853, 19, 13, 51742], "temperature": 0.0, "avg_logprob": -0.13613343657108776, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.3625977635383606}, {"id": 212, "seek": 98810, "start": 988.1, "end": 992.26, "text": " The two-stage pipeline thus needs to feed high-resolution images into the mask generator", "tokens": [50364, 440, 732, 12, 17882, 15517, 8807, 2203, 281, 3154, 1090, 12, 495, 3386, 5267, 666, 264, 6094, 19265, 50572], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 213, "seek": 98810, "start": 992.26, "end": 997.34, "text": " and low-resolution images into the CLIP classifier, making the model inefficient.", "tokens": [50572, 293, 2295, 12, 495, 3386, 5267, 666, 264, 12855, 9139, 1508, 9902, 11, 1455, 264, 2316, 43495, 13, 50826], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 214, "seek": 98810, "start": 997.34, "end": 1002.38, "text": " Naive single-stage open vocabulary segmentation to avoid increasing the model size and computational", "tokens": [50826, 6056, 488, 2167, 12, 17882, 1269, 19864, 9469, 399, 281, 5042, 5662, 264, 2316, 2744, 293, 28270, 51078], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 215, "seek": 98810, "start": 1002.38, "end": 1006.66, "text": " cost of duplicate feature extractions, one may naively formulate everything together", "tokens": [51078, 2063, 295, 23976, 4111, 8947, 626, 11, 472, 815, 1667, 3413, 47881, 1203, 1214, 51292], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 216, "seek": 98810, "start": 1006.66, "end": 1011.38, "text": " into a single-stage framework F, where both mask generator and mask classifier share the", "tokens": [51292, 666, 257, 2167, 12, 17882, 8388, 479, 11, 689, 1293, 6094, 19265, 293, 6094, 1508, 9902, 2073, 264, 51528], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 217, "seek": 98810, "start": 1011.38, "end": 1016.14, "text": " same CLIP pre-trained backbone CLIP, not frozen, for extracting features from an input image", "tokens": [51528, 912, 12855, 9139, 659, 12, 17227, 2001, 34889, 12855, 9139, 11, 406, 12496, 11, 337, 49844, 4122, 490, 364, 4846, 3256, 51766], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 218, "seek": 98810, "start": 1016.14, "end": 1017.14, "text": " I.", "tokens": [51766, 286, 13, 51816], "temperature": 0.0, "avg_logprob": -0.08667578235749275, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.12933513522148132}, {"id": 219, "seek": 101714, "start": 1017.18, "end": 1021.66, "text": " However, we empirically discover that fine-tuning this naive single-stage framework causes a", "tokens": [50366, 2908, 11, 321, 25790, 984, 4411, 300, 2489, 12, 83, 37726, 341, 29052, 2167, 12, 17882, 8388, 7700, 257, 50590], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 220, "seek": 101714, "start": 1021.66, "end": 1025.72, "text": " misalignment between image and text features in the pre-trained CLIP model, leading to", "tokens": [50590, 3346, 304, 41134, 1296, 3256, 293, 2487, 4122, 294, 264, 659, 12, 17227, 2001, 12855, 9139, 2316, 11, 5775, 281, 50793], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 221, "seek": 101714, "start": 1025.72, "end": 1029.8, "text": " sub-optimal performance, especially for novel unseen classes.", "tokens": [50793, 1422, 12, 5747, 10650, 3389, 11, 2318, 337, 7613, 40608, 5359, 13, 50997], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 222, "seek": 101714, "start": 1029.8, "end": 1035.5, "text": " It also increases the training costs by 2.1 times to 52.8 GPU days.", "tokens": [50997, 467, 611, 8637, 264, 3097, 5497, 538, 568, 13, 16, 1413, 281, 18079, 13, 23, 18407, 1708, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 223, "seek": 101714, "start": 1035.5, "end": 1039.94, "text": " Interestingly, our experiments also show that a frozen CLIP backbone can provide sufficient", "tokens": [51282, 30564, 11, 527, 12050, 611, 855, 300, 257, 12496, 12855, 9139, 34889, 393, 2893, 11563, 51504], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 224, "seek": 101714, "start": 1039.94, "end": 1044.42, "text": " features for mask generation, while preserving the image-text-aligned representation.", "tokens": [51504, 4122, 337, 6094, 5125, 11, 1339, 33173, 264, 3256, 12, 25111, 12, 304, 16690, 10290, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1151288768701386, "compression_ratio": 1.5659163987138263, "no_speech_prob": 0.03621416911482811}, {"id": 225, "seek": 104442, "start": 1044.7, "end": 1049.02, "text": " Nevertheless, we still face another challenge, where CLIP models are usually pre-trained", "tokens": [50378, 26554, 11, 321, 920, 1851, 1071, 3430, 11, 689, 12855, 9139, 5245, 366, 2673, 659, 12, 17227, 2001, 50594], "temperature": 0.0, "avg_logprob": -0.1252877616882324, "compression_ratio": 1.5084033613445378, "no_speech_prob": 0.014953420497477055}, {"id": 226, "seek": 104442, "start": 1049.02, "end": 1056.38, "text": " on low-resolution images, e.g., 224x224, whereas segmentation models prefer higher-resolution", "tokens": [50594, 322, 2295, 12, 495, 3386, 5267, 11, 308, 13, 70, 7933, 5853, 19, 87, 7490, 19, 11, 9735, 9469, 399, 5245, 4382, 2946, 12, 495, 3386, 50962], "temperature": 0.0, "avg_logprob": -0.1252877616882324, "compression_ratio": 1.5084033613445378, "no_speech_prob": 0.014953420497477055}, {"id": 227, "seek": 104442, "start": 1056.38, "end": 1063.0600000000002, "text": " inputs, e.g., 800x1333 for Cocoa or 1024x for Cityscapes.", "tokens": [50962, 15743, 11, 308, 13, 70, 7933, 13083, 87, 7668, 10191, 337, 29787, 64, 420, 1266, 7911, 87, 337, 4392, 44239, 5190, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1252877616882324, "compression_ratio": 1.5084033613445378, "no_speech_prob": 0.014953420497477055}, {"id": 228, "seek": 104442, "start": 1063.0600000000002, "end": 1067.22, "text": " This discrepancy results in the significant performance degradation when applying a frozen", "tokens": [51296, 639, 2983, 265, 6040, 1344, 3542, 294, 264, 4776, 3389, 40519, 562, 9275, 257, 12496, 51504], "temperature": 0.0, "avg_logprob": -0.1252877616882324, "compression_ratio": 1.5084033613445378, "no_speech_prob": 0.014953420497477055}, {"id": 229, "seek": 104442, "start": 1067.22, "end": 1069.6200000000001, "text": " CLIP on large input images.", "tokens": [51504, 12855, 9139, 322, 2416, 4846, 5267, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1252877616882324, "compression_ratio": 1.5084033613445378, "no_speech_prob": 0.014953420497477055}, {"id": 230, "seek": 106962, "start": 1069.82, "end": 1074.8999999999999, "text": " Digging into the details, we found that it is related to the popular VIT reference 25", "tokens": [50374, 10976, 3249, 666, 264, 4365, 11, 321, 1352, 300, 309, 307, 4077, 281, 264, 3743, 691, 3927, 6408, 3552, 50628], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 231, "seek": 106962, "start": 1074.8999999999999, "end": 1078.4599999999998, "text": " backbone used in CLIP that does not transfer well to different input sizes, which could", "tokens": [50628, 34889, 1143, 294, 12855, 9139, 300, 775, 406, 5003, 731, 281, 819, 4846, 11602, 11, 597, 727, 50806], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 232, "seek": 106962, "start": 1078.4599999999998, "end": 1085.6599999999999, "text": " be alleviated by extra careful designs, e.g., side adapter reference 16, 86, or cost aggregation", "tokens": [50806, 312, 33201, 770, 538, 2857, 5026, 11347, 11, 308, 13, 70, 7933, 1252, 22860, 6408, 3165, 11, 26687, 11, 420, 2063, 16743, 399, 51166], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 233, "seek": 106962, "start": 1085.6599999999999, "end": 1087.8999999999999, "text": " reference 96, 20.", "tokens": [51166, 6408, 24124, 11, 945, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 234, "seek": 106962, "start": 1087.8999999999999, "end": 1094.02, "text": " On the other hand, CNN-based CLIP models, such as ResNet reference 33 and ConvNext reference", "tokens": [51278, 1282, 264, 661, 1011, 11, 24859, 12, 6032, 12855, 9139, 5245, 11, 1270, 382, 5015, 31890, 6408, 11816, 293, 2656, 85, 31002, 6408, 51584], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 235, "seek": 106962, "start": 1094.02, "end": 1098.78, "text": " 56, exhibit better generalization ability to different input sizes, due to their fully", "tokens": [51584, 19687, 11, 20487, 1101, 2674, 2144, 3485, 281, 819, 4846, 11602, 11, 3462, 281, 641, 4498, 51822], "temperature": 0.0, "avg_logprob": -0.1290630845942049, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.0197154488414526}, {"id": 236, "seek": 109878, "start": 1098.94, "end": 1101.42, "text": " convolutional nature, reference 58.", "tokens": [50372, 45216, 304, 3687, 11, 6408, 21786, 13, 50496], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 237, "seek": 109878, "start": 1101.42, "end": 1106.7, "text": " Additionally, the CNN-based CLIP backbone, extracting multi-scale feature maps, can be", "tokens": [50496, 19927, 11, 264, 24859, 12, 6032, 12855, 9139, 34889, 11, 49844, 4825, 12, 20033, 4111, 11317, 11, 393, 312, 50760], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 238, "seek": 109878, "start": 1106.7, "end": 1111.8999999999999, "text": " used as a simple plug-in module into modern closed vocabulary segmentation models, reference", "tokens": [50760, 1143, 382, 257, 2199, 5452, 12, 259, 10088, 666, 4363, 5395, 19864, 9469, 399, 5245, 11, 6408, 51020], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 239, "seek": 109878, "start": 1111.8999999999999, "end": 1113.82, "text": " 19, 89.", "tokens": [51020, 1294, 11, 31877, 13, 51116], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 240, "seek": 109878, "start": 1113.82, "end": 1118.8999999999999, "text": " Motivated by the observations, we thus propose FC-CLIP, a simple yet effective single-stage", "tokens": [51116, 8956, 592, 770, 538, 264, 18163, 11, 321, 8807, 17421, 27168, 12, 22458, 9139, 11, 257, 2199, 1939, 4942, 2167, 12, 17882, 51370], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 241, "seek": 109878, "start": 1118.8999999999999, "end": 1123.44, "text": " open vocabulary segmentation framework built entirely on a single frozen convolutional", "tokens": [51370, 1269, 19864, 9469, 399, 8388, 3094, 7696, 322, 257, 2167, 12496, 45216, 304, 51597], "temperature": 0.0, "avg_logprob": -0.14999312162399292, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.12590089440345764}, {"id": 242, "seek": 112344, "start": 1123.44, "end": 1129.04, "text": " CLIP backbone CLIP asterisk operator CNN, FC-CLIP The proposed FC-CLIP leverages the", "tokens": [50364, 12855, 9139, 34889, 12855, 9139, 257, 3120, 7797, 12973, 24859, 11, 27168, 12, 22458, 9139, 440, 10348, 27168, 12, 22458, 9139, 12451, 1660, 264, 50644], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 243, "seek": 112344, "start": 1129.04, "end": 1133.72, "text": " semantic features of a frozen CNN-based CLIP backbone for both mask generation and CLIP", "tokens": [50644, 47982, 4122, 295, 257, 12496, 24859, 12, 6032, 12855, 9139, 34889, 337, 1293, 6094, 5125, 293, 12855, 9139, 50878], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 244, "seek": 112344, "start": 1133.72, "end": 1135.24, "text": " classification.", "tokens": [50878, 21538, 13, 50954], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 245, "seek": 112344, "start": 1135.24, "end": 1141.76, "text": " Unlike previous works, reference 85, 50, 24, 84, which often train a separate mask generator", "tokens": [50954, 17657, 3894, 1985, 11, 6408, 14695, 11, 2625, 11, 4022, 11, 29018, 11, 597, 2049, 3847, 257, 4994, 6094, 19265, 51280], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 246, "seek": 112344, "start": 1141.76, "end": 1147.16, "text": " and ignore the potential reuse of CLIP's semantic features, we incorporate the CNN-based CLIP", "tokens": [51280, 293, 11200, 264, 3995, 26225, 295, 12855, 9139, 311, 47982, 4122, 11, 321, 16091, 264, 24859, 12, 6032, 12855, 9139, 51550], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 247, "seek": 112344, "start": 1147.16, "end": 1152.52, "text": " backbone into the state-of-the-art segmentation method MASK2 former, reference 19.", "tokens": [51550, 34889, 666, 264, 1785, 12, 2670, 12, 3322, 12, 446, 9469, 399, 3170, 42129, 42, 17, 5819, 11, 6408, 1294, 13, 51818], "temperature": 0.0, "avg_logprob": -0.14195047907468653, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.1823229044675827}, {"id": 248, "seek": 115252, "start": 1152.6, "end": 1157.52, "text": " We note that FC-CLIP is a general meta-architecture that can build on top of several modern segmentation", "tokens": [50368, 492, 3637, 300, 27168, 12, 22458, 9139, 307, 257, 2674, 19616, 12, 1178, 5739, 540, 300, 393, 1322, 322, 1192, 295, 2940, 4363, 9469, 399, 50614], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 249, "seek": 115252, "start": 1157.52, "end": 1160.48, "text": " methods, reference 19, 89.", "tokens": [50614, 7150, 11, 6408, 1294, 11, 31877, 13, 50762], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 250, "seek": 115252, "start": 1160.48, "end": 1162.6, "text": " Our approach offers several advantages.", "tokens": [50762, 2621, 3109, 7736, 2940, 14906, 13, 50868], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 251, "seek": 115252, "start": 1162.6, "end": 1166.62, "text": " By freezing and sharing the backbone features, our model is significantly more efficient", "tokens": [50868, 3146, 20200, 293, 5414, 264, 34889, 4122, 11, 527, 2316, 307, 10591, 544, 7148, 51069], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 252, "seek": 115252, "start": 1166.62, "end": 1170.96, "text": " during both training and testing, i.e., avoiding feature duplication.", "tokens": [51069, 1830, 1293, 3097, 293, 4997, 11, 741, 13, 68, 7933, 20220, 4111, 17154, 399, 13, 51286], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 253, "seek": 115252, "start": 1170.96, "end": 1175.68, "text": " The CNN-based CLIP backbone not only transfers well to different input resolutions, from", "tokens": [51286, 440, 24859, 12, 6032, 12855, 9139, 34889, 406, 787, 29137, 731, 281, 819, 4846, 32179, 11, 490, 51522], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 254, "seek": 115252, "start": 1175.68, "end": 1180.74, "text": " its pre-trained image size, but also generates multi-scale feature maps, seamlessly compatible", "tokens": [51522, 1080, 659, 12, 17227, 2001, 3256, 2744, 11, 457, 611, 23815, 4825, 12, 20033, 4111, 11317, 11, 38083, 18218, 51775], "temperature": 0.0, "avg_logprob": -0.07574926905271386, "compression_ratio": 1.5962732919254659, "no_speech_prob": 0.047414083033800125}, {"id": 255, "seek": 118074, "start": 1180.96, "end": 1184.86, "text": " with modern segmentation methods, reference 19, 89.", "tokens": [50375, 365, 4363, 9469, 399, 7150, 11, 6408, 1294, 11, 31877, 13, 50570], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 256, "seek": 118074, "start": 1184.86, "end": 1190.02, "text": " At a high level, FC-CLIP consists of three components, class-agnostic mask generator,", "tokens": [50570, 1711, 257, 1090, 1496, 11, 27168, 12, 22458, 9139, 14689, 295, 1045, 6677, 11, 1508, 12, 4535, 19634, 6094, 19265, 11, 50828], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 257, "seek": 118074, "start": 1190.02, "end": 1193.86, "text": " in vocabulary classifier, and out-of-vocabulary classifier.", "tokens": [50828, 294, 19864, 1508, 9902, 11, 293, 484, 12, 2670, 12, 20836, 19189, 1508, 9902, 13, 51020], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 258, "seek": 118074, "start": 1193.86, "end": 1199.34, "text": " We detail each component below, class-agnostic mask generator following MASK2 former, reference", "tokens": [51020, 492, 2607, 1184, 6542, 2507, 11, 1508, 12, 4535, 19634, 6094, 19265, 3480, 42129, 42, 17, 5819, 11, 6408, 51294], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 259, "seek": 118074, "start": 1199.34, "end": 1200.34, "text": " 19.", "tokens": [51294, 1294, 13, 51344], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 260, "seek": 118074, "start": 1200.34, "end": 1205.86, "text": " We use a pixel decoder enhanced with multi-scale deformable attention, reference 98, to improve", "tokens": [51344, 492, 764, 257, 19261, 979, 19866, 21191, 365, 4825, 12, 20033, 36094, 712, 3202, 11, 6408, 20860, 11, 281, 3470, 51620], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 261, "seek": 118074, "start": 1205.86, "end": 1209.68, "text": " the features extracted from the frozen CNN-based CLIP backbone.", "tokens": [51620, 264, 4122, 34086, 490, 264, 12496, 24859, 12, 6032, 12855, 9139, 34889, 13, 51811], "temperature": 0.0, "avg_logprob": -0.12605980115059096, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.49180683493614197}, {"id": 262, "seek": 120968, "start": 1209.68, "end": 1214.92, "text": " The enhanced pixel features, together with a set of object queries, reference 7, 78,", "tokens": [50364, 440, 21191, 19261, 4122, 11, 1214, 365, 257, 992, 295, 2657, 24109, 11, 6408, 1614, 11, 26369, 11, 50626], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 263, "seek": 120968, "start": 1214.92, "end": 1219.88, "text": " are then passed through a series of mask decoders, where each consists of masked cross-attention,", "tokens": [50626, 366, 550, 4678, 807, 257, 2638, 295, 6094, 979, 378, 433, 11, 689, 1184, 14689, 295, 45249, 3278, 12, 1591, 1251, 11, 50874], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 264, "seek": 120968, "start": 1219.88, "end": 1224.92, "text": " reference 19, self-attention, reference 76, and a feed-forward network.", "tokens": [50874, 6408, 1294, 11, 2698, 12, 1591, 1251, 11, 6408, 24733, 11, 293, 257, 3154, 12, 13305, 3209, 13, 51126], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 265, "seek": 120968, "start": 1224.92, "end": 1229.3200000000002, "text": " The resulting segmentation logits are obtained by performing a matrix multiplication between", "tokens": [51126, 440, 16505, 9469, 399, 3565, 1208, 366, 14879, 538, 10205, 257, 8141, 27290, 1296, 51346], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 266, "seek": 120968, "start": 1229.3200000000002, "end": 1231.88, "text": " the object query and pixel features.", "tokens": [51346, 264, 2657, 14581, 293, 19261, 4122, 13, 51474], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 267, "seek": 120968, "start": 1231.88, "end": 1236.2, "text": " The predicted masks are matched with ground-truth masks in a one-to-one manner through Hungarian", "tokens": [51474, 440, 19147, 11830, 366, 21447, 365, 2727, 12, 6903, 2910, 11830, 294, 257, 472, 12, 1353, 12, 546, 9060, 807, 38034, 51690], "temperature": 0.0, "avg_logprob": -0.09114329020182292, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.01971549727022648}, {"id": 268, "seek": 123620, "start": 1236.2, "end": 1239.88, "text": " matching, reference 43, and are supervised accordingly.", "tokens": [50364, 14324, 11, 6408, 17914, 11, 293, 366, 46533, 19717, 13, 50548], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 269, "seek": 123620, "start": 1239.88, "end": 1244.16, "text": " Moreover, as the number of object queries is often greater than the number of labeled", "tokens": [50548, 19838, 11, 382, 264, 1230, 295, 2657, 24109, 307, 2049, 5044, 813, 264, 1230, 295, 21335, 50762], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 270, "seek": 123620, "start": 1244.16, "end": 1248.92, "text": " masks, only a subset of predicted masks are optimized through this matching process.", "tokens": [50762, 11830, 11, 787, 257, 25993, 295, 19147, 11830, 366, 26941, 807, 341, 14324, 1399, 13, 51000], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 271, "seek": 123620, "start": 1248.92, "end": 1253.56, "text": " We apply no penalty to the remaining unmatched proposals, which ensures that more mask proposals", "tokens": [51000, 492, 3079, 572, 16263, 281, 264, 8877, 19334, 24102, 20198, 11, 597, 28111, 300, 544, 6094, 20198, 51232], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 272, "seek": 123620, "start": 1253.56, "end": 1254.92, "text": " are obtained.", "tokens": [51232, 366, 14879, 13, 51300], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 273, "seek": 123620, "start": 1254.92, "end": 1259.78, "text": " In vocabulary classifier once the mask proposals are predicted, they are classified with category", "tokens": [51300, 682, 19864, 1508, 9902, 1564, 264, 6094, 20198, 366, 19147, 11, 436, 366, 20627, 365, 7719, 51543], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 274, "seek": 123620, "start": 1259.78, "end": 1264.28, "text": " text embedding in a contrastive manner, where the class embeddings for each mask and category", "tokens": [51543, 2487, 12240, 3584, 294, 257, 8712, 488, 9060, 11, 689, 264, 1508, 12240, 29432, 337, 1184, 6094, 293, 7719, 51768], "temperature": 0.0, "avg_logprob": -0.09950953722000122, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.6985019445419312}, {"id": 275, "seek": 126428, "start": 1265.28, "end": 1268.12, "text": " are projected into a common embedding space.", "tokens": [50414, 366, 26231, 666, 257, 2689, 12240, 3584, 1901, 13, 50556], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 276, "seek": 126428, "start": 1268.12, "end": 1273.24, "text": " That is, the predicted class probability by in vocabulary classifier is defined as follows.", "tokens": [50556, 663, 307, 11, 264, 19147, 1508, 8482, 538, 294, 19864, 1508, 9902, 307, 7642, 382, 10002, 13, 50812], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 277, "seek": 126428, "start": 1273.24, "end": 1277.72, "text": " For all i equals 1, n where T is a learnable temperature parameter with initialization", "tokens": [50812, 1171, 439, 741, 6915, 502, 11, 297, 689, 314, 307, 257, 1466, 712, 4292, 13075, 365, 5883, 2144, 51036], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 278, "seek": 126428, "start": 1277.72, "end": 1283.8, "text": " of 0.07 to control the sharpness of the distribution, cos is cosine distance measurement, vi is", "tokens": [51036, 295, 1958, 13, 16231, 281, 1969, 264, 8199, 1287, 295, 264, 7316, 11, 3792, 307, 23565, 4560, 13160, 11, 1932, 307, 51340], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 279, "seek": 126428, "start": 1283.8, "end": 1288.3, "text": " the class embeddings for i-th predicted mask, which is obtained by mask pooling over the", "tokens": [51340, 264, 1508, 12240, 29432, 337, 741, 12, 392, 19147, 6094, 11, 597, 307, 14879, 538, 6094, 7005, 278, 670, 264, 51565], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 280, "seek": 126428, "start": 1288.3, "end": 1292.44, "text": " final pixel features from pixel decoder, similar to, reference 28.", "tokens": [51565, 2572, 19261, 4122, 490, 19261, 979, 19866, 11, 2531, 281, 11, 6408, 7562, 13, 51772], "temperature": 0.0, "avg_logprob": -0.18510683447913787, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.6577943563461304}, {"id": 281, "seek": 129244, "start": 1292.92, "end": 1297.24, "text": " Tj is the category name's text embeddings of class j, which is obtained by feeding the", "tokens": [50388, 314, 73, 307, 264, 7719, 1315, 311, 2487, 12240, 29432, 295, 1508, 361, 11, 597, 307, 14879, 538, 12919, 264, 50604], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 282, "seek": 129244, "start": 1297.24, "end": 1300.64, "text": " category name to a clip pre-trained text encoder.", "tokens": [50604, 7719, 1315, 281, 257, 7353, 659, 12, 17227, 2001, 2487, 2058, 19866, 13, 50774], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 283, "seek": 129244, "start": 1300.64, "end": 1304.72, "text": " Note that these category text embeddings only need to be generated once.", "tokens": [50774, 11633, 300, 613, 7719, 2487, 12240, 29432, 787, 643, 281, 312, 10833, 1564, 13, 50978], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 284, "seek": 129244, "start": 1304.72, "end": 1308.88, "text": " They are then kept in memory to serve as text classifiers, and thus it incurs negligible", "tokens": [50978, 814, 366, 550, 4305, 294, 4675, 281, 4596, 382, 2487, 1508, 23463, 11, 293, 8807, 309, 834, 2156, 32570, 964, 51186], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 285, "seek": 129244, "start": 1308.88, "end": 1311.16, "text": " additional cost during training.", "tokens": [51186, 4497, 2063, 1830, 3097, 13, 51300], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 286, "seek": 129244, "start": 1311.16, "end": 1313.6200000000001, "text": " This forms our in vocabulary classifier.", "tokens": [51300, 639, 6422, 527, 294, 19864, 1508, 9902, 13, 51423], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 287, "seek": 129244, "start": 1313.6200000000001, "end": 1318.3200000000002, "text": " Out of vocabulary classifier during inference, however, we notice that using the in vocabulary", "tokens": [51423, 5925, 295, 19864, 1508, 9902, 1830, 38253, 11, 4461, 11, 321, 3449, 300, 1228, 264, 294, 19864, 51658], "temperature": 0.0, "avg_logprob": -0.08838624025868104, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.014500672928988934}, {"id": 288, "seek": 131832, "start": 1318.32, "end": 1323.3999999999999, "text": " classifier alone fails to generalize to completely novel unseen classes, as the model is only", "tokens": [50364, 1508, 9902, 3312, 18199, 281, 2674, 1125, 281, 2584, 7613, 40608, 5359, 11, 382, 264, 2316, 307, 787, 50618], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 289, "seek": 131832, "start": 1323.3999999999999, "end": 1328.56, "text": " trained on a finite set of categories and thus could not recognize diverse novel concepts.", "tokens": [50618, 8895, 322, 257, 19362, 992, 295, 10479, 293, 8807, 727, 406, 5521, 9521, 7613, 10392, 13, 50876], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 290, "seek": 131832, "start": 1328.56, "end": 1333.02, "text": " To address this issue, we introduce an out of vocabulary classifier, which applies mask", "tokens": [50876, 1407, 2985, 341, 2734, 11, 321, 5366, 364, 484, 295, 19864, 1508, 9902, 11, 597, 13165, 6094, 51099], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 291, "seek": 131832, "start": 1333.02, "end": 1337.84, "text": " pooling to the frozen clip backbone features, aiming to borrow the pre-trained, intact,", "tokens": [51099, 7005, 278, 281, 264, 12496, 7353, 34889, 4122, 11, 20253, 281, 11172, 264, 659, 12, 17227, 2001, 11, 23493, 11, 51340], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 292, "seek": 131832, "start": 1337.84, "end": 1340.8, "text": " open vocabulary recognition ability from clip.", "tokens": [51340, 1269, 19864, 11150, 3485, 490, 7353, 13, 51488], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 293, "seek": 131832, "start": 1340.8, "end": 1347.32, "text": " Unlike the other two stage methods, reference 85, 50, 24, 84, where one or multiple forward", "tokens": [51488, 17657, 264, 661, 732, 3233, 7150, 11, 6408, 14695, 11, 2625, 11, 4022, 11, 29018, 11, 689, 472, 420, 3866, 2128, 51814], "temperature": 0.0, "avg_logprob": -0.08680887981853654, "compression_ratio": 1.6360655737704919, "no_speech_prob": 0.12584760785102844}, {"id": 294, "seek": 134732, "start": 1347.32, "end": 1351.98, "text": " processes of clip are needed, the adopted out of vocabulary classifier introduces marginal", "tokens": [50364, 7555, 295, 7353, 366, 2978, 11, 264, 12175, 484, 295, 19864, 1508, 9902, 31472, 16885, 50597], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 295, "seek": 134732, "start": 1351.98, "end": 1356.4399999999998, "text": " additional costs, since the backbone features are already extracted, and only lightweight", "tokens": [50597, 4497, 5497, 11, 1670, 264, 34889, 4122, 366, 1217, 34086, 11, 293, 787, 22052, 50820], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 296, "seek": 134732, "start": 1356.4399999999998, "end": 1358.4399999999998, "text": " mask pooling is performed.", "tokens": [50820, 6094, 7005, 278, 307, 10332, 13, 50920], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 297, "seek": 134732, "start": 1358.4399999999998, "end": 1363.06, "text": " The predicted class probability by out of vocabulary classifier chi, out is then obtained", "tokens": [50920, 440, 19147, 1508, 8482, 538, 484, 295, 19864, 1508, 9902, 13228, 11, 484, 307, 550, 14879, 51151], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 298, "seek": 134732, "start": 1363.06, "end": 1364.8799999999999, "text": " in a manner similar to eq.", "tokens": [51151, 294, 257, 9060, 2531, 281, 308, 80, 13, 51242], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 299, "seek": 134732, "start": 1364.8799999999999, "end": 1365.8799999999999, "text": " 6.", "tokens": [51242, 1386, 13, 51292], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 300, "seek": 134732, "start": 1365.8799999999999, "end": 1370.6399999999999, "text": " By replacing vi with the mask pooled features over frozen clip backbone features.", "tokens": [51292, 3146, 19139, 1932, 365, 264, 6094, 7005, 292, 4122, 670, 12496, 7353, 34889, 4122, 13, 51530], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 301, "seek": 134732, "start": 1370.6399999999999, "end": 1374.72, "text": " This classifier strictly maintains the original clipped feature distribution, allowing us", "tokens": [51530, 639, 1508, 9902, 20792, 33385, 264, 3380, 596, 5529, 4111, 7316, 11, 8293, 505, 51734], "temperature": 0.0, "avg_logprob": -0.13638008965386283, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.08751298487186432}, {"id": 302, "seek": 137472, "start": 1375.72, "end": 1377.68, "text": " brand new categories.", "tokens": [50414, 3360, 777, 10479, 13, 50512], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 303, "seek": 137472, "start": 1377.68, "end": 1382.0, "text": " Note that the out of vocabulary classifier is only performed during testing.", "tokens": [50512, 11633, 300, 264, 484, 295, 19864, 1508, 9902, 307, 787, 10332, 1830, 4997, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 304, "seek": 137472, "start": 1382.0, "end": 1388.04, "text": " Combining in and out of vocabulary classifiers following prior works, reference 30, 28, 44,", "tokens": [50728, 25939, 1760, 294, 293, 484, 295, 19864, 1508, 23463, 3480, 4059, 1985, 11, 6408, 2217, 11, 7562, 11, 16408, 11, 51030], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 305, "seek": 137472, "start": 1388.04, "end": 1393.0, "text": " 84, we employ geometric ensemble to fuse the classification scores between in vocabulary", "tokens": [51030, 29018, 11, 321, 3188, 33246, 19492, 281, 31328, 264, 21538, 13444, 1296, 294, 19864, 51278], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 306, "seek": 137472, "start": 1393.0, "end": 1395.6200000000001, "text": " and out of vocabulary classifiers.", "tokens": [51278, 293, 484, 295, 19864, 1508, 23463, 13, 51409], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 307, "seek": 137472, "start": 1395.6200000000001, "end": 1402.56, "text": " That is, for all j equals 1, c, waric i, j denotes the jth element ofci, and the underscripts", "tokens": [51409, 663, 307, 11, 337, 439, 361, 6915, 502, 11, 269, 11, 1516, 299, 741, 11, 361, 1441, 17251, 264, 361, 392, 4478, 295, 537, 11, 293, 264, 16692, 5944, 82, 51756], "temperature": 0.0, "avg_logprob": -0.1986999326539271, "compression_ratio": 1.7, "no_speech_prob": 0.6786143183708191}, {"id": 308, "seek": 140256, "start": 1402.56, "end": 1407.44, "text": " in and out referred to in vocabulary and out of vocabulary classifier, respectively.", "tokens": [50364, 294, 293, 484, 10839, 281, 294, 19864, 293, 484, 295, 19864, 1508, 9902, 11, 25009, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 309, "seek": 140256, "start": 1407.44, "end": 1413.1599999999999, "text": " Alpha, beta element of, reference 0, 1, balance the predictions between in and out of vocabulary", "tokens": [50608, 20588, 11, 9861, 4478, 295, 11, 6408, 1958, 11, 502, 11, 4772, 264, 21264, 1296, 294, 293, 484, 295, 19864, 50894], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 310, "seek": 140256, "start": 1413.1599999999999, "end": 1416.6399999999999, "text": " classifiers for seen and novel unseen categories.", "tokens": [50894, 1508, 23463, 337, 1612, 293, 7613, 40608, 10479, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 311, "seek": 140256, "start": 1416.6399999999999, "end": 1418.22, "text": " 4.", "tokens": [51068, 1017, 13, 51147], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 312, "seek": 140256, "start": 1418.22, "end": 1419.22, "text": " Experimental results.", "tokens": [51147, 37933, 304, 3542, 13, 51197], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 313, "seek": 140256, "start": 1419.22, "end": 1424.76, "text": " Herein, we provide implementation details of FC clip in sec. 4.1.", "tokens": [51197, 1692, 259, 11, 321, 2893, 11420, 4365, 295, 27168, 7353, 294, 907, 13, 1017, 13, 16, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 314, "seek": 140256, "start": 1424.76, "end": 1428.54, "text": " After setting the stage, we introduce our main results, compared with state-of-the-art", "tokens": [51474, 2381, 3287, 264, 3233, 11, 321, 5366, 527, 2135, 3542, 11, 5347, 365, 1785, 12, 2670, 12, 3322, 12, 446, 51663], "temperature": 0.0, "avg_logprob": -0.1987605461707482, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.2876525819301605}, {"id": 315, "seek": 142854, "start": 1428.54, "end": 1431.98, "text": " methods and ablation studies in sec. 4.2.", "tokens": [50364, 7150, 293, 410, 24278, 5313, 294, 907, 13, 1017, 13, 17, 13, 50536], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 316, "seek": 142854, "start": 1431.98, "end": 1439.02, "text": " 4.1 Implementation details architecture We use Convnext Large Clip, reference 56, 66,", "tokens": [50536, 1017, 13, 16, 4331, 781, 19631, 4365, 9482, 492, 764, 2656, 85, 716, 734, 33092, 2033, 647, 11, 6408, 19687, 11, 21126, 11, 50888], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 317, "seek": 142854, "start": 1439.02, "end": 1446.32, "text": " Backbones from Open Clip, reference 36, one pre-trained on LAION2B, reference 70, dataset.", "tokens": [50888, 5833, 44954, 490, 7238, 2033, 647, 11, 6408, 8652, 11, 472, 659, 12, 17227, 2001, 322, 9855, 40, 1928, 17, 33, 11, 6408, 5285, 11, 28872, 13, 51253], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 318, "seek": 142854, "start": 1446.32, "end": 1451.22, "text": " On top of the clip backbone, we build the mask generator, following mask 2 former, reference", "tokens": [51253, 1282, 1192, 295, 264, 7353, 34889, 11, 321, 1322, 264, 6094, 19265, 11, 3480, 6094, 568, 5819, 11, 6408, 51498], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 319, "seek": 142854, "start": 1451.22, "end": 1452.22, "text": " 19.", "tokens": [51498, 1294, 13, 51548], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 320, "seek": 142854, "start": 1452.22, "end": 1456.74, "text": " Nine mask decoders are employed to generate the class agnostic masks by taking as inputs", "tokens": [51548, 18939, 6094, 979, 378, 433, 366, 20115, 281, 8460, 264, 1508, 623, 77, 19634, 11830, 538, 1940, 382, 15743, 51774], "temperature": 0.0, "avg_logprob": -0.22346376550608668, "compression_ratio": 1.565891472868217, "no_speech_prob": 0.2449249029159546}, {"id": 321, "seek": 145674, "start": 1457.74, "end": 1460.14, "text": " the enhanced pixel features in a set of object queries.", "tokens": [50414, 264, 21191, 19261, 4122, 294, 257, 992, 295, 2657, 24109, 13, 50534], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 322, "seek": 145674, "start": 1460.14, "end": 1465.18, "text": " For in vocabulary classification, following, reference 28, the class embeddings are obtained", "tokens": [50534, 1171, 294, 19864, 21538, 11, 3480, 11, 6408, 7562, 11, 264, 1508, 12240, 29432, 366, 14879, 50786], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 323, "seek": 145674, "start": 1465.18, "end": 1469.1, "text": " by mask pooling the pixel features from the pixel decoder's final output.", "tokens": [50786, 538, 6094, 7005, 278, 264, 19261, 4122, 490, 264, 19261, 979, 19866, 311, 2572, 5598, 13, 50982], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 324, "seek": 145674, "start": 1469.1, "end": 1474.26, "text": " Afterwards, the classification logits, before softmax, is obtained by matrix multiplication", "tokens": [50982, 41357, 11, 264, 21538, 3565, 1208, 11, 949, 2787, 41167, 11, 307, 14879, 538, 8141, 27290, 51240], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 325, "seek": 145674, "start": 1474.26, "end": 1478.52, "text": " between the predicted class embeddings and categories text embeddings.", "tokens": [51240, 1296, 264, 19147, 1508, 12240, 29432, 293, 10479, 2487, 12240, 29432, 13, 51453], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 326, "seek": 145674, "start": 1478.52, "end": 1483.06, "text": " Training strategy We follow, reference 19, and adopt the same training recipe and losses", "tokens": [51453, 20620, 5206, 492, 1524, 11, 6408, 1294, 11, 293, 6878, 264, 912, 3097, 6782, 293, 15352, 51680], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 327, "seek": 145674, "start": 1483.06, "end": 1485.06, "text": " without any special design.", "tokens": [51680, 1553, 604, 2121, 1715, 13, 51780], "temperature": 0.0, "avg_logprob": -0.15356372903894494, "compression_ratio": 1.799283154121864, "no_speech_prob": 0.6987640857696533}, {"id": 328, "seek": 148506, "start": 1485.06, "end": 1492.58, "text": " The training is optimized with Atom W, reference 39, 59, Optimizer and Weight Decay 0.05.", "tokens": [50364, 440, 3097, 307, 26941, 365, 1711, 298, 343, 11, 6408, 15238, 11, 24624, 11, 35013, 6545, 293, 44464, 12427, 320, 1958, 13, 13328, 13, 50740], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 329, "seek": 148506, "start": 1492.58, "end": 1496.1799999999998, "text": " We use a crop size of 1024\u00d71024.", "tokens": [50740, 492, 764, 257, 9086, 2744, 295, 1266, 7911, 37550, 3279, 7911, 13, 50920], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 330, "seek": 148506, "start": 1496.1799999999998, "end": 1501.02, "text": " We employ the learning rate 1\u00d710-4 and a multi-step decay schedule.", "tokens": [50920, 492, 3188, 264, 2539, 3314, 502, 37550, 3279, 12, 19, 293, 257, 4825, 12, 16792, 21039, 7567, 13, 51162], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 331, "seek": 148506, "start": 1501.02, "end": 1505.62, "text": " The training batch size is 16, and the model is trained for 50 epochs on Cocoa Panoptic", "tokens": [51162, 440, 3097, 15245, 2744, 307, 3165, 11, 293, 264, 2316, 307, 8895, 337, 2625, 30992, 28346, 322, 29787, 64, 7557, 5747, 299, 51392], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 332, "seek": 148506, "start": 1505.62, "end": 1508.52, "text": " Training Set, reference 52.", "tokens": [51392, 20620, 8928, 11, 6408, 18079, 13, 51537], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 333, "seek": 148506, "start": 1508.52, "end": 1512.98, "text": " Inference strategy During inference, the shorted side of input images will be resized to 800", "tokens": [51537, 682, 5158, 5206, 6842, 38253, 11, 264, 2099, 292, 1252, 295, 4846, 5267, 486, 312, 725, 1602, 281, 13083, 51760], "temperature": 0.0, "avg_logprob": -0.15883198613705843, "compression_ratio": 1.5482625482625483, "no_speech_prob": 0.0850527361035347}, {"id": 334, "seek": 151298, "start": 1512.98, "end": 1516.5, "text": " while ensuring longer side not exceeds 1333.", "tokens": [50364, 1339, 16882, 2854, 1252, 406, 43305, 3705, 10191, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 335, "seek": 151298, "start": 1516.5, "end": 1521.58, "text": " For cityscapes and mappillary vistas, we increase the shorter side size to 1024.", "tokens": [50540, 1171, 2307, 44239, 5190, 293, 463, 427, 46367, 371, 14858, 11, 321, 3488, 264, 11639, 1252, 2744, 281, 1266, 7911, 13, 50794], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 336, "seek": 151298, "start": 1521.58, "end": 1526.58, "text": " We adopt mask-wise merging scheme, reference 19, for the mask predictions.", "tokens": [50794, 492, 6878, 6094, 12, 3711, 44559, 12232, 11, 6408, 1294, 11, 337, 264, 6094, 21264, 13, 51044], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 337, "seek": 151298, "start": 1526.58, "end": 1530.9, "text": " The out-of-vocabulary classifier is only performed during inference by mask pooling over the", "tokens": [51044, 440, 484, 12, 2670, 12, 20836, 19189, 1508, 9902, 307, 787, 10332, 1830, 38253, 538, 6094, 7005, 278, 670, 264, 51260], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 338, "seek": 151298, "start": 1530.9, "end": 1533.18, "text": " frozen-clip backbone features.", "tokens": [51260, 12496, 12, 21614, 34889, 4122, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 339, "seek": 151298, "start": 1533.18, "end": 1537.94, "text": " The final classification results are then obtained by geometric ensembling in-and-out-of-vocabulary", "tokens": [51374, 440, 2572, 21538, 3542, 366, 550, 14879, 538, 33246, 12567, 2504, 1688, 294, 12, 474, 12, 346, 12, 2670, 12, 20836, 19189, 51612], "temperature": 0.0, "avg_logprob": -0.1291822769023754, "compression_ratio": 1.5880149812734083, "no_speech_prob": 0.21192865073680878}, {"id": 340, "seek": 153794, "start": 1538.1000000000001, "end": 1543.38, "text": " classifiers, reference 30, 28, 44, 84, as in EQ.", "tokens": [50372, 1508, 23463, 11, 6408, 2217, 11, 7562, 11, 16408, 11, 29018, 11, 382, 294, 33580, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15628829755281148, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.5462677478790283}, {"id": 341, "seek": 153794, "start": 1543.38, "end": 1549.22, "text": " 7, where we default \u03b1 equals 0.4 and \u03b2 equals 0.8.", "tokens": [50636, 1614, 11, 689, 321, 7576, 5691, 6915, 1958, 13, 19, 293, 15787, 6915, 1958, 13, 23, 13, 50928], "temperature": 0.0, "avg_logprob": -0.15628829755281148, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.5462677478790283}, {"id": 342, "seek": 153794, "start": 1549.22, "end": 1554.3, "text": " Following prior arts, we also adopt prompt engineering from, reference 28, 84, and prompt", "tokens": [50928, 19192, 4059, 8609, 11, 321, 611, 6878, 12391, 7043, 490, 11, 6408, 7562, 11, 29018, 11, 293, 12391, 51182], "temperature": 0.0, "avg_logprob": -0.15628829755281148, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.5462677478790283}, {"id": 343, "seek": 153794, "start": 1554.3, "end": 1557.02, "text": " templates from, reference 30, 50.", "tokens": [51182, 21165, 490, 11, 6408, 2217, 11, 2625, 13, 51318], "temperature": 0.0, "avg_logprob": -0.15628829755281148, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.5462677478790283}, {"id": 344, "seek": 153794, "start": 1557.02, "end": 1563.14, "text": " If not specified, FC-CLIP is only trained on Cocoa Panoptic Dataset, reference 52.", "tokens": [51318, 759, 406, 22206, 11, 27168, 12, 22458, 9139, 307, 787, 8895, 322, 29787, 64, 7557, 5747, 299, 9315, 296, 302, 11, 6408, 18079, 13, 51624], "temperature": 0.0, "avg_logprob": -0.15628829755281148, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.5462677478790283}, {"id": 345, "seek": 156314, "start": 1563.14, "end": 1570.0600000000002, "text": " Following prior works, reference 28, 84, we zero-shot evaluate the model on ADE20K, reference", "tokens": [50364, 19192, 4059, 1985, 11, 6408, 7562, 11, 29018, 11, 321, 4018, 12, 18402, 13059, 264, 2316, 322, 9135, 36, 2009, 42, 11, 6408, 50710], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 346, "seek": 156314, "start": 1570.0600000000002, "end": 1576.7800000000002, "text": " 95, cityscapes, reference 21, and mappillary vistas, reference 62, for open-vocabulary", "tokens": [50710, 13420, 11, 2307, 44239, 5190, 11, 6408, 5080, 11, 293, 463, 427, 46367, 371, 14858, 11, 6408, 24536, 11, 337, 1269, 12, 20836, 19189, 51046], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 347, "seek": 156314, "start": 1576.7800000000002, "end": 1578.8000000000002, "text": " panoptic segmentation.", "tokens": [51046, 2462, 5747, 299, 9469, 399, 13, 51147], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 348, "seek": 156314, "start": 1578.8000000000002, "end": 1584.22, "text": " We also report open-vocabulary semantic segmentation results on those datasets along with Pascal", "tokens": [51147, 492, 611, 2275, 1269, 12, 20836, 19189, 47982, 9469, 399, 3542, 322, 729, 42856, 2051, 365, 41723, 51418], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 349, "seek": 156314, "start": 1584.22, "end": 1587.1000000000001, "text": " datasets, reference 26, 61.", "tokens": [51418, 42856, 11, 6408, 7551, 11, 28294, 13, 51562], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 350, "seek": 156314, "start": 1587.1000000000001, "end": 1592.7, "text": " The panoptic segmentation results are evaluated with the panoptic quality, PQ, reference 42,", "tokens": [51562, 440, 2462, 5747, 299, 9469, 399, 3542, 366, 25509, 365, 264, 2462, 5747, 299, 3125, 11, 430, 48, 11, 6408, 14034, 11, 51842], "temperature": 0.0, "avg_logprob": -0.11547233784093267, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.11275693774223328}, {"id": 351, "seek": 159270, "start": 1592.82, "end": 1598.14, "text": " average precision, AP, and mean intersection over union, MIOU, and semantic segmentation", "tokens": [50370, 4274, 18356, 11, 5372, 11, 293, 914, 15236, 670, 11671, 11, 13696, 4807, 11, 293, 47982, 9469, 399, 50636], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 352, "seek": 159270, "start": 1598.14, "end": 1601.94, "text": " is evaluated with MIOU, reference 26.", "tokens": [50636, 307, 25509, 365, 13696, 4807, 11, 6408, 7551, 13, 50826], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 353, "seek": 159270, "start": 1601.94, "end": 1605.94, "text": " Note that all results are obtained with the same single checkpoint trained on Cocoa Panoptic", "tokens": [50826, 11633, 300, 439, 3542, 366, 14879, 365, 264, 912, 2167, 42269, 8895, 322, 29787, 64, 7557, 5747, 299, 51026], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 354, "seek": 159270, "start": 1605.94, "end": 1607.5, "text": " Data only.", "tokens": [51026, 11888, 787, 13, 51104], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 355, "seek": 159270, "start": 1607.5, "end": 1608.5, "text": " 4.2.", "tokens": [51104, 1017, 13, 17, 13, 51154], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 356, "seek": 159270, "start": 1608.5, "end": 1609.98, "text": " Results.", "tokens": [51154, 5015, 33361, 13, 51228], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 357, "seek": 159270, "start": 1609.98, "end": 1614.9, "text": " We summarize the main results for open-vocabulary panoptic segmentation and semantic segmentation", "tokens": [51228, 492, 20858, 264, 2135, 3542, 337, 1269, 12, 20836, 19189, 2462, 5747, 299, 9469, 399, 293, 47982, 9469, 399, 51474], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 358, "seek": 159270, "start": 1614.9, "end": 1615.9, "text": " in tab.", "tokens": [51474, 294, 4421, 13, 51524], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 359, "seek": 159270, "start": 1615.9, "end": 1616.9, "text": " 1, tab.", "tokens": [51524, 502, 11, 4421, 13, 51574], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 360, "seek": 159270, "start": 1616.9, "end": 1617.9, "text": " 2 in tab.", "tokens": [51574, 568, 294, 4421, 13, 51624], "temperature": 0.0, "avg_logprob": -0.19929372484438887, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.078059621155262}, {"id": 361, "seek": 161790, "start": 1617.98, "end": 1623.02, "text": " 3, where we train FC clip on Cocoa trains set with panoptic annotation and evaluated", "tokens": [50368, 805, 11, 689, 321, 3847, 27168, 7353, 322, 29787, 64, 16329, 992, 365, 2462, 5747, 299, 48654, 293, 25509, 50620], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 362, "seek": 161790, "start": 1623.02, "end": 1626.18, "text": " on various datasets in a zero-shot manner.", "tokens": [50620, 322, 3683, 42856, 294, 257, 4018, 12, 18402, 9060, 13, 50778], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 363, "seek": 161790, "start": 1626.18, "end": 1630.5800000000002, "text": " Open-vocabulary panoptic segmentation evaluation on ADE20K in tab.", "tokens": [50778, 7238, 12, 20836, 19189, 2462, 5747, 299, 9469, 399, 13344, 322, 9135, 36, 2009, 42, 294, 4421, 13, 50998], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 364, "seek": 161790, "start": 1630.5800000000002, "end": 1637.3400000000001, "text": " 1, we compare our FC clip with other state-of-the-art methods on ADE20K, reference 95, the main", "tokens": [50998, 502, 11, 321, 6794, 527, 27168, 7353, 365, 661, 1785, 12, 2670, 12, 3322, 12, 446, 7150, 322, 9135, 36, 2009, 42, 11, 6408, 13420, 11, 264, 2135, 51336], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 365, "seek": 161790, "start": 1637.3400000000001, "end": 1641.7, "text": " testbed of zero-shot open-vocabulary panoptic segmentation.", "tokens": [51336, 1500, 2883, 295, 4018, 12, 18402, 1269, 12, 20836, 19189, 2462, 5747, 299, 9469, 399, 13, 51554], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 366, "seek": 161790, "start": 1641.7, "end": 1645.5, "text": " As shown in the table, our method achieves significantly better performance compared", "tokens": [51554, 1018, 4898, 294, 264, 3199, 11, 527, 3170, 3538, 977, 10591, 1101, 3389, 5347, 51744], "temperature": 0.0, "avg_logprob": -0.09774601559679047, "compression_ratio": 1.69921875, "no_speech_prob": 0.04206758365035057}, {"id": 367, "seek": 164550, "start": 1645.5, "end": 1653.54, "text": " to mask clip, reference 24, with plus 11.7pq, plus 10.8 AP and plus 10.4 MIOU, even though", "tokens": [50364, 281, 6094, 7353, 11, 6408, 4022, 11, 365, 1804, 2975, 13, 22, 79, 80, 11, 1804, 1266, 13, 23, 5372, 293, 1804, 1266, 13, 19, 13696, 4807, 11, 754, 1673, 50766], "temperature": 0.0, "avg_logprob": -0.1836482629008677, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.45273569226264954}, {"id": 368, "seek": 164550, "start": 1653.54, "end": 1659.58, "text": " we use fewer frozen, minus 66m, and trainable, minus 42m, parameters.", "tokens": [50766, 321, 764, 13366, 12496, 11, 3175, 21126, 76, 11, 293, 3847, 712, 11, 3175, 14034, 76, 11, 9834, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1836482629008677, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.45273569226264954}, {"id": 369, "seek": 164550, "start": 1659.58, "end": 1665.86, "text": " When compared to the concurrent methods free seg, reference 65, and ODISE, reference 84,", "tokens": [51068, 1133, 5347, 281, 264, 37702, 7150, 1737, 3896, 11, 6408, 11624, 11, 293, 422, 35, 19413, 11, 6408, 29018, 11, 51382], "temperature": 0.0, "avg_logprob": -0.1836482629008677, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.45273569226264954}, {"id": 370, "seek": 164550, "start": 1665.86, "end": 1668.1, "text": " the advantage of FC clip persists.", "tokens": [51382, 264, 5002, 295, 27168, 7353, 868, 1751, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1836482629008677, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.45273569226264954}, {"id": 371, "seek": 166810, "start": 1668.34, "end": 1675.5, "text": " FC clip is plus 10.5pq, plus 10.3 AP, and plus 9.5 MIOU better than free seg without", "tokens": [50376, 27168, 7353, 307, 1804, 1266, 13, 20, 79, 80, 11, 1804, 1266, 13, 18, 5372, 11, 293, 1804, 1722, 13, 20, 13696, 4807, 1101, 813, 1737, 3896, 1553, 50734], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 372, "seek": 166810, "start": 1675.5, "end": 1680.3, "text": " using Cocoa stuff annotations, reference 5, which contains more semantic classes than", "tokens": [50734, 1228, 29787, 64, 1507, 25339, 763, 11, 6408, 1025, 11, 597, 8306, 544, 47982, 5359, 813, 50974], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 373, "seek": 166810, "start": 1680.3, "end": 1681.98, "text": " Cocoa panoptic.", "tokens": [50974, 29787, 64, 2462, 5747, 299, 13, 51058], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 374, "seek": 166810, "start": 1681.98, "end": 1690.06, "text": " Our pq, AP, MIOU score are also plus 4.2, plus 2.4, plus 4.2 higher than ODISE under", "tokens": [51058, 2621, 280, 80, 11, 5372, 11, 13696, 4807, 6175, 366, 611, 1804, 1017, 13, 17, 11, 1804, 568, 13, 19, 11, 1804, 1017, 13, 17, 2946, 813, 422, 35, 19413, 833, 51462], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 375, "seek": 166810, "start": 1690.06, "end": 1692.26, "text": " the same training settings.", "tokens": [51462, 264, 912, 3097, 6257, 13, 51572], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 376, "seek": 166810, "start": 1692.26, "end": 1697.6599999999999, "text": " Compared to ODISE with caption, reference 14, for supervision, our model still outperforms", "tokens": [51572, 30539, 281, 422, 35, 19413, 365, 31974, 11, 6408, 3499, 11, 337, 32675, 11, 527, 2316, 920, 484, 26765, 82, 51842], "temperature": 0.0, "avg_logprob": -0.09660867055257162, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.08384072780609131}, {"id": 377, "seek": 169766, "start": 1697.74, "end": 1701.8600000000001, "text": " it by plus 3.4pq, setting a new state of the art record.", "tokens": [50368, 309, 538, 1804, 805, 13, 19, 79, 80, 11, 3287, 257, 777, 1785, 295, 264, 1523, 2136, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14789915084838867, "compression_ratio": 1.4638783269961977, "no_speech_prob": 0.16442491114139557}, {"id": 378, "seek": 169766, "start": 1701.8600000000001, "end": 1707.66, "text": " Meanwhile, it is noticeable that our model has 6.3 times, 5.9 times, significantly fewer", "tokens": [50574, 13879, 11, 309, 307, 26041, 300, 527, 2316, 575, 1386, 13, 18, 1413, 11, 1025, 13, 24, 1413, 11, 10591, 13366, 50864], "temperature": 0.0, "avg_logprob": -0.14789915084838867, "compression_ratio": 1.4638783269961977, "no_speech_prob": 0.16442491114139557}, {"id": 379, "seek": 169766, "start": 1707.66, "end": 1713.14, "text": " frozen, total, parameters compared to ODISE, which utilizes a strong large backbone from", "tokens": [50864, 12496, 11, 3217, 11, 9834, 5347, 281, 422, 35, 19413, 11, 597, 4976, 5660, 257, 2068, 2416, 34889, 490, 51138], "temperature": 0.0, "avg_logprob": -0.14789915084838867, "compression_ratio": 1.4638783269961977, "no_speech_prob": 0.16442491114139557}, {"id": 380, "seek": 169766, "start": 1713.14, "end": 1717.42, "text": " stable diffusion, reference 68, for feature extraction.", "tokens": [51138, 8351, 25242, 11, 6408, 23317, 11, 337, 4111, 30197, 13, 51352], "temperature": 0.0, "avg_logprob": -0.14789915084838867, "compression_ratio": 1.4638783269961977, "no_speech_prob": 0.16442491114139557}, {"id": 381, "seek": 169766, "start": 1717.42, "end": 1723.3400000000001, "text": " Open vocabulary panoptic segmentation evaluation on street view datasets in tab 2, we evaluate", "tokens": [51352, 7238, 19864, 2462, 5747, 299, 9469, 399, 13344, 322, 4838, 1910, 42856, 294, 4421, 568, 11, 321, 13059, 51648], "temperature": 0.0, "avg_logprob": -0.14789915084838867, "compression_ratio": 1.4638783269961977, "no_speech_prob": 0.16442491114139557}, {"id": 382, "seek": 172334, "start": 1723.34, "end": 1728.06, "text": " on cityscapes and mappaleri vistas, which focus on street driving scenes.", "tokens": [50364, 322, 2307, 44239, 5190, 293, 463, 427, 304, 16310, 371, 14858, 11, 597, 1879, 322, 4838, 4840, 8026, 13, 50600], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 383, "seek": 172334, "start": 1728.06, "end": 1732.6999999999998, "text": " Compared to state of the art method ODISE, FC clip achieves better performances on both", "tokens": [50600, 30539, 281, 1785, 295, 264, 1523, 3170, 422, 35, 19413, 11, 27168, 7353, 3538, 977, 1101, 16087, 322, 1293, 50832], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 384, "seek": 172334, "start": 1732.6999999999998, "end": 1733.6999999999998, "text": " datasets.", "tokens": [50832, 42856, 13, 50882], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 385, "seek": 172334, "start": 1733.6999999999998, "end": 1740.9399999999998, "text": " Specifically, it outperforms ODISE by plus 4.0pq and plus 20.1pq on mappaleri vistas", "tokens": [50882, 26058, 11, 309, 484, 26765, 82, 422, 35, 19413, 538, 1804, 1017, 13, 15, 79, 80, 293, 1804, 945, 13, 16, 79, 80, 322, 463, 427, 304, 16310, 371, 14858, 51244], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 386, "seek": 172334, "start": 1740.9399999999998, "end": 1742.98, "text": " and cityscapes, respectively.", "tokens": [51244, 293, 2307, 44239, 5190, 11, 25009, 13, 51346], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 387, "seek": 172334, "start": 1742.98, "end": 1748.3799999999999, "text": " Notably, FC clip has a slightly lower SQ, which indicates our mask generator is actually", "tokens": [51346, 1726, 1188, 11, 27168, 7353, 575, 257, 4748, 3126, 318, 48, 11, 597, 16203, 527, 6094, 19265, 307, 767, 51616], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 388, "seek": 172334, "start": 1748.3799999999999, "end": 1753.26, "text": " weaker than the one in ODISE, which utilizes a much larger backbone.", "tokens": [51616, 24286, 813, 264, 472, 294, 422, 35, 19413, 11, 597, 4976, 5660, 257, 709, 4833, 34889, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12695234136063924, "compression_ratio": 1.5913978494623655, "no_speech_prob": 0.22797399759292603}, {"id": 389, "seek": 175326, "start": 1753.26, "end": 1758.06, "text": " Open vocabulary semantics segmentation evaluation Although our model was trained on cocoa panoptic", "tokens": [50364, 7238, 19864, 4361, 45298, 9469, 399, 13344, 5780, 527, 2316, 390, 8895, 322, 30634, 2462, 5747, 299, 50604], "temperature": 0.0, "avg_logprob": -0.14440859919009003, "compression_ratio": 1.7723577235772359, "no_speech_prob": 0.10966954380273819}, {"id": 390, "seek": 175326, "start": 1758.06, "end": 1763.14, "text": " data only, it also performs well on open vocabulary semantic segmentation.", "tokens": [50604, 1412, 787, 11, 309, 611, 26213, 731, 322, 1269, 19864, 47982, 9469, 399, 13, 50858], "temperature": 0.0, "avg_logprob": -0.14440859919009003, "compression_ratio": 1.7723577235772359, "no_speech_prob": 0.10966954380273819}, {"id": 391, "seek": 175326, "start": 1763.14, "end": 1768.56, "text": " In tab 3, we report our model's performance on various benchmarks against other open vocabulary", "tokens": [50858, 682, 4421, 805, 11, 321, 2275, 527, 2316, 311, 3389, 322, 3683, 43751, 1970, 661, 1269, 19864, 51129], "temperature": 0.0, "avg_logprob": -0.14440859919009003, "compression_ratio": 1.7723577235772359, "no_speech_prob": 0.10966954380273819}, {"id": 392, "seek": 175326, "start": 1768.56, "end": 1773.3, "text": " segmentation models, where FC clip shows an overall superior performance.", "tokens": [51129, 9469, 399, 5245, 11, 689, 27168, 7353, 3110, 364, 4787, 13028, 3389, 13, 51366], "temperature": 0.0, "avg_logprob": -0.14440859919009003, "compression_ratio": 1.7723577235772359, "no_speech_prob": 0.10966954380273819}, {"id": 393, "seek": 175326, "start": 1773.3, "end": 1778.7, "text": " Specifically, with the same training annotations used, FC clip outperforms mask clip by plus", "tokens": [51366, 26058, 11, 365, 264, 912, 3097, 25339, 763, 1143, 11, 27168, 7353, 484, 26765, 82, 6094, 7353, 538, 1804, 51636], "temperature": 0.0, "avg_logprob": -0.14440859919009003, "compression_ratio": 1.7723577235772359, "no_speech_prob": 0.10966954380273819}, {"id": 394, "seek": 177870, "start": 1778.7, "end": 1791.74, "text": " 6.6, plus 8.2, plus 10.4, plus 12.5 miou across A847, PC459, A150, and PC59, respectively.", "tokens": [50364, 1386, 13, 21, 11, 1804, 1649, 13, 17, 11, 1804, 1266, 13, 19, 11, 1804, 2272, 13, 20, 2752, 263, 2108, 316, 23, 14060, 11, 6465, 8465, 24, 11, 316, 20120, 11, 293, 6465, 19600, 11, 25009, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11768000324567159, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.6106520295143127}, {"id": 395, "seek": 177870, "start": 1791.74, "end": 1796.1000000000001, "text": " Compared to methods with caption annotations, FC clip persists its advantages, where it", "tokens": [51016, 30539, 281, 7150, 365, 31974, 25339, 763, 11, 27168, 7353, 868, 1751, 1080, 14906, 11, 689, 309, 51234], "temperature": 0.0, "avg_logprob": -0.11768000324567159, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.6106520295143127}, {"id": 396, "seek": 177870, "start": 1796.1000000000001, "end": 1805.14, "text": " outperforms ODISE, caption, by plus 3.8, plus 4.4, plus 5.4, plus 3.1 miou across datasets", "tokens": [51234, 484, 26765, 82, 422, 35, 19413, 11, 31974, 11, 538, 1804, 805, 13, 23, 11, 1804, 1017, 13, 19, 11, 1804, 1025, 13, 19, 11, 1804, 805, 13, 16, 2752, 263, 2108, 42856, 51686], "temperature": 0.0, "avg_logprob": -0.11768000324567159, "compression_ratio": 1.4157894736842105, "no_speech_prob": 0.6106520295143127}, {"id": 397, "seek": 180514, "start": 1805.14, "end": 1812.18, "text": " A847, PC459, A150, PC59, respectively.", "tokens": [50364, 316, 23, 14060, 11, 6465, 8465, 24, 11, 316, 20120, 11, 6465, 19600, 11, 25009, 13, 50716], "temperature": 0.0, "avg_logprob": -0.10046140882703992, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.3918055295944214}, {"id": 398, "seek": 180514, "start": 1812.18, "end": 1816.7800000000002, "text": " Against other open vocabulary semantic segmentation methods, our model maintains its advantages", "tokens": [50716, 29995, 661, 1269, 19864, 47982, 9469, 399, 7150, 11, 527, 2316, 33385, 1080, 14906, 50946], "temperature": 0.0, "avg_logprob": -0.10046140882703992, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.3918055295944214}, {"id": 399, "seek": 180514, "start": 1816.7800000000002, "end": 1821.38, "text": " across different datasets, despite being trained solely with panoptic annotations.", "tokens": [50946, 2108, 819, 42856, 11, 7228, 885, 8895, 23309, 365, 2462, 5747, 299, 25339, 763, 13, 51176], "temperature": 0.0, "avg_logprob": -0.10046140882703992, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.3918055295944214}, {"id": 400, "seek": 180514, "start": 1821.38, "end": 1826.22, "text": " Furthermore, it demonstrates comparable performance to state-of-the-art open vocabulary semantic", "tokens": [51176, 23999, 11, 309, 31034, 25323, 3389, 281, 1785, 12, 2670, 12, 3322, 12, 446, 1269, 19864, 47982, 51418], "temperature": 0.0, "avg_logprob": -0.10046140882703992, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.3918055295944214}, {"id": 401, "seek": 180514, "start": 1826.22, "end": 1830.98, "text": " segmentation methods, which utilize the Cocoa Stuff dataset as their training set.", "tokens": [51418, 9469, 399, 7150, 11, 597, 16117, 264, 29787, 64, 31347, 28872, 382, 641, 3097, 992, 13, 51656], "temperature": 0.0, "avg_logprob": -0.10046140882703992, "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.3918055295944214}, {"id": 402, "seek": 183098, "start": 1830.98, "end": 1837.18, "text": " The Cocoa Stuff dataset comprises 171 classes, 38 more classes than Cocoa Panoptic, and offers", "tokens": [50364, 440, 29787, 64, 31347, 28872, 16802, 3598, 3282, 16, 5359, 11, 12843, 544, 5359, 813, 29787, 64, 7557, 5747, 299, 11, 293, 7736, 50674], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 403, "seek": 183098, "start": 1837.18, "end": 1841.26, "text": " highly desirable annotations for semantic segmentation tasks.", "tokens": [50674, 5405, 30533, 25339, 763, 337, 47982, 9469, 399, 9608, 13, 50878], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 404, "seek": 183098, "start": 1841.26, "end": 1844.98, "text": " It is worth mentioning that these methods build their approach on top of VIT-L, with", "tokens": [50878, 467, 307, 3163, 18315, 300, 613, 7150, 1322, 641, 3109, 322, 1192, 295, 691, 3927, 12, 43, 11, 365, 51064], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 405, "seek": 183098, "start": 1844.98, "end": 1849.8600000000001, "text": " extra designs, reference 86, resulting in a significantly larger model size compared", "tokens": [51064, 2857, 11347, 11, 6408, 26687, 11, 16505, 294, 257, 10591, 4833, 2316, 2744, 5347, 51308], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 406, "seek": 183098, "start": 1849.8600000000001, "end": 1854.98, "text": " to our deployed Convinext-L, 304M vs. 198M.", "tokens": [51308, 281, 527, 17826, 2656, 85, 533, 734, 12, 43, 11, 2217, 19, 44, 12041, 13, 6375, 44, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 407, "seek": 183098, "start": 1854.98, "end": 1860.14, "text": " Despite the disparity in model size, FC clip remains competitive in terms of performance.", "tokens": [51564, 11334, 264, 47415, 294, 2316, 2744, 11, 27168, 7353, 7023, 10043, 294, 2115, 295, 3389, 13, 51822], "temperature": 0.0, "avg_logprob": -0.14034019436752587, "compression_ratio": 1.5231788079470199, "no_speech_prob": 0.19906164705753326}, {"id": 408, "seek": 186014, "start": 1860.14, "end": 1865.5, "text": " Specifically, FC clip outperforms state-of-the-art open vocabulary semantic segmentation methods", "tokens": [50364, 26058, 11, 27168, 7353, 484, 26765, 82, 1785, 12, 2670, 12, 3322, 12, 446, 1269, 19864, 47982, 9469, 399, 7150, 50632], "temperature": 0.0, "avg_logprob": -0.16547323010631443, "compression_ratio": 1.45136186770428, "no_speech_prob": 0.3995538055896759}, {"id": 409, "seek": 186014, "start": 1865.5, "end": 1874.66, "text": " SAN, reference 86, by 1.1 and 1.1 miou on the challenging A847 and PC459 datasets.", "tokens": [50632, 49557, 11, 6408, 26687, 11, 538, 502, 13, 16, 293, 502, 13, 16, 2752, 263, 322, 264, 7595, 316, 23, 14060, 293, 6465, 8465, 24, 42856, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16547323010631443, "compression_ratio": 1.45136186770428, "no_speech_prob": 0.3995538055896759}, {"id": 410, "seek": 186014, "start": 1874.66, "end": 1880.1200000000001, "text": " Inference speed we provide a comparison of FPS, frames per second, in tab.4.", "tokens": [51090, 682, 5158, 3073, 321, 2893, 257, 9660, 295, 26429, 11, 12083, 680, 1150, 11, 294, 4421, 13, 19, 13, 51363], "temperature": 0.0, "avg_logprob": -0.16547323010631443, "compression_ratio": 1.45136186770428, "no_speech_prob": 0.3995538055896759}, {"id": 411, "seek": 186014, "start": 1880.1200000000001, "end": 1885.26, "text": " The proposed FC clip not only demonstrates superior performances, but also enjoys a significant", "tokens": [51363, 440, 10348, 27168, 7353, 406, 787, 31034, 13028, 16087, 11, 457, 611, 29750, 257, 4776, 51620], "temperature": 0.0, "avg_logprob": -0.16547323010631443, "compression_ratio": 1.45136186770428, "no_speech_prob": 0.3995538055896759}, {"id": 412, "seek": 186014, "start": 1885.26, "end": 1886.6200000000001, "text": " fast inference time.", "tokens": [51620, 2370, 38253, 565, 13, 51688], "temperature": 0.0, "avg_logprob": -0.16547323010631443, "compression_ratio": 1.45136186770428, "no_speech_prob": 0.3995538055896759}, {"id": 413, "seek": 188662, "start": 1887.1, "end": 1893.86, "text": " FC clip runs 6.61 times and 7.08 times faster than ODISE evaluated on ADE 20K and Cocoa", "tokens": [50388, 27168, 7353, 6676, 1386, 13, 31537, 1413, 293, 1614, 13, 16133, 1413, 4663, 813, 48447, 19413, 25509, 322, 9135, 36, 945, 42, 293, 29787, 64, 50726], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 414, "seek": 188662, "start": 1893.86, "end": 1896.1, "text": " datasets, respectively.", "tokens": [50726, 42856, 11, 25009, 13, 50838], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 415, "seek": 188662, "start": 1896.1, "end": 1900.5, "text": " Training on ADE 20K and evaluating on Cocoa we further validate the effectiveness of FC", "tokens": [50838, 20620, 322, 9135, 36, 945, 42, 293, 27479, 322, 29787, 64, 321, 3052, 29562, 264, 21208, 295, 27168, 51058], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 416, "seek": 188662, "start": 1900.5, "end": 1903.1399999999999, "text": " clip by using a different training dataset.", "tokens": [51058, 7353, 538, 1228, 257, 819, 3097, 28872, 13, 51190], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 417, "seek": 188662, "start": 1903.1399999999999, "end": 1909.7399999999998, "text": " Specifically, we follow reference 65, 84, to train our model on ADE 20K dataset with", "tokens": [51190, 26058, 11, 321, 1524, 6408, 11624, 11, 29018, 11, 281, 3847, 527, 2316, 322, 9135, 36, 945, 42, 28872, 365, 51520], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 418, "seek": 188662, "start": 1909.7399999999998, "end": 1914.06, "text": " panoptic annotation, and evaluate it on Cocoa panoptic dataset.", "tokens": [51520, 2462, 5747, 299, 48654, 11, 293, 13059, 309, 322, 29787, 64, 2462, 5747, 299, 28872, 13, 51736], "temperature": 0.0, "avg_logprob": -0.17790985107421875, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.20674078166484833}, {"id": 419, "seek": 191406, "start": 1914.06, "end": 1923.02, "text": " As shown in tab.5, FC clip outperforms FreeSeg, reference 65, by plus 10.5pq, and ODISE, reference", "tokens": [50364, 1018, 4898, 294, 4421, 13, 20, 11, 27168, 7353, 484, 26765, 82, 11551, 50, 1146, 11, 6408, 11624, 11, 538, 1804, 1266, 13, 20, 79, 80, 11, 293, 48447, 19413, 11, 6408, 50812], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 420, "seek": 191406, "start": 1923.02, "end": 1927.06, "text": " 84, by plus 2.0pq on Cocoa dataset.", "tokens": [50812, 29018, 11, 538, 1804, 568, 13, 15, 79, 80, 322, 29787, 64, 28872, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 421, "seek": 191406, "start": 1927.06, "end": 1933.86, "text": " Notably, our model actually has a lower SQ, minus 1.4, compared to ODISE, which utilizes", "tokens": [51014, 1726, 1188, 11, 527, 2316, 767, 575, 257, 3126, 318, 48, 11, 3175, 502, 13, 19, 11, 5347, 281, 48447, 19413, 11, 597, 4976, 5660, 51354], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 422, "seek": 191406, "start": 1933.86, "end": 1937.4199999999998, "text": " a much larger backbone and thus has a stronger mask generator.", "tokens": [51354, 257, 709, 4833, 34889, 293, 8807, 575, 257, 7249, 6094, 19265, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 423, "seek": 191406, "start": 1937.4199999999998, "end": 1942.74, "text": " Nevertheless, FC clip still outperforms ODISE significantly with a simple yet effective", "tokens": [51532, 26554, 11, 27168, 7353, 920, 484, 26765, 82, 48447, 19413, 10591, 365, 257, 2199, 1939, 4942, 51798], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 424, "seek": 191406, "start": 1942.74, "end": 1943.74, "text": " design.", "tokens": [51798, 1715, 13, 51848], "temperature": 0.0, "avg_logprob": -0.15197485218877377, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.058328624814748764}, {"id": 425, "seek": 194374, "start": 1944.02, "end": 1948.46, "text": " Fine tuning clip backbone harms performance on novel vocabularies we validate the necessity", "tokens": [50378, 12024, 15164, 7353, 34889, 48505, 3389, 322, 7613, 2329, 455, 1040, 530, 321, 29562, 264, 24217, 50600], "temperature": 0.0, "avg_logprob": -0.12945046632186227, "compression_ratio": 1.8, "no_speech_prob": 0.07366600632667542}, {"id": 426, "seek": 194374, "start": 1948.46, "end": 1953.46, "text": " of freezing clip backbone to ensure a better generalization to novel vocabularies.", "tokens": [50600, 295, 20200, 7353, 34889, 281, 5586, 257, 1101, 2674, 2144, 281, 7613, 2329, 455, 1040, 530, 13, 50850], "temperature": 0.0, "avg_logprob": -0.12945046632186227, "compression_ratio": 1.8, "no_speech_prob": 0.07366600632667542}, {"id": 427, "seek": 194374, "start": 1953.46, "end": 1958.58, "text": " We compare the performance of trainable clip variant and frozen clip variant in Fig.4,", "tokens": [50850, 492, 6794, 264, 3389, 295, 3847, 712, 7353, 17501, 293, 12496, 7353, 17501, 294, 22443, 13, 19, 11, 51106], "temperature": 0.0, "avg_logprob": -0.12945046632186227, "compression_ratio": 1.8, "no_speech_prob": 0.07366600632667542}, {"id": 428, "seek": 194374, "start": 1958.58, "end": 1962.14, "text": " where we use the same mask proposals to ensure a fair comparison.", "tokens": [51106, 689, 321, 764, 264, 912, 6094, 20198, 281, 5586, 257, 3143, 9660, 13, 51284], "temperature": 0.0, "avg_logprob": -0.12945046632186227, "compression_ratio": 1.8, "no_speech_prob": 0.07366600632667542}, {"id": 429, "seek": 194374, "start": 1962.14, "end": 1966.38, "text": " Specifically, we compare the performance on 10 scene classes, which are shared by both", "tokens": [51284, 26058, 11, 321, 6794, 264, 3389, 322, 1266, 4145, 5359, 11, 597, 366, 5507, 538, 1293, 51496], "temperature": 0.0, "avg_logprob": -0.12945046632186227, "compression_ratio": 1.8, "no_speech_prob": 0.07366600632667542}, {"id": 430, "seek": 196638, "start": 1966.38, "end": 1972.8600000000001, "text": " Cocoa and ADE20K, e.g., Person, Sky, and 10 unseen classes, which are only included", "tokens": [50364, 29787, 64, 293, 9135, 36, 2009, 42, 11, 308, 13, 70, 7933, 8443, 11, 9879, 11, 293, 1266, 40608, 5359, 11, 597, 366, 787, 5556, 50688], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 431, "seek": 196638, "start": 1972.8600000000001, "end": 1978.22, "text": " in ADE20K dataset, e.g., Arcade Machine, Dishwasher.", "tokens": [50688, 294, 9135, 36, 2009, 42, 28872, 11, 308, 13, 70, 7933, 21727, 762, 22155, 11, 413, 742, 6569, 511, 13, 50956], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 432, "seek": 196638, "start": 1978.22, "end": 1983.14, "text": " As shown in the figure, tuning clip backbone leads to a worse performance on unseen concepts,", "tokens": [50956, 1018, 4898, 294, 264, 2573, 11, 15164, 7353, 34889, 6689, 281, 257, 5324, 3389, 322, 40608, 10392, 11, 51202], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 433, "seek": 196638, "start": 1983.14, "end": 1986.7800000000002, "text": " which breaks the clip feature alignment and thus loses its recognition ability on a much", "tokens": [51202, 597, 9857, 264, 7353, 4111, 18515, 293, 8807, 18293, 1080, 11150, 3485, 322, 257, 709, 51384], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 434, "seek": 196638, "start": 1986.7800000000002, "end": 1988.3400000000001, "text": " wider vocabulary.", "tokens": [51384, 11842, 19864, 13, 51462], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 435, "seek": 196638, "start": 1988.3400000000001, "end": 1989.3400000000001, "text": " 5.", "tokens": [51462, 1025, 13, 51512], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 436, "seek": 196638, "start": 1989.3400000000001, "end": 1990.74, "text": " Conclusion.", "tokens": [51512, 18200, 6485, 13, 51582], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 437, "seek": 196638, "start": 1990.74, "end": 1995.2, "text": " In this work, we have presented FC clip, a simple yet effective single stage framework", "tokens": [51582, 682, 341, 589, 11, 321, 362, 8212, 27168, 7353, 11, 257, 2199, 1939, 4942, 2167, 3233, 8388, 51805], "temperature": 0.0, "avg_logprob": -0.12354819242619286, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.0518241785466671}, {"id": 438, "seek": 199520, "start": 1995.4, "end": 1997.4, "text": " for open vocabulary segmentation.", "tokens": [50374, 337, 1269, 19864, 9469, 399, 13, 50474], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 439, "seek": 199520, "start": 1997.4, "end": 2002.2, "text": " FC clip shows great potential by building everything on top of a shared frozen convolutional", "tokens": [50474, 27168, 7353, 3110, 869, 3995, 538, 2390, 1203, 322, 1192, 295, 257, 5507, 12496, 45216, 304, 50714], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 440, "seek": 199520, "start": 2002.2, "end": 2006.92, "text": " clip backbone, which not only significantly reduces training and testing costs, but also", "tokens": [50714, 7353, 34889, 11, 597, 406, 787, 10591, 18081, 3097, 293, 4997, 5497, 11, 457, 611, 50950], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 441, "seek": 199520, "start": 2006.92, "end": 2010.28, "text": " establishes a strong baseline on multiple benchmarks.", "tokens": [50950, 8327, 279, 257, 2068, 20518, 322, 3866, 43751, 13, 51118], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 442, "seek": 199520, "start": 2010.28, "end": 2014.16, "text": " Our study demonstrates how to better adapt a pre-trained clip model for downstream dense", "tokens": [51118, 2621, 2979, 31034, 577, 281, 1101, 6231, 257, 659, 12, 17227, 2001, 7353, 2316, 337, 30621, 18011, 51312], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 443, "seek": 199520, "start": 2014.16, "end": 2018.8400000000001, "text": " prediction tasks, which we hope will shed the light on unleashing CLIP's potential for", "tokens": [51312, 17630, 9608, 11, 597, 321, 1454, 486, 14951, 264, 1442, 322, 25272, 11077, 12855, 9139, 311, 3995, 337, 51546], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 444, "seek": 199520, "start": 2018.8400000000001, "end": 2021.72, "text": " other various downstream tasks.", "tokens": [51546, 661, 3683, 30621, 9608, 13, 51690], "temperature": 0.0, "avg_logprob": -0.13677029516182693, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.37720564007759094}, {"id": 445, "seek": 202172, "start": 2021.72, "end": 2026.1200000000001, "text": " The Clip's FC clip presents a simple single stage open vocabulary segmentation framework", "tokens": [50364, 440, 2033, 647, 311, 27168, 7353, 13533, 257, 2199, 2167, 3233, 1269, 19864, 9469, 399, 8388, 50584], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 446, "seek": 202172, "start": 2026.1200000000001, "end": 2028.22, "text": " with state-of-the-art performance.", "tokens": [50584, 365, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 13, 50689], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 447, "seek": 202172, "start": 2028.22, "end": 2031.72, "text": " We note that there exists some interesting research topics to be explored in the near", "tokens": [50689, 492, 3637, 300, 456, 8198, 512, 1880, 2132, 8378, 281, 312, 24016, 294, 264, 2651, 50864], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 448, "seek": 202172, "start": 2031.72, "end": 2037.56, "text": " future, such as better unleashing CLIP's potential in both mask segmentation and classification,", "tokens": [50864, 2027, 11, 1270, 382, 1101, 25272, 11077, 12855, 9139, 311, 3995, 294, 1293, 6094, 9469, 399, 293, 21538, 11, 51156], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 449, "seek": 202172, "start": 2037.56, "end": 2043.48, "text": " how to deal with conflict or overlapping vocabularies, e.g., cat vs. cathead, etc.", "tokens": [51156, 577, 281, 2028, 365, 6596, 420, 33535, 2329, 455, 1040, 530, 11, 308, 13, 70, 7933, 3857, 12041, 13, 3857, 1934, 11, 5183, 13, 51452], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 450, "seek": 202172, "start": 2043.48, "end": 2047.72, "text": " Broader Impact FC clip shows great potential for segmenting and naming every object in", "tokens": [51452, 5425, 8312, 31005, 27168, 7353, 3110, 869, 3995, 337, 9469, 278, 293, 25290, 633, 2657, 294, 51664], "temperature": 0.0, "avg_logprob": -0.21035293110629968, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.3374970555305481}, {"id": 451, "seek": 204772, "start": 2047.72, "end": 2053.08, "text": " the scene, which could facilitate many applications including intelligent home assistance, robots,", "tokens": [50364, 264, 4145, 11, 597, 727, 20207, 867, 5821, 3009, 13232, 1280, 9683, 11, 14733, 11, 50632], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 452, "seek": 204772, "start": 2053.08, "end": 2054.92, "text": " self-driving, etc.", "tokens": [50632, 2698, 12, 47094, 11, 5183, 13, 50724], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 453, "seek": 204772, "start": 2054.92, "end": 2058.88, "text": " Yet it relies on CLIP model pre-trained on the internet data that may be biased, which", "tokens": [50724, 10890, 309, 30910, 322, 12855, 9139, 2316, 659, 12, 17227, 2001, 322, 264, 4705, 1412, 300, 815, 312, 28035, 11, 597, 50922], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 454, "seek": 204772, "start": 2058.88, "end": 2062.64, "text": " calls for future research for calibration to avoid misuse.", "tokens": [50922, 5498, 337, 2027, 2132, 337, 38732, 281, 5042, 3346, 438, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 455, "seek": 204772, "start": 2062.64, "end": 2067.12, "text": " Appendix in the following supplementary materials, we present additional experimental results", "tokens": [51110, 3132, 521, 970, 294, 264, 3480, 15436, 822, 5319, 11, 321, 1974, 4497, 17069, 3542, 51334], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 456, "seek": 204772, "start": 2067.12, "end": 2069.76, "text": " pertaining to the design of FC clip.", "tokens": [51334, 49582, 281, 264, 1715, 295, 27168, 7353, 13, 51466], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 457, "seek": 204772, "start": 2069.76, "end": 2074.3, "text": " Our supplementary analysis also includes comparisons against other methods that specifically address", "tokens": [51466, 2621, 15436, 822, 5215, 611, 5974, 33157, 1970, 661, 7150, 300, 4682, 2985, 51693], "temperature": 0.0, "avg_logprob": -0.12607774280366443, "compression_ratio": 1.6123778501628665, "no_speech_prob": 0.5151739120483398}, {"id": 458, "seek": 207430, "start": 2075.3, "end": 2079.6200000000003, "text": " vocabulary semantic segmentation, ensemble methods, and hyperparameter tuning.", "tokens": [50414, 19864, 47982, 9469, 399, 11, 19492, 7150, 11, 293, 9848, 2181, 335, 2398, 15164, 13, 50630], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 459, "seek": 207430, "start": 2079.6200000000003, "end": 2084.7000000000003, "text": " Furthermore, we provide a quantitative comparison between VIT-based CLIP and CNN-based CLIP across", "tokens": [50630, 23999, 11, 321, 2893, 257, 27778, 9660, 1296, 691, 3927, 12, 6032, 12855, 9139, 293, 24859, 12, 6032, 12855, 9139, 2108, 50884], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 460, "seek": 207430, "start": 2084.7000000000003, "end": 2090.54, "text": " varying input sizes, along with additional visualizations and comprehensive dataset details.", "tokens": [50884, 22984, 4846, 11602, 11, 2051, 365, 4497, 5056, 14455, 293, 13914, 28872, 4365, 13, 51176], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 461, "seek": 207430, "start": 2090.54, "end": 2095.02, "text": " Six additional experimental results fine tuning or freezing CLIP backbone in FC clip in this", "tokens": [51176, 11678, 4497, 17069, 3542, 2489, 15164, 420, 20200, 12855, 9139, 34889, 294, 27168, 7353, 294, 341, 51400], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 462, "seek": 207430, "start": 2095.02, "end": 2099.36, "text": " study, we provide a comprehensive analysis of the impact of fine tuning or freezing the", "tokens": [51400, 2979, 11, 321, 2893, 257, 13914, 5215, 295, 264, 2712, 295, 2489, 15164, 420, 20200, 264, 51617], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 463, "seek": 207430, "start": 2099.36, "end": 2101.6200000000003, "text": " CLIP backbone in our framework.", "tokens": [51617, 12855, 9139, 34889, 294, 527, 8388, 13, 51730], "temperature": 0.0, "avg_logprob": -0.20443118197246662, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.5768276453018188}, {"id": 464, "seek": 210162, "start": 2101.62, "end": 2106.58, "text": " We specifically focus on the PQ scene and PQ unseen metrics, which evaluate the performance", "tokens": [50364, 492, 4682, 1879, 322, 264, 430, 48, 4145, 293, 430, 48, 40608, 16367, 11, 597, 13059, 264, 3389, 50612], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 465, "seek": 210162, "start": 2106.58, "end": 2111.22, "text": " for classes that overlap and do not overlap between the training and testing datasets,", "tokens": [50612, 337, 5359, 300, 19959, 293, 360, 406, 19959, 1296, 264, 3097, 293, 4997, 42856, 11, 50844], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 466, "seek": 210162, "start": 2111.22, "end": 2112.22, "text": " respectively.", "tokens": [50844, 25009, 13, 50894], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 467, "seek": 210162, "start": 2112.22, "end": 2116.38, "text": " To determine whether a class is seen or unseen, we adopt the prompt engineering technique", "tokens": [50894, 1407, 6997, 1968, 257, 1508, 307, 1612, 420, 40608, 11, 321, 6878, 264, 12391, 7043, 6532, 51102], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 468, "seek": 210162, "start": 2116.38, "end": 2121.46, "text": " described in reference 28, which provides synonyms or subcategories of classes.", "tokens": [51102, 7619, 294, 6408, 7562, 11, 597, 6417, 5451, 2526, 2592, 420, 1422, 66, 2968, 2083, 295, 5359, 13, 51356], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 469, "seek": 210162, "start": 2121.46, "end": 2126.22, "text": " Specifically, if any category name in test dataset overlaps with a category name in training", "tokens": [51356, 26058, 11, 498, 604, 7719, 1315, 294, 1500, 28872, 15986, 2382, 365, 257, 7719, 1315, 294, 3097, 51594], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 470, "seek": 210162, "start": 2126.22, "end": 2130.44, "text": " dataset, we consider it as a seen class, otherwise unseen.", "tokens": [51594, 28872, 11, 321, 1949, 309, 382, 257, 1612, 1508, 11, 5911, 40608, 13, 51805], "temperature": 0.0, "avg_logprob": -0.12012600276781166, "compression_ratio": 1.8035087719298246, "no_speech_prob": 0.18697834014892578}, {"id": 471, "seek": 213044, "start": 2130.44, "end": 2135.2000000000003, "text": " As discussed in the main paper, the proposed FC CLIP contains three components, a class", "tokens": [50364, 1018, 7152, 294, 264, 2135, 3035, 11, 264, 10348, 27168, 12855, 9139, 8306, 1045, 6677, 11, 257, 1508, 50602], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 472, "seek": 213044, "start": 2135.2000000000003, "end": 2141.04, "text": " agnostic mask generator, an invocabulary classifier, and an out-of-vocabulary classifier.", "tokens": [50602, 623, 77, 19634, 6094, 19265, 11, 364, 1048, 905, 19189, 1508, 9902, 11, 293, 364, 484, 12, 2670, 12, 20836, 19189, 1508, 9902, 13, 50894], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 473, "seek": 213044, "start": 2141.04, "end": 2145.08, "text": " We thus explore using frozen or trainable CLIP for each component, and summarize the", "tokens": [50894, 492, 8807, 6839, 1228, 12496, 420, 3847, 712, 12855, 9139, 337, 1184, 6542, 11, 293, 20858, 264, 51096], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 474, "seek": 213044, "start": 2145.08, "end": 2146.56, "text": " results in tab.", "tokens": [51096, 3542, 294, 4421, 13, 51170], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 475, "seek": 213044, "start": 2146.56, "end": 2147.56, "text": " 6.", "tokens": [51170, 1386, 13, 51220], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 476, "seek": 213044, "start": 2147.56, "end": 2152.28, "text": " To ensure a fair comparison, all, trainable, modules utilize the same weights, resulting", "tokens": [51220, 1407, 5586, 257, 3143, 9660, 11, 439, 11, 3847, 712, 11, 16679, 16117, 264, 912, 17443, 11, 16505, 51456], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 477, "seek": 213044, "start": 2152.28, "end": 2156.66, "text": " in identical mask proposals and invocabulary classification results.", "tokens": [51456, 294, 14800, 6094, 20198, 293, 1048, 905, 19189, 21538, 3542, 13, 51675], "temperature": 0.0, "avg_logprob": -0.10658073425292969, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.07367533445358276}, {"id": 478, "seek": 215666, "start": 2156.66, "end": 2161.3599999999997, "text": " Our findings reveal that an invocabulary classifier built upon a trainable CLIP backbone achieves", "tokens": [50364, 2621, 16483, 10658, 300, 364, 1048, 905, 19189, 1508, 9902, 3094, 3564, 257, 3847, 712, 12855, 9139, 34889, 3538, 977, 50599], "temperature": 0.0, "avg_logprob": -0.10026066362365219, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.30053672194480896}, {"id": 479, "seek": 215666, "start": 2161.3599999999997, "end": 2168.8599999999997, "text": " a higher PQ scene score, 37.9 compared to 32.4, but experiences a decrease in PQ unseen,", "tokens": [50599, 257, 2946, 430, 48, 4145, 6175, 11, 13435, 13, 24, 5347, 281, 8858, 13, 19, 11, 457, 5235, 257, 11514, 294, 430, 48, 40608, 11, 50974], "temperature": 0.0, "avg_logprob": -0.10026066362365219, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.30053672194480896}, {"id": 480, "seek": 215666, "start": 2168.8599999999997, "end": 2174.06, "text": " 2.6 compared to 12.6, compared to a frozen out-of-vocabulary classifier.", "tokens": [50974, 568, 13, 21, 5347, 281, 2272, 13, 21, 11, 5347, 281, 257, 12496, 484, 12, 2670, 12, 20836, 19189, 1508, 9902, 13, 51234], "temperature": 0.0, "avg_logprob": -0.10026066362365219, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.30053672194480896}, {"id": 481, "seek": 215666, "start": 2174.06, "end": 2178.3599999999997, "text": " Consequently, a model that incorporates a trainable CLIP backbone for all components", "tokens": [51234, 2656, 46027, 11, 257, 2316, 300, 50193, 257, 3847, 712, 12855, 9139, 34889, 337, 439, 6677, 51449], "temperature": 0.0, "avg_logprob": -0.10026066362365219, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.30053672194480896}, {"id": 482, "seek": 215666, "start": 2178.3599999999997, "end": 2184.7799999999997, "text": " yields a PQ of 24.1, which is 2.7 lower than our final model, last row, that relies on", "tokens": [51449, 32168, 257, 430, 48, 295, 4022, 13, 16, 11, 597, 307, 568, 13, 22, 3126, 813, 527, 2572, 2316, 11, 1036, 5386, 11, 300, 30910, 322, 51770], "temperature": 0.0, "avg_logprob": -0.10026066362365219, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.30053672194480896}, {"id": 483, "seek": 218478, "start": 2184.86, "end": 2187.38, "text": " a single frozen CLIP backbone.", "tokens": [50368, 257, 2167, 12496, 12855, 9139, 34889, 13, 50494], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 484, "seek": 218478, "start": 2187.38, "end": 2192.34, "text": " Using a trainable mask generator and invocabulary classifier, along with a frozen out-of-vocabulary", "tokens": [50494, 11142, 257, 3847, 712, 6094, 19265, 293, 1048, 905, 19189, 1508, 9902, 11, 2051, 365, 257, 12496, 484, 12, 2670, 12, 20836, 19189, 50742], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 485, "seek": 218478, "start": 2192.34, "end": 2197.02, "text": " classifier boosts the performance but requires maintaining one trainable and one frozen CLIP", "tokens": [50742, 1508, 9902, 9194, 82, 264, 3389, 457, 7029, 14916, 472, 3847, 712, 293, 472, 12496, 12855, 9139, 50976], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 486, "seek": 218478, "start": 2197.02, "end": 2200.38, "text": " weights, resulting in two times more backbone parameters.", "tokens": [50976, 17443, 11, 16505, 294, 732, 1413, 544, 34889, 9834, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 487, "seek": 218478, "start": 2200.38, "end": 2204.6200000000003, "text": " In summary, our observations demonstrate that building the entire framework upon a frozen", "tokens": [51144, 682, 12691, 11, 527, 18163, 11698, 300, 2390, 264, 2302, 8388, 3564, 257, 12496, 51356], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 488, "seek": 218478, "start": 2204.6200000000003, "end": 2209.02, "text": " CLIP backbone is not only effective but also efficient, providing a better balance between", "tokens": [51356, 12855, 9139, 34889, 307, 406, 787, 4942, 457, 611, 7148, 11, 6530, 257, 1101, 4772, 1296, 51576], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 489, "seek": 218478, "start": 2209.02, "end": 2212.5800000000004, "text": " PQ scene and PQ unseen metrics.", "tokens": [51576, 430, 48, 4145, 293, 430, 48, 40608, 16367, 13, 51754], "temperature": 0.0, "avg_logprob": -0.12667581864765712, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.6072513461112976}, {"id": 490, "seek": 221258, "start": 2212.58, "end": 2216.66, "text": " In combination with grounding PQ and grounding MIOU it is worth emphasizing that despite", "tokens": [50364, 682, 6562, 365, 46727, 430, 48, 293, 46727, 13696, 4807, 309, 307, 3163, 45550, 300, 7228, 50568], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 491, "seek": 221258, "start": 2216.66, "end": 2223.58, "text": " the absence of grounding loss, reference 31, 92, 28, 84, during training, our model exhibits", "tokens": [50568, 264, 17145, 295, 46727, 4470, 11, 6408, 10353, 11, 28225, 11, 7562, 11, 29018, 11, 1830, 3097, 11, 527, 2316, 39205, 50914], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 492, "seek": 221258, "start": 2223.58, "end": 2226.94, "text": " exceptional grounding segmentation capabilities.", "tokens": [50914, 19279, 46727, 9469, 399, 10862, 13, 51082], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 493, "seek": 221258, "start": 2226.94, "end": 2233.54, "text": " Tab 7 presents the grounding PQ and grounding MIOU scores of FCCLIP, following the evaluation", "tokens": [51082, 14106, 1614, 13533, 264, 46727, 430, 48, 293, 46727, 13696, 4807, 13444, 295, 48671, 43, 9139, 11, 3480, 264, 13344, 51412], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 494, "seek": 221258, "start": 2233.54, "end": 2236.56, "text": " methodology outlined in, reference 28.", "tokens": [51412, 24850, 27412, 294, 11, 6408, 7562, 13, 51563], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 495, "seek": 221258, "start": 2236.56, "end": 2241.06, "text": " In this evaluation, we exclusively employ ground truth classes as text query inputs", "tokens": [51563, 682, 341, 13344, 11, 321, 20638, 3188, 2727, 3494, 5359, 382, 2487, 14581, 15743, 51788], "temperature": 0.0, "avg_logprob": -0.16673932756696427, "compression_ratio": 1.6433823529411764, "no_speech_prob": 0.41443660855293274}, {"id": 496, "seek": 224106, "start": 2241.14, "end": 2244.18, "text": " to assess the effectiveness of concept grounding.", "tokens": [50368, 281, 5877, 264, 21208, 295, 3410, 46727, 13, 50520], "temperature": 0.0, "avg_logprob": -0.1678812816889599, "compression_ratio": 1.4808510638297872, "no_speech_prob": 0.5693727731704712}, {"id": 497, "seek": 224106, "start": 2244.18, "end": 2249.2999999999997, "text": " Compared to OpenSEG, reference 28, FCCLIP achieves a substantial performance improvement,", "tokens": [50520, 30539, 281, 7238, 5879, 38, 11, 6408, 7562, 11, 27168, 22458, 9139, 3538, 977, 257, 16726, 3389, 10444, 11, 50776], "temperature": 0.0, "avg_logprob": -0.1678812816889599, "compression_ratio": 1.4808510638297872, "no_speech_prob": 0.5693727731704712}, {"id": 498, "seek": 224106, "start": 2249.2999999999997, "end": 2259.2999999999997, "text": " with notable enhancements of plus 11.6, plus 9.1, plus 13.1, and plus 17.7 on A847, PC459,", "tokens": [50776, 365, 22556, 11985, 1117, 295, 1804, 2975, 13, 21, 11, 1804, 1722, 13, 16, 11, 1804, 3705, 13, 16, 11, 293, 1804, 3282, 13, 22, 322, 316, 23, 14060, 11, 6465, 8465, 24, 11, 51276], "temperature": 0.0, "avg_logprob": -0.1678812816889599, "compression_ratio": 1.4808510638297872, "no_speech_prob": 0.5693727731704712}, {"id": 499, "seek": 224106, "start": 2259.2999999999997, "end": 2263.34, "text": " A150, and PC59, respectively.", "tokens": [51276, 316, 20120, 11, 293, 6465, 19600, 11, 25009, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1678812816889599, "compression_ratio": 1.4808510638297872, "no_speech_prob": 0.5693727731704712}, {"id": 500, "seek": 224106, "start": 2263.34, "end": 2268.42, "text": " Even when compared to OpenSEG trained with a localized narrative dataset, reference 63,", "tokens": [51478, 2754, 562, 5347, 281, 7238, 5879, 38, 8895, 365, 257, 44574, 9977, 28872, 11, 6408, 25082, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1678812816889599, "compression_ratio": 1.4808510638297872, "no_speech_prob": 0.5693727731704712}, {"id": 501, "seek": 226842, "start": 2268.42, "end": 2272.86, "text": " which enables training on a significantly larger vocabulary, FCCLIP still surpasses", "tokens": [50364, 597, 17077, 3097, 322, 257, 10591, 4833, 19864, 11, 27168, 22458, 9139, 920, 27650, 279, 50586], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 502, "seek": 226842, "start": 2272.86, "end": 2283.1800000000003, "text": " it with improvements of plus 8.0, plus 2.2, plus 8.6, and plus 13.4 on A847, PC459, A150,", "tokens": [50586, 309, 365, 13797, 295, 1804, 1649, 13, 15, 11, 1804, 568, 13, 17, 11, 1804, 1649, 13, 21, 11, 293, 1804, 3705, 13, 19, 322, 316, 23, 14060, 11, 6465, 8465, 24, 11, 316, 20120, 11, 51102], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 503, "seek": 226842, "start": 2283.1800000000003, "end": 2288.98, "text": " and PC59, respectively, underscoring the grounding proficiency of FCCLIP.", "tokens": [51102, 293, 6465, 19600, 11, 25009, 11, 16692, 66, 3662, 264, 46727, 1740, 42081, 295, 27168, 22458, 9139, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 504, "seek": 226842, "start": 2288.98, "end": 2292.58, "text": " Ensemble in vocabulary and out of vocabulary classifiers in Tab.", "tokens": [51392, 2193, 37227, 294, 19864, 293, 484, 295, 19864, 1508, 23463, 294, 14106, 13, 51572], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 505, "seek": 226842, "start": 2292.58, "end": 2293.58, "text": " 8.", "tokens": [51572, 1649, 13, 51622], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 506, "seek": 226842, "start": 2293.58, "end": 2297.38, "text": " We present experiments conducted to evaluate the impact of ensemble methods and ensemble", "tokens": [51622, 492, 1974, 12050, 13809, 281, 13059, 264, 2712, 295, 19492, 7150, 293, 19492, 51812], "temperature": 0.0, "avg_logprob": -0.12975246841843063, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.21716943383216858}, {"id": 507, "seek": 229738, "start": 2297.38, "end": 2301.94, "text": " parameters on the performance of the in vocabulary and out of vocabulary classifiers.", "tokens": [50364, 9834, 322, 264, 3389, 295, 264, 294, 19864, 293, 484, 295, 19864, 1508, 23463, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 508, "seek": 229738, "start": 2301.94, "end": 2306.6400000000003, "text": " Specifically, we examine two ensemble methods, arithmetic and geometric.", "tokens": [50592, 26058, 11, 321, 17496, 732, 19492, 7150, 11, 42973, 293, 33246, 13, 50827], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 509, "seek": 229738, "start": 2306.6400000000003, "end": 2310.92, "text": " The arithmetic method involves a linear combination of the in vocabulary classifier and the out", "tokens": [50827, 440, 42973, 3170, 11626, 257, 8213, 6562, 295, 264, 294, 19864, 1508, 9902, 293, 264, 484, 51041], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 510, "seek": 229738, "start": 2310.92, "end": 2316.1400000000003, "text": " of vocabulary classifier, while the geometric method is defined as shown in equation 7 of", "tokens": [51041, 295, 19864, 1508, 9902, 11, 1339, 264, 33246, 3170, 307, 7642, 382, 4898, 294, 5367, 1614, 295, 51302], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 511, "seek": 229738, "start": 2316.1400000000003, "end": 2317.5, "text": " main paper.", "tokens": [51302, 2135, 3035, 13, 51370], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 512, "seek": 229738, "start": 2317.5, "end": 2322.44, "text": " It is worth noting that FCCLIP exhibits robustness to different ensemble methods, with both methods", "tokens": [51370, 467, 307, 3163, 26801, 300, 27168, 22458, 9139, 39205, 13956, 1287, 281, 819, 19492, 7150, 11, 365, 1293, 7150, 51617], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 513, "seek": 229738, "start": 2322.44, "end": 2326.54, "text": " displaying a consistent trend within the explored hyperparameter ranges.", "tokens": [51617, 36834, 257, 8398, 6028, 1951, 264, 24016, 9848, 2181, 335, 2398, 22526, 13, 51822], "temperature": 0.0, "avg_logprob": -0.12960641878145235, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.5461949110031128}, {"id": 514, "seek": 232654, "start": 2326.7, "end": 2331.58, "text": " However, the geometric ensemble consistently outperforms the arithmetic ensemble by a slight", "tokens": [50372, 2908, 11, 264, 33246, 19492, 14961, 484, 26765, 82, 264, 42973, 19492, 538, 257, 4036, 50616], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 515, "seek": 232654, "start": 2331.58, "end": 2332.58, "text": " margin.", "tokens": [50616, 10270, 13, 50666], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 516, "seek": 232654, "start": 2332.58, "end": 2336.74, "text": " Additionally, we observe that preference is given to values of \u03b1 is less than or equal", "tokens": [50666, 19927, 11, 321, 11441, 300, 17502, 307, 2212, 281, 4190, 295, 5691, 307, 1570, 813, 420, 2681, 50874], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 517, "seek": 232654, "start": 2336.74, "end": 2342.44, "text": " to 0.5 and \u03b2 is greater than or equal to 0.5, which biases the model towards using", "tokens": [50874, 281, 1958, 13, 20, 293, 15787, 307, 5044, 813, 420, 2681, 281, 1958, 13, 20, 11, 597, 32152, 264, 2316, 3030, 1228, 51159], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 518, "seek": 232654, "start": 2342.44, "end": 2347.1, "text": " the in vocabulary classifier for seen classes and the out of vocabulary classifier for unseen", "tokens": [51159, 264, 294, 19864, 1508, 9902, 337, 1612, 5359, 293, 264, 484, 295, 19864, 1508, 9902, 337, 40608, 51392], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 519, "seek": 232654, "start": 2347.1, "end": 2348.42, "text": " classes.", "tokens": [51392, 5359, 13, 51458], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 520, "seek": 232654, "start": 2348.42, "end": 2355.6, "text": " We also explore extreme cases, including \u03b1 equals 0.0 and \u03b2 equals 0.0, i.e., exclusively", "tokens": [51458, 492, 611, 6839, 8084, 3331, 11, 3009, 5691, 6915, 1958, 13, 15, 293, 15787, 6915, 1958, 13, 15, 11, 741, 13, 68, 7933, 20638, 51817], "temperature": 0.0, "avg_logprob": -0.09702778691830842, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.023683642968535423}, {"id": 521, "seek": 235560, "start": 2355.66, "end": 2362.0, "text": " utilizing the in vocabulary classifier for every class, \u03b1 equals 1.0 and \u03b2 equals 1.0,", "tokens": [50367, 26775, 264, 294, 19864, 1508, 9902, 337, 633, 1508, 11, 5691, 6915, 502, 13, 15, 293, 15787, 6915, 502, 13, 15, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12844573750215418, "compression_ratio": 3.1631205673758864, "no_speech_prob": 0.6503072381019592}, {"id": 522, "seek": 235560, "start": 2362.0, "end": 2368.36, "text": " i.e., exclusively utilizing the out of vocabulary classifier for every class, \u03b1 equals 0.0", "tokens": [50684, 741, 13, 68, 7933, 20638, 26775, 264, 484, 295, 19864, 1508, 9902, 337, 633, 1508, 11, 5691, 6915, 1958, 13, 15, 51002], "temperature": 0.0, "avg_logprob": -0.12844573750215418, "compression_ratio": 3.1631205673758864, "no_speech_prob": 0.6503072381019592}, {"id": 523, "seek": 235560, "start": 2368.36, "end": 2373.7999999999997, "text": " and \u03b2 equals 1.0, i.e., using the in vocabulary classifier for seen classes and the out of", "tokens": [51002, 293, 15787, 6915, 502, 13, 15, 11, 741, 13, 68, 7933, 1228, 264, 294, 19864, 1508, 9902, 337, 1612, 5359, 293, 264, 484, 295, 51274], "temperature": 0.0, "avg_logprob": -0.12844573750215418, "compression_ratio": 3.1631205673758864, "no_speech_prob": 0.6503072381019592}, {"id": 524, "seek": 235560, "start": 2373.7999999999997, "end": 2380.92, "text": " vocabulary classifier for unseen classes, and \u03b1 equals 1.0 and \u03b2 equals 0.0, i.e.,", "tokens": [51274, 19864, 1508, 9902, 337, 40608, 5359, 11, 293, 5691, 6915, 502, 13, 15, 293, 15787, 6915, 1958, 13, 15, 11, 741, 13, 68, 7933, 51630], "temperature": 0.0, "avg_logprob": -0.12844573750215418, "compression_ratio": 3.1631205673758864, "no_speech_prob": 0.6503072381019592}, {"id": 525, "seek": 235560, "start": 2380.92, "end": 2385.24, "text": " using the out of vocabulary classifier for seen classes and the in vocabulary classifier", "tokens": [51630, 1228, 264, 484, 295, 19864, 1508, 9902, 337, 1612, 5359, 293, 264, 294, 19864, 1508, 9902, 51846], "temperature": 0.0, "avg_logprob": -0.12844573750215418, "compression_ratio": 3.1631205673758864, "no_speech_prob": 0.6503072381019592}, {"id": 526, "seek": 238524, "start": 2385.8799999999997, "end": 2387.08, "text": " for unseen classes.", "tokens": [50396, 337, 40608, 5359, 13, 50456], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 527, "seek": 238524, "start": 2387.08, "end": 2391.4199999999996, "text": " The results align with our observations that it is preferable to bias towards the in vocabulary", "tokens": [50456, 440, 3542, 7975, 365, 527, 18163, 300, 309, 307, 4382, 712, 281, 12577, 3030, 264, 294, 19864, 50673], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 528, "seek": 238524, "start": 2391.4199999999996, "end": 2396.9799999999996, "text": " classifier for seen classes and the out of vocabulary classifier for unseen classes.", "tokens": [50673, 1508, 9902, 337, 1612, 5359, 293, 264, 484, 295, 19864, 1508, 9902, 337, 40608, 5359, 13, 50951], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 529, "seek": 238524, "start": 2396.9799999999996, "end": 2401.2, "text": " Quantitative VIT-based clip versus CNN-based clip when input size scales training our model", "tokens": [50951, 26968, 14275, 691, 3927, 12, 6032, 7353, 5717, 24859, 12, 6032, 7353, 562, 4846, 2744, 17408, 3097, 527, 2316, 51162], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 530, "seek": 238524, "start": 2401.2, "end": 2408.2, "text": " solely with VIT-based clip, without any additional modifications, reference 96, 16, 86, 20, is", "tokens": [51162, 23309, 365, 691, 3927, 12, 6032, 7353, 11, 1553, 604, 4497, 26881, 11, 6408, 24124, 11, 3165, 11, 26687, 11, 945, 11, 307, 51512], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 531, "seek": 238524, "start": 2408.2, "end": 2409.2, "text": " infeasible.", "tokens": [51512, 1536, 68, 296, 964, 13, 51562], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 532, "seek": 238524, "start": 2409.2, "end": 2413.72, "text": " Furthermore, applying VIT to large input sizes is computationally expensive.", "tokens": [51562, 23999, 11, 9275, 691, 3927, 281, 2416, 4846, 11602, 307, 24903, 379, 5124, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15432223939059073, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.08267126232385635}, {"id": 533, "seek": 241372, "start": 2414.2, "end": 2419.12, "text": " Therefore, to evaluate the effects of using VTOR CNN-based clip in our framework, we incorporate", "tokens": [50388, 7504, 11, 281, 13059, 264, 5065, 295, 1228, 691, 51, 2483, 24859, 12, 6032, 7353, 294, 527, 8388, 11, 321, 16091, 50634], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 534, "seek": 241372, "start": 2419.12, "end": 2423.8399999999997, "text": " them into our out of vocabulary classifier, which is performed only during inference.", "tokens": [50634, 552, 666, 527, 484, 295, 19864, 1508, 9902, 11, 597, 307, 10332, 787, 1830, 38253, 13, 50870], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 535, "seek": 241372, "start": 2423.8399999999997, "end": 2428.68, "text": " To ensure a fair comparison, we use the same MASC proposals and disable the geometric ensemble", "tokens": [50870, 1407, 5586, 257, 3143, 9660, 11, 321, 764, 264, 912, 42129, 34, 20198, 293, 28362, 264, 33246, 19492, 51112], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 536, "seek": 241372, "start": 2428.68, "end": 2429.8199999999997, "text": " scheme.", "tokens": [51112, 12232, 13, 51169], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 537, "seek": 241372, "start": 2429.8199999999997, "end": 2435.12, "text": " In tab 9, we conduct an ablation study to analyze the impact of different input resolutions", "tokens": [51169, 682, 4421, 1722, 11, 321, 6018, 364, 410, 24278, 2979, 281, 12477, 264, 2712, 295, 819, 4846, 32179, 51434], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 538, "seek": 241372, "start": 2435.12, "end": 2436.72, "text": " for clip models.", "tokens": [51434, 337, 7353, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 539, "seek": 241372, "start": 2436.72, "end": 2443.04, "text": " We consider both VIT-based, VIT-L, 14, and CNN-based, Convinext-L, clip models.", "tokens": [51514, 492, 1949, 1293, 691, 3927, 12, 6032, 11, 691, 3927, 12, 43, 11, 3499, 11, 293, 24859, 12, 6032, 11, 2656, 85, 533, 734, 12, 43, 11, 7353, 5245, 13, 51830], "temperature": 0.0, "avg_logprob": -0.12396275329589844, "compression_ratio": 1.5540983606557377, "no_speech_prob": 0.016910996288061142}, {"id": 540, "seek": 244304, "start": 2443.36, "end": 2447.74, "text": " By employing them as zero-shot MASC classifiers and varying the input resolutions, we observe", "tokens": [50380, 3146, 3188, 278, 552, 382, 4018, 12, 18402, 42129, 34, 1508, 23463, 293, 22984, 264, 4846, 32179, 11, 321, 11441, 50599], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 541, "seek": 244304, "start": 2447.74, "end": 2452.68, "text": " that CNN-based clip demonstrates superior generalization ability as the input size scales", "tokens": [50599, 300, 24859, 12, 6032, 7353, 31034, 13028, 2674, 2144, 3485, 382, 264, 4846, 2744, 17408, 50846], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 542, "seek": 244304, "start": 2452.68, "end": 2453.68, "text": " up.", "tokens": [50846, 493, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 543, "seek": 244304, "start": 2453.68, "end": 2458.72, "text": " Specifically, we observe that the VIT-L, 14 clip has a higher PQ at a lower resolution,", "tokens": [50896, 26058, 11, 321, 11441, 300, 264, 691, 3927, 12, 43, 11, 3499, 7353, 575, 257, 2946, 430, 48, 412, 257, 3126, 8669, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 544, "seek": 244304, "start": 2458.72, "end": 2464.7, "text": " i.e., input size 224, but suffers from a higher resolution, which leads existing two-stage", "tokens": [51148, 741, 13, 68, 7933, 4846, 2744, 5853, 19, 11, 457, 33776, 490, 257, 2946, 8669, 11, 597, 6689, 6741, 732, 12, 17882, 51447], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 545, "seek": 244304, "start": 2464.7, "end": 2471.96, "text": " methods, reference 85, 50, 24, 86, 84, to adopt different input resolutions for MASC", "tokens": [51447, 7150, 11, 6408, 14695, 11, 2625, 11, 4022, 11, 26687, 11, 29018, 11, 281, 6878, 819, 4846, 32179, 337, 42129, 34, 51810], "temperature": 0.0, "avg_logprob": -0.1278080817980644, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00831493642181158}, {"id": 546, "seek": 247196, "start": 2472.88, "end": 2474.4, "text": " generator and classifier branches.", "tokens": [50410, 19265, 293, 1508, 9902, 14770, 13, 50486], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 547, "seek": 247196, "start": 2474.4, "end": 2479.2, "text": " On the contrary, FC-clip provides a simple solution by adopting a CNN-based clip that", "tokens": [50486, 1282, 264, 19506, 11, 27168, 12, 21614, 6417, 257, 2199, 3827, 538, 32328, 257, 24859, 12, 6032, 7353, 300, 50726], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 548, "seek": 247196, "start": 2479.2, "end": 2482.7200000000003, "text": " generalizes well to different input sizes.", "tokens": [50726, 2674, 5660, 731, 281, 819, 4846, 11602, 13, 50902], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 549, "seek": 247196, "start": 2482.7200000000003, "end": 2486.2400000000002, "text": " Visualization We provide visualization on ADE20K val set", "tokens": [50902, 23187, 2144, 492, 2893, 25801, 322, 9135, 36, 2009, 42, 1323, 992, 51078], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 550, "seek": 247196, "start": 2486.2400000000002, "end": 2487.92, "text": " in Fig. 5.", "tokens": [51078, 294, 22443, 13, 1025, 13, 51162], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 551, "seek": 247196, "start": 2487.92, "end": 2492.4, "text": " 7 datasets information and licenses The datasets we used for training and or testing", "tokens": [51162, 1614, 42856, 1589, 293, 32821, 440, 42856, 321, 1143, 337, 3097, 293, 420, 4997, 51386], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 552, "seek": 247196, "start": 2492.4, "end": 2495.0, "text": " FC-clip are described as follows.", "tokens": [51386, 27168, 12, 21614, 366, 7619, 382, 10002, 13, 51516], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 553, "seek": 247196, "start": 2495.0, "end": 2498.52, "text": " Cocoa We train FC-clip on cocoa data with panoptic", "tokens": [51516, 29787, 64, 492, 3847, 27168, 12, 21614, 322, 30634, 1412, 365, 2462, 5747, 299, 51692], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 554, "seek": 247196, "start": 2498.52, "end": 2501.12, "text": " annotation, reference 52.", "tokens": [51692, 48654, 11, 6408, 18079, 13, 51822], "temperature": 0.0, "avg_logprob": -0.22357505935806413, "compression_ratio": 1.599250936329588, "no_speech_prob": 0.4604933559894562}, {"id": 555, "seek": 250112, "start": 2501.2799999999997, "end": 2507.04, "text": " We follow the 2017 splits which include 118K images for train split and 5K images for val", "tokens": [50372, 492, 1524, 264, 6591, 37741, 597, 4090, 2975, 23, 42, 5267, 337, 3847, 7472, 293, 1025, 42, 5267, 337, 1323, 50660], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 556, "seek": 250112, "start": 2507.04, "end": 2508.04, "text": " split.", "tokens": [50660, 7472, 13, 50710], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 557, "seek": 250112, "start": 2508.04, "end": 2512.56, "text": " If not specified, we train our model on the cocoa train split and report results on val", "tokens": [50710, 759, 406, 22206, 11, 321, 3847, 527, 2316, 322, 264, 30634, 3847, 7472, 293, 2275, 3542, 322, 1323, 50936], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 558, "seek": 250112, "start": 2512.56, "end": 2515.0, "text": " set of various datasets.", "tokens": [50936, 992, 295, 3683, 42856, 13, 51058], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 559, "seek": 250112, "start": 2515.0, "end": 2518.4, "text": " License Creative Commons Attribution 4.0 License URL", "tokens": [51058, 40627, 1288, 26598, 34894, 7298, 30783, 1017, 13, 15, 40627, 1288, 12905, 51228], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 560, "seek": 250112, "start": 2518.4, "end": 2523.24, "text": " https://cocodataset.org/.home-ade20k", "tokens": [51228, 34426, 21492, 66, 905, 378, 37892, 302, 13, 4646, 48550, 25336, 12, 762, 2009, 74, 51470], "temperature": 0.0, "avg_logprob": -0.28064796536467795, "compression_ratio": 1.4875621890547264, "no_speech_prob": 0.01542170811444521}, {"id": 561, "seek": 252324, "start": 2524.24, "end": 2530.16, "text": " ADE20K, reference 95, covers a wide range of indoor and outdoor scenes, with 2K val", "tokens": [50414, 9135, 36, 2009, 42, 11, 6408, 13420, 11, 10538, 257, 4874, 3613, 295, 24029, 293, 15942, 8026, 11, 365, 568, 42, 1323, 50710], "temperature": 0.0, "avg_logprob": -0.20230455887623322, "compression_ratio": 1.4, "no_speech_prob": 0.17767642438411713}, {"id": 562, "seek": 252324, "start": 2530.16, "end": 2531.3999999999996, "text": " images.", "tokens": [50710, 5267, 13, 50772], "temperature": 0.0, "avg_logprob": -0.20230455887623322, "compression_ratio": 1.4, "no_speech_prob": 0.17767642438411713}, {"id": 563, "seek": 252324, "start": 2531.3999999999996, "end": 2537.7999999999997, "text": " We evaluate FC-clip on both the version with 847 classes, A847, and the more widely used", "tokens": [50772, 492, 13059, 27168, 12, 21614, 322, 1293, 264, 3037, 365, 1649, 14060, 5359, 11, 316, 23, 14060, 11, 293, 264, 544, 13371, 1143, 51092], "temperature": 0.0, "avg_logprob": -0.20230455887623322, "compression_ratio": 1.4, "no_speech_prob": 0.17767642438411713}, {"id": 564, "seek": 252324, "start": 2537.7999999999997, "end": 2542.2, "text": " version with 150 frequent categories, A150.", "tokens": [51092, 3037, 365, 8451, 18004, 10479, 11, 316, 20120, 13, 51312], "temperature": 0.0, "avg_logprob": -0.20230455887623322, "compression_ratio": 1.4, "no_speech_prob": 0.17767642438411713}, {"id": 565, "seek": 252324, "start": 2542.2, "end": 2545.3199999999997, "text": " License Creative Commons BSD3 License URL", "tokens": [51312, 40627, 1288, 26598, 34894, 363, 23969, 18, 40627, 1288, 12905, 51468], "temperature": 0.0, "avg_logprob": -0.20230455887623322, "compression_ratio": 1.4, "no_speech_prob": 0.17767642438411713}, {"id": 566, "seek": 254532, "start": 2545.4, "end": 2557.2400000000002, "text": " https://groups.csale.mit.edu/.vision/.dataset/.ade20k/.citiescapes.citiescapes.reference21.focuses", "tokens": [50368, 34426, 21492, 17377, 82, 13, 14368, 1220, 13, 3508, 13, 22938, 48550, 6763, 48550, 20367, 296, 302, 48550, 762, 2009, 74, 48550, 66, 1088, 496, 5190, 13, 66, 1088, 496, 5190, 13, 265, 5158, 4436, 13, 69, 905, 8355, 50960], "temperature": 0.0, "avg_logprob": -0.19336096130975403, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.06276243925094604}, {"id": 567, "seek": 254532, "start": 2557.2400000000002, "end": 2560.28, "text": " on semantic understanding of urban street scenes.", "tokens": [50960, 322, 47982, 3701, 295, 9681, 4838, 8026, 13, 51112], "temperature": 0.0, "avg_logprob": -0.19336096130975403, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.06276243925094604}, {"id": 568, "seek": 254532, "start": 2560.28, "end": 2564.6400000000003, "text": " We use the fine data includes 500 images for validation set.", "tokens": [51112, 492, 764, 264, 2489, 1412, 5974, 5923, 5267, 337, 24071, 992, 13, 51330], "temperature": 0.0, "avg_logprob": -0.19336096130975403, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.06276243925094604}, {"id": 569, "seek": 254532, "start": 2564.6400000000003, "end": 2567.7200000000003, "text": " License This dataset is made freely available to academic", "tokens": [51330, 40627, 1288, 639, 28872, 307, 1027, 16433, 2435, 281, 7778, 51484], "temperature": 0.0, "avg_logprob": -0.19336096130975403, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.06276243925094604}, {"id": 570, "seek": 254532, "start": 2567.7200000000003, "end": 2572.44, "text": " and non-academic entities for non-commercial purposes such as academic research, teaching,", "tokens": [51484, 293, 2107, 12, 326, 345, 3438, 16667, 337, 2107, 12, 1112, 39260, 9932, 1270, 382, 7778, 2132, 11, 4571, 11, 51720], "temperature": 0.0, "avg_logprob": -0.19336096130975403, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.06276243925094604}, {"id": 571, "seek": 257244, "start": 2572.44, "end": 2575.8, "text": " scientific publications, or personal experimentation.", "tokens": [50364, 8134, 25618, 11, 420, 2973, 37142, 13, 50532], "temperature": 0.0, "avg_logprob": -0.3466725651221939, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.8303840756416321}, {"id": 572, "seek": 257244, "start": 2575.8, "end": 2588.7200000000003, "text": " URL https://www.citiescapes-dataset.com/.mapillaryvistas.mapillaryvistas.reference62 is a large-scale traffic-related", "tokens": [50532, 12905, 34426, 21492, 17919, 13, 66, 1088, 496, 5190, 12, 20367, 296, 302, 13, 1112, 48550, 24223, 46367, 85, 14858, 13, 24223, 46367, 85, 14858, 13, 265, 5158, 28052, 307, 257, 2416, 12, 20033, 6419, 12, 12004, 51178], "temperature": 0.0, "avg_logprob": -0.3466725651221939, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.8303840756416321}, {"id": 573, "seek": 257244, "start": 2588.7200000000003, "end": 2592.64, "text": " dataset, including 2K images for validation purposes.", "tokens": [51178, 28872, 11, 3009, 568, 42, 5267, 337, 24071, 9932, 13, 51374], "temperature": 0.0, "avg_logprob": -0.3466725651221939, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.8303840756416321}, {"id": 574, "seek": 257244, "start": 2592.64, "end": 2596.16, "text": " License Creative Commons Attribution Non-Commercial Share-Alike.", "tokens": [51374, 40627, 1288, 26598, 34894, 7298, 30783, 8774, 12, 14627, 39260, 14945, 12, 32, 4092, 13, 51550], "temperature": 0.0, "avg_logprob": -0.3466725651221939, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.8303840756416321}, {"id": 575, "seek": 259616, "start": 2597.16, "end": 2620.7599999999998, "text": " We evaluate FC-clip on both its full version, PC459, with 459 classes and the more common", "tokens": [50414, 492, 13059, 27168, 12, 21614, 322, 1293, 1080, 1577, 3037, 11, 6465, 8465, 24, 11, 365, 6905, 24, 5359, 293, 264, 544, 2689, 51594], "temperature": 0.0, "avg_logprob": -0.3111940434104518, "compression_ratio": 1.2346938775510203, "no_speech_prob": 0.48395803570747375}, {"id": 576, "seek": 259616, "start": 2620.7599999999998, "end": 2624.52, "text": " version, PC59, with 59 classes.", "tokens": [51594, 3037, 11, 6465, 19600, 11, 365, 24624, 5359, 13, 51782], "temperature": 0.0, "avg_logprob": -0.3111940434104518, "compression_ratio": 1.2346938775510203, "no_speech_prob": 0.48395803570747375}, {"id": 577, "seek": 262452, "start": 2625.52, "end": 2644.16, "text": " URL https://www.cs.stanford.edu/.ruseba/.pascal-context/.pascalvoc.pascalvoc.reference26.contains1.5Kval images with 20 foreground classes and 1 background class.", "tokens": [50414, 12905, 34426, 21492, 17919, 13, 14368, 13, 18758, 7404, 13, 22938, 48550, 81, 438, 4231, 48550, 79, 27303, 12, 9000, 3828, 48550, 79, 27303, 85, 905, 13, 79, 27303, 85, 905, 13, 265, 5158, 10880, 13, 9000, 2315, 16, 13, 20, 42, 3337, 5267, 365, 945, 32058, 5359, 293, 502, 3678, 1508, 13, 51346], "temperature": 0.0, "avg_logprob": -0.30452660510414525, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.5803875923156738}, {"id": 578, "seek": 262452, "start": 2644.16, "end": 2648.56, "text": " Due to the ambiguity in definition of background, we assign the background class to the pixels", "tokens": [51346, 18980, 281, 264, 46519, 294, 7123, 295, 3678, 11, 321, 6269, 264, 3678, 1508, 281, 264, 18668, 51566], "temperature": 0.0, "avg_logprob": -0.30452660510414525, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.5803875923156738}, {"id": 579, "seek": 264856, "start": 2648.56, "end": 2654.24, "text": " predicted as PC59 categories that are not in Pascal VOC following, Reference 28, which", "tokens": [50364, 19147, 382, 6465, 19600, 10479, 300, 366, 406, 294, 41723, 15216, 34, 3480, 11, 1300, 5158, 7562, 11, 597, 50648], "temperature": 0.0, "avg_logprob": -0.3059445487128364, "compression_ratio": 1.3125, "no_speech_prob": 0.39896902441978455}, {"id": 580, "seek": 264856, "start": 2654.24, "end": 2656.5, "text": " leads to PAS21.", "tokens": [50648, 6689, 281, 430, 3160, 4436, 13, 50761], "temperature": 0.0, "avg_logprob": -0.3059445487128364, "compression_ratio": 1.3125, "no_speech_prob": 0.39896902441978455}, {"id": 581, "seek": 264856, "start": 2656.5, "end": 2661.44, "text": " We also evaluate the model with background class excluded, which leads to PAS20.", "tokens": [50761, 492, 611, 13059, 264, 2316, 365, 3678, 1508, 29486, 11, 597, 6689, 281, 430, 3160, 2009, 13, 51008], "temperature": 0.0, "avg_logprob": -0.3059445487128364, "compression_ratio": 1.3125, "no_speech_prob": 0.39896902441978455}, {"id": 582, "seek": 264856, "start": 2661.44, "end": 2669.44, "text": " URL https://host.robots.ox.ac.uk/.pascal/.voc/.", "tokens": [51008, 12905, 34426, 21492, 6037, 13, 16614, 1971, 13, 5230, 13, 326, 13, 2034, 48550, 79, 27303, 48550, 85, 905, 48550, 51408], "temperature": 0.0, "avg_logprob": -0.3059445487128364, "compression_ratio": 1.3125, "no_speech_prob": 0.39896902441978455}, {"id": 583, "seek": 266944, "start": 2669.44, "end": 2679.48, "text": " Thanks for listening to this reading.", "tokens": [50364, 2561, 337, 4764, 281, 341, 3760, 13, 50866], "temperature": 0.0, "avg_logprob": -0.31649929682413735, "compression_ratio": 1.148936170212766, "no_speech_prob": 0.31262901425361633}, {"id": 584, "seek": 266944, "start": 2679.48, "end": 2687.08, "text": " For the entire paper, and more, check out our homepage, papersread.ai.", "tokens": [50866, 1171, 264, 2302, 3035, 11, 293, 544, 11, 1520, 484, 527, 31301, 11, 10577, 2538, 13, 1301, 13, 51246], "temperature": 0.0, "avg_logprob": -0.31649929682413735, "compression_ratio": 1.148936170212766, "no_speech_prob": 0.31262901425361633}], "language": "en"}